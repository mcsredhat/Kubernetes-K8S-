## 7. Scaling & Reliability

### HorizontalPodAutoscaler
**Description:** Auto-scale pod replicas based on CPU/memory/custom metrics
**Use Case:** Dynamic workload scaling, cost optimization

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: example-hpa
  namespace: example-namespace
  labels:
    autoscaling: cpu
  annotations:
    description: "Scale based on CPU utilization"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: example-deployment
  minReplicas: 2
  maxReplicas: 10
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Apply Example:**
```bash
# CORRECTED: Verify Metrics Server is installed and running
kubectl get apiservice v1beta1.metrics.k8s.io -o yaml

# If not present, install it:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Then apply HPA
kubectl apply -f hpa.yaml -n example-namespace
kubectl get hpa -n example-namespace
kubectl describe hpa example-hpa -n example-namespace
kubectl top pods -n example-namespace
```

**Customization Notes:**
- Requires Metrics Server installed and running (check APIService)
- Set realistic thresholds based on application behavior
- Use `behavior` to tune scale-up/down aggressiveness
- Allow time for metrics collection (initial delay ~1-2 minutes)

**Common Pitfalls:**
- Metrics Server not installed/running (HPA remains inactive without metrics)
- Setting thresholds too low (constant scaling churn)
- Not accounting for initialization time
- Using HPA with VPA in Auto mode (conflicts on decisions)

---

### VerticalPodAutoscaler
**Description:** Auto-tune resource requests and limits
**Use Case:** Right-sizing workloads, eliminating over/under-provisioning

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: example-vpa
  namespace: example-namespace
  labels:
    autoscaling: vertical
  annotations:
    description: "Auto-tune container resources"
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: example-deployment
  updatePolicy:
    updateMode: "Initial"  # Start with Initial to validate recommendations
  resourcePolicy:
    containerPolicies:
    - containerName: "*"
      minAllowed:
        cpu: 50m
        memory: 50Mi
      maxAllowed:
        cpu: 500m
        memory: 500Mi
      controlledValues: RequestsAndLimits
    - containerName: sidecar
      minAllowed:
        cpu: 10m
        memory: 10Mi
      maxAllowed:
        cpu: 50m
        memory: 50Mi
```

**Apply Example:**
```bash
kubectl apply -f vpa.yaml -n example-namespace
kubectl describe vpa example-vpa -n example-namespace
kubectl get vpa -n example-namespace

# View recommendations without applying
kubectl describe vpa example-vpa -n example-namespace | grep -A 5 "Recommendation"
```

**Customization Notes:**
- Use `Initial` mode first to see recommendations without restarts
- Transition to `Auto` mode only after validating recommendations
- Set reasonable min/max bounds to prevent extreme values
- NEVER use both HPA and VPA in Auto mode (they conflict and cause constant pod restarts)

**Common Pitfalls:**
- Using `Auto` mode without proper testing (pods restart constantly)
- VPA and HPA both in Auto mode targeting same metrics (fights for control)
- Recommendations ignored without proper setup
- VPA controller not installed/running

---

### PodDisruptionBudget
**Description:** Guarantee minimum pod availability during disruptions
**Use Case:** High-availability applications, planned maintenance

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: example-pdb
  namespace: example-namespace
  labels:
    reliability: high
  annotations:
    description: "Ensure minimum pods available"
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: example
      tier: frontend
  unhealthyPodEvictionPolicy: IfHealthyBudget
```

**Apply Example:**
```bash
kubectl apply -f pdb.yaml -n example-namespace
kubectl get pdb -n example-namespace
kubectl describe pdb example-pdb -n example-namespace
```

**Customization Notes:**
- Use `minAvailable` for critical apps (absolute number)
- Use `maxUnavailable` for flexible workloads (percentage)
- Monitor `DisruptionsAllowed` status
- Validate with: `kubectl get pdb -w` during disruptions

**Common Pitfalls:**
- Setting `minAvailable` too high preventing node maintenance
- Not accounting for pods not ready before disruption
- Using with single replica pods (defeats purpose)

---