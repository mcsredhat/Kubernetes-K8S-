# ============================================================================
# CORRECTED: 10-backup-maintenance.yaml - Security Context Fixes
# FIX: Added proper security contexts to all containers
# ============================================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: ${APP_NAME}-backup-registry
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    app.kubernetes.io/component: backup
data:
  last-successful-backup: ""
  backup-count: "0"
  last-restore: ""
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${APP_NAME}-backup-pvc
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    app.kubernetes.io/component: backup-storage
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: "${STORAGE_CLASS}"
  resources:
    requests:
      storage: ${BACKUP_STORAGE_SIZE}
---
# Enhanced Backup CronJob with proper security
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ${APP_NAME}-backup
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    app.kubernetes.io/component: backup
spec:
  schedule: "${BACKUP_SCHEDULE}"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      activeDeadlineSeconds: 3600
      template:
        spec:
          restartPolicy: OnFailure
          serviceAccountName: ${APP_NAME}-sa
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            runAsGroup: 65534
            fsGroup: 65534
            seccompProfile:
              type: RuntimeDefault
          containers:
          - name: backup
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_DIR="/backup/${BACKUP_DATE}"

              echo "=== Backup Started: $(date) ==="
              mkdir -p "${BACKUP_DIR}"

              if [ -d "/data" ] && [ "$(ls -A /data 2>/dev/null)" ]; then
                tar -czf "${BACKUP_DIR}/app-data.tar.gz" -C /data . 2>/dev/null || true
                echo "✓ Data backed up"
              fi

              # Get deployment info
              REPLICAS=$(kubectl get deployment ${APP_NAME}-deployment -n ${NAMESPACE} -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")

              # Create metadata
              cat > "${BACKUP_DIR}/backup-info.json" << EOF
              {
                "app": "${APP_NAME}",
                "backup_date": "${BACKUP_DATE}",
                "deployment_replicas": "${REPLICAS}"
              }
              EOF

              # Checksums
              cd "${BACKUP_DIR}"
              [ -f "app-data.tar.gz" ] && sha256sum app-data.tar.gz > checksums.sha256

              # Update registry
              kubectl patch configmap ${APP_NAME}-backup-registry -n ${NAMESPACE} \
                --type merge -p "{\"data\":{\"last-successful-backup\":\"${BACKUP_DATE}\"}}" 2>/dev/null || true

              # Cleanup old backups
              find /backup -name "20*" -type d -mtime +${BACKUP_RETENTION_DAYS} -exec rm -rf {} + 2>/dev/null || true

              echo "=== Backup Complete ==="

            volumeMounts:
            - name: app-data
              mountPath: /data
              readOnly: true
            - name: backup-storage
              mountPath: /backup

            securityContext:
              runAsUser: 65534
              runAsGroup: 65534
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: [ "ALL" ]
              seccompProfile:
                type: RuntimeDefault

            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "1Gi"
                cpu: "500m"

          volumes:
          - name: app-data
            persistentVolumeClaim:
              claimName: ${APP_NAME}-data-pvc
          - name: backup-storage
            persistentVolumeClaim:
              claimName: ${APP_NAME}-backup-pvc
---
# CORRECTED: Rollback Job with proper security contexts
apiVersion: batch/v1
kind: Job
metadata:
  name: ${APP_NAME}-rollback
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    app.kubernetes.io/component: rollback
spec:
  activeDeadlineSeconds: 3600
  backoffLimit: 1
  template:
    spec:
      restartPolicy: Never
      serviceAccountName: ${APP_NAME}-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault

      initContainers:
      - name: pre-rollback-check
        image: busybox:1.36
        command: [ '/bin/sh', '-c' ]
        args:
        - |
          LATEST_BACKUP=$(ls -t /backup | grep "^20" | head -n 1)
          if [ -z "$LATEST_BACKUP" ]; then
            echo "ERROR: No backup found!"
            exit 1
          fi
          echo "$LATEST_BACKUP" > /shared/backup-to-restore
          echo "✓ Found backup: $LATEST_BACKUP"
        volumeMounts:
        - name: backup-storage
          mountPath: /backup
          readOnly: true
        - name: shared
          mountPath: /shared
        securityContext:
          runAsUser: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          capabilities:
            drop: [ "ALL" ]
          seccompProfile:
            type: RuntimeDefault
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

      containers:
      - name: rollback
        image: bitnami/kubectl:latest
        command: [ '/bin/bash', '-c' ]
        args:
        - |
          set -e
          echo "=== ROLLBACK STARTED ==="

          BACKUP_TO_RESTORE=$(cat /shared/backup-to-restore)
          echo "Using backup: $BACKUP_TO_RESTORE"

          # Scale down
          ORIGINAL_REPLICAS=$(kubectl get deployment ${APP_NAME}-deployment -n ${NAMESPACE} -o jsonpath='{.spec.replicas}')
          kubectl scale deployment ${APP_NAME}-deployment --replicas=0 -n ${NAMESPACE}
          sleep 10

          # Restore data
          if [ -f "/backup/$BACKUP_TO_RESTORE/app-data.tar.gz" ]; then
            rm -rf /data/* 2>/dev/null || true
            tar -xzf "/backup/$BACKUP_TO_RESTORE/app-data.tar.gz" -C /data
            echo "✓ Data restored"
          fi

          # Scale up
          kubectl scale deployment ${APP_NAME}-deployment --replicas=$ORIGINAL_REPLICAS -n ${NAMESPACE}

          echo "=== ROLLBACK COMPLETE ==="

        volumeMounts:
        - name: app-data
          mountPath: /data
        - name: backup-storage
          mountPath: /backup
          readOnly: true
        - name: shared
          mountPath: /shared
          readOnly: true

        securityContext:
          runAsUser: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          capabilities:
            drop: [ "ALL" ]
          seccompProfile:
            type: RuntimeDefault

        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "500m"

      volumes:
      - name: app-data
        persistentVolumeClaim:
          claimName: ${APP_NAME}-data-pvc
      - name: backup-storage
        persistentVolumeClaim:
          claimName: ${APP_NAME}-backup-pvc
      - name: shared
        emptyDir: {}
