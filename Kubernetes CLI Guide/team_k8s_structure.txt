# =============================================================================
# KUBERNETES TEAM-READY STRUCTURE
# =============================================================================
# This structure separates concerns for different team roles:
# - Platform Team: Infrastructure, networking, security policies
# - Development Team: Application deployment, configuration  
# - SRE Team: Monitoring, scaling, reliability
# - Security Team: RBAC, secrets, network policies
# =============================================================================

# -----------------------------------------------------------------------------
# FILE: base/deployment.yaml
# OWNER: Development Team
# PURPOSE: Core application deployment without environment-specific configs
# -----------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: generic-app
  labels:
    app: generic-app
    component: application
    managed-by: development-team
  annotations:
    # Deployment tracking for change management
    deployment.kubernetes.io/revision: "1"
    kubernetes.io/change-cause: "Base application deployment"
    # Contact information for incident response
    contact.team: "development-team@company.com"
    documentation.url: "https://wiki.company.com/apps/generic-app"
spec:
  # Strategic deployment configuration for zero-downtime updates
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Allow one extra pod during updates
      maxUnavailable: 0    # Never take down existing pods until new ones are ready
  
  # Base replica count - will be overridden by environment patches
  replicas: 2
  
  selector:
    matchLabels:
      app: generic-app
  
  template:
    metadata:
      labels:
        app: generic-app
        component: application
        # Version label will be patched per environment
        version: base
      annotations:
        # Monitoring configuration - SRE team manages these
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
        
        # Service mesh integration - Platform team manages
        sidecar.istio.io/inject: "true"
        
        # Application metadata for troubleshooting
        deployed.by: "ci-cd-pipeline"
        last.updated: "2025-01-15T10:00:00Z"
    
    spec:
      # Security: Use dedicated service account (defined separately)
      serviceAccountName: generic-app-sa
      
      # Graceful shutdown period for clean application termination
      terminationGracePeriodSeconds: 30
      
      # Modern pod distribution using topology spread constraints
      # This replaces older anti-affinity rules for better scheduling
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: generic-app
      
      # Pod-level security context - Security team defines these standards
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000
        # Modern security profile for container runtime protection
        seccompProfile:
          type: RuntimeDefault
      
      containers:
      - name: app
        # Image will be patched per environment with specific tags/digests
        image: placeholder/generic-app:latest
        imagePullPolicy: IfNotPresent
        
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP
        
        # Resource specifications - will be overridden per environment
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
            ephemeral-storage: "1Gi"
          limits:
            memory: "256Mi"
            cpu: "500m"
            ephemeral-storage: "2Gi"
        
        # Comprehensive health checking strategy
        # Startup probe handles slow-starting applications
        startupProbe:
          httpGet:
            path: /health/startup
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # Allow 5 minutes for startup
          successThreshold: 1
        
        # Liveness probe detects hung applications
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
            httpHeaders:
            - name: X-Health-Check
              value: "liveness"
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        
        # Readiness probe controls traffic routing
        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
            httpHeaders:
            - name: X-Health-Check
              value: "readiness"
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
          successThreshold: 1
        
        # Lifecycle hooks for graceful startup and shutdown
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                echo "Application started at $(date)" > /tmp/app-lifecycle.log
                # Signal readiness to external monitoring systems
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                echo "Graceful shutdown initiated at $(date)" >> /tmp/app-lifecycle.log
                # Allow time for load balancer to remove this pod from rotation
                sleep 15
        
        # Environment variables - base set, extended per environment
        env:
        - name: APP_NAME
          value: "generic-app"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        
        # Configuration injection from ConfigMaps and Secrets
        envFrom:
        - configMapRef:
            name: generic-app-config
        - secretRef:
            name: generic-app-secrets
        
        # Volume mounts for configuration and temporary storage
        volumeMounts:
        - name: app-config
          mountPath: /etc/app/config
          readOnly: true
        - name: tmp-storage
          mountPath: /tmp
        - name: cache-storage
          mountPath: /app/cache
        
        # Container-level security hardening
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        
        # Working directory for application
        workingDir: /app
      
      # Volume definitions - shared across all containers in pod
      volumes:
      - name: app-config
        configMap:
          name: generic-app-config
          defaultMode: 0644
      - name: tmp-storage
        emptyDir:
          sizeLimit: 1Gi
      - name: cache-storage
        emptyDir:
          sizeLimit: 2Gi
      
      # DNS configuration for service discovery optimization
      dnsPolicy: ClusterFirst
      dnsConfig:
        options:
        - name: ndots
          value: "2"
        - name: edns0
      
      # Node selection criteria - will be overridden per environment
      nodeSelector:
        kubernetes.io/os: linux

---
# -----------------------------------------------------------------------------
# FILE: base/service.yaml  
# OWNER: Platform Team
# PURPOSE: Service definition for application connectivity
# -----------------------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: generic-app-service
  labels:
    app: generic-app
    component: networking
    managed-by: platform-team
  annotations:
    # Service discovery and load balancing configuration
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    # Monitoring integration
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
    # Documentation and contact information
    contact.team: "platform-team@company.com"
spec:
  type: ClusterIP
  selector:
    app: generic-app
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: metrics
    protocol: TCP
  # Session affinity for stateful applications (disabled by default)
  sessionAffinity: None

---
# -----------------------------------------------------------------------------
# FILE: base/serviceaccount.yaml
# OWNER: Security Team  
# PURPOSE: RBAC service account with minimal permissions
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  name: generic-app-sa
  labels:
    app: generic-app
    component: security
    managed-by: security-team
  annotations:
    contact.team: "security-team@company.com"
    security.policy: "least-privilege"
# Security best practice: disable automatic token mounting unless needed
automountServiceAccountToken: false

---
# -----------------------------------------------------------------------------
# FILE: base/configmap.yaml
# OWNER: Development Team
# PURPOSE: Application configuration that changes per environment
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: generic-app-config
  labels:
    app: generic-app
    component: configuration
    managed-by: development-team
  annotations:
    contact.team: "development-team@company.com"
    config.version: "1.0.0"
data:
  # Application configuration in YAML format
  app-config.yaml: |
    server:
      port: 8080
      host: "0.0.0.0"
      read_timeout: "30s"
      write_timeout: "30s"
    
    logging:
      level: "info"
      format: "json"
      output: "stdout"
    
    health:
      startup_path: "/health/startup"
      liveness_path: "/health/live"  
      readiness_path: "/health/ready"
    
    metrics:
      enabled: true
      port: 9090
      path: "/metrics"
    
    # Database connection pool settings
    database:
      max_connections: 10
      connection_timeout: "5s"
      idle_timeout: "10m"
    
    # Cache configuration
    cache:
      ttl: "1h"
      max_size: "100MB"
  
  # Nginx configuration for reverse proxy scenarios
  nginx.conf: |
    upstream app_backend {
        server 127.0.0.1:8080;
    }
    
    server {
        listen 80;
        server_name _;
        
        # Health check endpoints
        location /health/ {
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        
        # Main application traffic
        location / {
            proxy_pass http://app_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # Metrics endpoint (internal only)
        location /metrics {
            proxy_pass http://app_backend:9090;
            # Restrict access to monitoring systems
            allow 10.0.0.0/8;
            deny all;
        }
    }

---
# -----------------------------------------------------------------------------
# FILE: base/kustomization.yaml
# OWNER: Platform Team
# PURPOSE: Base Kustomize configuration for common resources
# -----------------------------------------------------------------------------
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

metadata:
  name: generic-app-base
  annotations:
    config.kubernetes.io/local-config: "true"

# Common labels applied to all resources
commonLabels:
  app: generic-app
  managed-by: kustomize

# Base resources that are common across all environments
resources:
- deployment.yaml
- service.yaml
- serviceaccount.yaml
- configmap.yaml

# Images that will be updated per environment
images:
- name: placeholder/generic-app
  newTag: latest

# ConfigMap and Secret generators for dynamic configuration
configMapGenerator:
- name: generic-app-environment
  literals:
  - ENVIRONMENT=base
  - LOG_LEVEL=info

# Resource transformations applied to all resources
replicas:
- name: generic-app
  count: 2

---
# -----------------------------------------------------------------------------
# FILE: overlays/development/kustomization.yaml
# OWNER: Development Team
# PURPOSE: Development environment specific configurations
# -----------------------------------------------------------------------------
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

metadata:
  name: generic-app-development

# Reference to base configuration
resources:
- ../../base

# Development-specific labels
commonLabels:
  environment: development
  cost-center: development

# Environment-specific patches
patchesStrategicMerge:
- deployment-patch.yaml
- configmap-patch.yaml

# Development image configuration
images:
- name: placeholder/generic-app
  newTag: dev-latest

# ConfigMap generator for development environment
configMapGenerator:
- name: generic-app-environment
  behavior: merge
  literals:
  - ENVIRONMENT=development
  - LOG_LEVEL=debug
  - DEBUG_MODE=true

# Resource adjustments for development
replicas:
- name: generic-app
  count: 1

---
# -----------------------------------------------------------------------------
# FILE: overlays/development/deployment-patch.yaml
# OWNER: Development Team  
# PURPOSE: Development-specific deployment modifications
# -----------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: generic-app
  annotations:
    # Development-specific annotations
    environment: "development"
    auto-deploy: "true"
spec:
  template:
    metadata:
      labels:
        version: development
      annotations:
        # Enable verbose logging for development
        logging.level: "debug"
        # Development-specific monitoring
        prometheus.io/port: "9090"
    spec:
      containers:
      - name: app
        # Development resource limits (lower for cost efficiency)
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
            ephemeral-storage: "500Mi"
          limits:
            memory: "128Mi"
            cpu: "200m"
            ephemeral-storage: "1Gi"
        
        # Development environment variables
        env:
        - name: ENVIRONMENT
          value: "development"
        - name: DEBUG_MODE
          value: "true"
        - name: LOG_LEVEL
          value: "debug"
        
        # Relaxed health check timings for development
        livenessProbe:
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
      
      # Development node selector (may use spot instances)
      nodeSelector:
        kubernetes.io/os: linux
        node-type: development

---
# -----------------------------------------------------------------------------
# FILE: overlays/staging/kustomization.yaml
# OWNER: SRE Team
# PURPOSE: Staging environment that mirrors production
# -----------------------------------------------------------------------------
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

metadata:
  name: generic-app-staging

resources:
- ../../base
- hpa.yaml              # Autoscaling for staging
- monitoring.yaml       # Enhanced monitoring

commonLabels:
  environment: staging
  cost-center: engineering

patchesStrategicMerge:
- deployment-patch.yaml
- service-patch.yaml

images:
- name: placeholder/generic-app
  newTag: staging-v1.2.3

configMapGenerator:
- name: generic-app-environment
  behavior: merge
  literals:
  - ENVIRONMENT=staging
  - LOG_LEVEL=info
  - MONITORING_ENABLED=true

replicas:
- name: generic-app
  count: 2

---
# -----------------------------------------------------------------------------
# FILE: overlays/staging/hpa.yaml
# OWNER: SRE Team
# PURPOSE: Horizontal Pod Autoscaler for staging load testing
# -----------------------------------------------------------------------------
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: generic-app-hpa
  labels:
    app: generic-app
    component: autoscaling
    managed-by: sre-team
  annotations:
    contact.team: "sre-team@company.com"
    autoscaling.policy: "reactive"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: generic-app
  
  # Staging scaling parameters (moderate scaling)
  minReplicas: 2
  maxReplicas: 5
  
  # Multi-metric scaling strategy
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # Scaling behavior to prevent thrashing
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max

---
# -----------------------------------------------------------------------------
# FILE: overlays/production/kustomization.yaml
# OWNER: SRE Team
# PURPOSE: Production environment with full reliability features
# -----------------------------------------------------------------------------
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

metadata:
  name: generic-app-production

resources:
- ../../base
- hpa.yaml                  # Production autoscaling
- pdb.yaml                  # Pod disruption budget
- networkpolicy.yaml        # Security network policies
- monitoring.yaml           # Production monitoring
- alerts.yaml              # Alerting rules

commonLabels:
  environment: production
  cost-center: production
  criticality: high

patchesStrategicMerge:
- deployment-patch.yaml
- service-patch.yaml

# Production images use immutable digests for security
images:
- name: placeholder/generic-app
  digest: sha256:abcd1234567890abcdef1234567890abcdef1234567890abcdef1234567890ab

configMapGenerator:
- name: generic-app-environment
  behavior: merge
  literals:
  - ENVIRONMENT=production
  - LOG_LEVEL=warn
  - MONITORING_ENABLED=true
  - SECURITY_ENHANCED=true

replicas:
- name: generic-app
  count: 3

---
# -----------------------------------------------------------------------------
# FILE: overlays/production/pdb.yaml
# OWNER: SRE Team
# PURPOSE: Ensure high availability during maintenance and failures
# -----------------------------------------------------------------------------
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: generic-app-pdb
  labels:
    app: generic-app
    component: availability
    managed-by: sre-team
  annotations:
    contact.team: "sre-team@company.com"
    availability.target: "99.9%"
spec:
  # Ensure at least 2 pods remain available during disruptions
  minAvailable: 2
  selector:
    matchLabels:
      app: generic-app
      environment: production

---
# -----------------------------------------------------------------------------
# FILE: overlays/production/networkpolicy.yaml
# OWNER: Security Team
# PURPOSE: Production network security and micro-segmentation
# -----------------------------------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: generic-app-netpol
  labels:
    app: generic-app
    component: security
    managed-by: security-team
  annotations:
    contact.team: "security-team@company.com"
    security.policy: "zero-trust"
spec:
  podSelector:
    matchLabels:
      app: generic-app
      environment: production
  
  policyTypes:
  - Ingress
  - Egress
  
  # Ingress rules: Define what can connect to this application
  ingress:
  - from:
    # Allow traffic from ingress controllers
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    # Allow traffic from monitoring systems
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 9090
  
  # Egress rules: Define what this application can connect to
  egress:
  # DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
  
  # HTTPS external services
  - to: []
    ports:
    - protocol: TCP
      port: 443
  
  # Database access (specific namespace)
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
      podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
  
  # Redis cache access
  - to:
    - namespaceSelector:
        matchLabels:
          name: cache
      podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

---
# -----------------------------------------------------------------------------
# FILE: secrets/secret-template.yaml
# OWNER: Security Team
# PURPOSE: Template for application secrets (actual values managed externally)
# NOTE: This file should NOT contain real secrets - use external secret management
# -----------------------------------------------------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: generic-app-secrets
  labels:
    app: generic-app
    component: security
    managed-by: security-team
  annotations:
    contact.team: "security-team@company.com"
    secret.management: "external-secrets-operator"
type: Opaque
stringData:
  # Database credentials (populated by external secret management)
  DATABASE_URL: "placeholder://user:password@host:port/database"
  DATABASE_USERNAME: "placeholder-user"
  DATABASE_PASSWORD: "placeholder-password"
  
  # API keys and tokens
  API_KEY: "placeholder-api-key"
  JWT_SECRET: "placeholder-jwt-secret"
  
  # Third-party service credentials
  REDIS_PASSWORD: "placeholder-redis-password"
  SMTP_PASSWORD: "placeholder-smtp-password"

---
# -----------------------------------------------------------------------------
# FILE: secrets/.env.template
# OWNER: Development Team
# PURPOSE: Environment variables template for local development
# NOTE: Copy to .env and fill with actual values (never commit .env to git)
# -----------------------------------------------------------------------------
# Application Configuration
APP_NAME=generic-app
ENVIRONMENT=development
LOG_LEVEL=debug
DEBUG_MODE=true

# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/appdb
DATABASE_MAX_CONNECTIONS=10
DATABASE_TIMEOUT=30s

# Cache Configuration  
REDIS_URL=redis://localhost:6379
REDIS_PASSWORD=your-redis-password

# Security Configuration
JWT_SECRET=your-jwt-secret-key
API_KEY=your-api-key
ENCRYPTION_KEY=your-encryption-key

# External Services
SMTP_HOST=smtp.company.com
SMTP_PORT=587
SMTP_USERNAME=app@company.com
SMTP_PASSWORD=your-smtp-password

# Monitoring Configuration
METRICS_ENABLED=true
TRACING_ENABLED=true
JAEGER_ENDPOINT=http://localhost:14268/api/traces

---
# -----------------------------------------------------------------------------
# FILE: secrets/sealed-secret-example.yaml
# OWNER: Security Team
# PURPOSE: Example of using Sealed Secrets for GitOps-friendly secret management
# NOTE: Requires Sealed Secrets Controller in cluster
# -----------------------------------------------------------------------------
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: generic-app-secrets
  namespace: default
  labels:
    app: generic-app
    component: security
    managed-by: security-team
spec:
  # Encrypted secrets that can safely be stored in Git
  encryptedData:
    DATABASE_PASSWORD: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEQAx...
    API_KEY: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEQAx...
    JWT_SECRET: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEQAx...
  template:
    metadata:
      name: generic-app-secrets
      namespace: default
      labels:
        app: generic-app
    type: Opaque

---
# -----------------------------------------------------------------------------
# FILE: secrets/external-secret-example.yaml
# OWNER: Security Team
# PURPOSE: External Secrets Operator configuration for enterprise secret management
# NOTE: Requires External Secrets Operator and connection to secret backend
# -----------------------------------------------------------------------------
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: generic-app-secrets
  labels:
    app: generic-app
    component: security
    managed-by: security-team
  annotations:
    contact.team: "security-team@company.com"
spec:
  # Refresh secrets every 15 minutes
  refreshInterval: 15m
  
  # Reference to secret store (configured separately)
  secretStoreRef:
    name: vault-secret-store
    kind: SecretStore
  
  # Target secret to create in Kubernetes
  target:
    name: generic-app-secrets
    creationPolicy: Owner
    deletionPolicy: Retain
  
  # Map external secrets to Kubernetes secret keys
  data:
  - secretKey: DATABASE_PASSWORD
    remoteRef:
      key: apps/generic-app/database
      property: password
  
  - secretKey: API_KEY
    remoteRef:
      key: apps/generic-app/api
      property: key
  
  - secretKey: JWT_SECRET
    remoteRef:
      key: apps/generic-app/auth
      property: jwt_secret

---
# -----------------------------------------------------------------------------
# FILE: monitoring/servicemonitor.yaml
# OWNER: SRE Team
# PURPOSE: Prometheus monitoring configuration for application metrics
# -----------------------------------------------------------------------------
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: generic-app-metrics
  labels:
    app: generic-app
    component: monitoring
    managed-by: sre-team
  annotations:
    contact.team: "sre-team@company.com"
spec:
  selector:
    matchLabels:
      app: generic-app
  
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    
    # Metric relabeling for consistent naming
    metricRelabelings:
    - sourceLabels: [__name__]
      regex: 'app_(.+)'
      targetLabel: __name__
      replacement: 'generic_app_${1}'
    
    # Add environment label to all metrics
    - targetLabel: environment
      replacement: '{{ .Values.environment }}'

---
# -----------------------------------------------------------------------------
# FILE: monitoring/prometheusrule.yaml
# OWNER: SRE Team
# PURPOSE: Alerting rules for application monitoring and SLA enforcement
# -----------------------------------------------------------------------------
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: generic-app-alerts
  labels:
    app: generic-app
    component: alerting
    managed-by: sre-team
spec:
  groups:
  - name: generic-app.availability
    interval: 30s
    rules:
    # High availability alerts based on SLA requirements
    - alert: GenericAppHighErrorRate
      expr: |
        (
          rate(http_requests_total{app="generic-app",status=~"5.."}[5m]) /
          rate(http_requests_total{app="generic-app"}[5m])
        ) > 0.05
      for: 2m
      labels:
        severity: critical
        team: sre
        service: generic-app
      annotations:
        summary: "Generic App experiencing high error rate"
        description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
        runbook_url: "https://wiki.company.com/runbooks/generic-app/high-error-rate"
    
    - alert: GenericAppHighLatency
      expr: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket{app="generic-app"}[5m])
        ) > 1.0
      for: 5m
      labels:
        severity: warning
        team: sre
        service: generic-app
      annotations:
        summary: "Generic App pod using high memory"
        description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit"

---
# -----------------------------------------------------------------------------
# FILE: scripts/deploy.sh
# OWNER: Platform Team
# PURPOSE: Deployment script with safety checks and rollback capabilities
# -----------------------------------------------------------------------------
#!/bin/bash
set -euo pipefail

# =============================================================================
# DEPLOYMENT SCRIPT FOR GENERIC APP
# =============================================================================
# Usage: ./deploy.sh <environment> [image-tag] [--dry-run] [--rollback]
# Examples:
#   ./deploy.sh development
#   ./deploy.sh staging v1.2.3
#   ./deploy.sh production v1.2.3 --dry-run
#   ./deploy.sh production --rollback
# =============================================================================

# Script configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
K8S_DIR="$(dirname "$SCRIPT_DIR")"
APP_NAME="generic-app"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

# Usage function
usage() {
    cat << EOF
Usage: $0 <environment> [image-tag] [options]

Environments:
    development     Deploy to development environment
    staging         Deploy to staging environment  
    production      Deploy to production environment

Options:
    --dry-run       Show what would be deployed without applying
    --rollback      Rollback to previous deployment
    --force         Skip safety checks (not recommended for production)
    --help          Show this help message

Examples:
    $0 development
    $0 staging v1.2.3
    $0 production v1.2.3 --dry-run
    $0 production --rollback
EOF
}

# Validate environment
validate_environment() {
    local env=$1
    case $env in
        development|staging|production)
            return 0
            ;;
        *)
            log_error "Invalid environment: $env"
            log_error "Valid environments: development, staging, production"
            exit 1
            ;;
    esac
}

# Check prerequisites
check_prerequisites() {
    local required_tools=("kubectl" "kustomize" "jq")
    
    for tool in "${required_tools[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            log_error "Required tool not found: $tool"
            exit 1
        fi
    done
    
    # Check kubectl connectivity
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi
    
    log_info "Prerequisites check passed"
}

# Production safety checks
production_safety_checks() {
    if [[ "$ENVIRONMENT" == "production" && "$FORCE" != "true" ]]; then
        log_warn "Production deployment safety checks..."
        
        # Check if image tag is provided for production
        if [[ -z "$IMAGE_TAG" ]]; then
            log_error "Image tag is required for production deployments"
            exit 1
        fi
        
        # Check if image tag follows semantic versioning
        if [[ ! "$IMAGE_TAG" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            log_warn "Image tag '$IMAGE_TAG' doesn't follow semantic versioning (v1.2.3)"
            read -p "Continue anyway? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                exit 1
            fi
        fi
        
        # Confirm production deployment
        log_warn "You are about to deploy to PRODUCTION environment"
        log_warn "Environment: $ENVIRONMENT"
        log_warn "Image: $IMAGE_TAG"
        read -p "Are you sure? Type 'yes' to continue: " -r
        if [[ $REPLY != "yes" ]]; then
            log_info "Deployment cancelled"
            exit 0
        fi
    fi
}

# Backup current state for rollback
backup_current_state() {
    local backup_dir="$K8S_DIR/backups"
    local timestamp=$(date +%Y%m%d-%H%M%S)
    local backup_file="$backup_dir/${APP_NAME}-${ENVIRONMENT}-${timestamp}.yaml"
    
    mkdir -p "$backup_dir"
    
    log_info "Backing up current state to $backup_file"
    kubectl get deployment "$APP_NAME" -o yaml > "$backup_file" 2>/dev/null || true
    
    # Keep only last 10 backups
    ls -t "$backup_dir"/${APP_NAME}-${ENVIRONMENT}-*.yaml | tail -n +11 | xargs -r rm
}

# Perform rollback
perform_rollback() {
    log_info "Rolling back deployment for $APP_NAME in $ENVIRONMENT"
    
    if ! kubectl rollout undo deployment/"$APP_NAME"; then
        log_error "Rollback failed"
        exit 1
    fi
    
    log_info "Waiting for rollback to complete..."
    kubectl rollout status deployment/"$APP_NAME" --timeout=300s
    log_success "Rollback completed successfully"
}

# Deploy application
deploy_application() {
    local overlay_dir="$K8S_DIR/overlays/$ENVIRONMENT"
    
    if [[ ! -d "$overlay_dir" ]]; then
        log_error "Environment overlay not found: $overlay_dir"
        exit 1
    fi
    
    # Update image tag if provided
    if [[ -n "$IMAGE_TAG" ]]; then
        log_info "Setting image tag to: $IMAGE_TAG"
        cd "$overlay_dir"
        kustomize edit set image "placeholder/generic-app:$IMAGE_TAG"
        cd - > /dev/null
    fi
    
    # Backup current state
    backup_current_state
    
    # Generate manifests
    log_info "Generating Kubernetes manifests..."
    local manifests
    manifests=$(kustomize build "$overlay_dir")
    
    if [[ "$DRY_RUN" == "true" ]]; then
        log_info "DRY RUN - Manifests that would be applied:"
        echo "$manifests"
        return 0
    fi
    
    # Apply manifests
    log_info "Applying manifests to $ENVIRONMENT environment..."
    echo "$manifests" | kubectl apply -f -
    
    # Wait for deployment to complete
    log_info "Waiting for deployment to complete..."
    if ! kubectl rollout status deployment/"$APP_NAME" --timeout=300s; then
        log_error "Deployment failed - consider rolling back"
        exit 1
    fi
    
    # Verify deployment
    verify_deployment
    
    log_success "Deployment completed successfully!"
}

# Verify deployment health
verify_deployment() {
    log_info "Verifying deployment health..."
    
    # Check pod status
    local ready_pods
    ready_pods=$(kubectl get pods -l app="$APP_NAME" -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
    local desired_replicas
    desired_replicas=$(kubectl get deployment "$APP_NAME" -o jsonpath='{.spec.replicas}')
    
    if [[ "$ready_pods" -lt "$desired_replicas" ]]; then
        log_warn "Not all pods are ready: $ready_pods/$desired_replicas"
    else
        log_success "All pods are ready: $ready_pods/$desired_replicas"
    fi
    
    # Check service endpoints
    local endpoints
    endpoints=$(kubectl get endpoints "${APP_NAME}-service" -o jsonpath='{.subsets[0].addresses}' | jq length 2>/dev/null || echo "0")
    
    if [[ "$endpoints" -gt 0 ]]; then
        log_success "Service endpoints are healthy: $endpoints"
    else
        log_warn "No healthy service endpoints found"
    fi
    
    # Display deployment info
    log_info "Deployment summary:"
    kubectl get deployment "$APP_NAME" -o wide
    kubectl get pods -l app="$APP_NAME" -o wide
}

# Main execution
main() {
    # Parse arguments
    ENVIRONMENT=""
    IMAGE_TAG=""
    DRY_RUN="false"
    ROLLBACK="false"
    FORCE="false"
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --dry-run)
                DRY_RUN="true"
                shift
                ;;
            --rollback)
                ROLLBACK="true"
                shift
                ;;
            --force)
                FORCE="true"
                shift
                ;;
            --help)
                usage
                exit 0
                ;;
            -*)
                log_error "Unknown option: $1"
                usage
                exit 1
                ;;
            *)
                if [[ -z "$ENVIRONMENT" ]]; then
                    ENVIRONMENT="$1"
                elif [[ -z "$IMAGE_TAG" ]]; then
                    IMAGE_TAG="$1"
                else
                    log_error "Too many arguments"
                    usage
                    exit 1
                fi
                shift
                ;;
        esac
    done
    
    # Validate required arguments
    if [[ -z "$ENVIRONMENT" ]]; then
        log_error "Environment is required"
        usage
        exit 1
    fi
    
    validate_environment "$ENVIRONMENT"
    check_prerequisites
    
    # Set kubectl context based on environment
    case $ENVIRONMENT in
        development)
            kubectl config use-context dev-cluster || log_warn "Could not switch to dev-cluster context"
            ;;
        staging)
            kubectl config use-context staging-cluster || log_warn "Could not switch to staging-cluster context"
            ;;
        production)
            kubectl config use-context prod-cluster || log_warn "Could not switch to prod-cluster context"
            ;;
    esac
    
    log_info "Deploying $APP_NAME to $ENVIRONMENT environment"
    log_info "Current kubectl context: $(kubectl config current-context)"
    
    if [[ "$ROLLBACK" == "true" ]]; then
        perform_rollback
    else
        production_safety_checks
        deploy_application
    fi
}

# Execute main function
main "$@"

---
# -----------------------------------------------------------------------------
# FILE: scripts/health-check.sh
# OWNER: SRE Team
# PURPOSE: Comprehensive health check script for deployment verification
# -----------------------------------------------------------------------------
#!/bin/bash
set -euo pipefail

# =============================================================================
# HEALTH CHECK SCRIPT FOR GENERIC APP
# =============================================================================
# Usage: ./health-check.sh <environment> [--verbose] [--timeout=300]
# =============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
APP_NAME="generic-app"
TIMEOUT=300
VERBOSE=false

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }

# Health check functions
check_deployment_status() {
    log_info "Checking deployment status..."
    
    local available_replicas
    available_replicas=$(kubectl get deployment "$APP_NAME" -o jsonpath='{.status.availableReplicas}' 2>/dev/null || echo "0")
    local desired_replicas
    desired_replicas=$(kubectl get deployment "$APP_NAME" -o jsonpath='{.spec.replicas}')
    
    if [[ "$available_replicas" == "$desired_replicas" ]]; then
        log_success "Deployment is healthy: $available_replicas/$desired_replicas replicas available"
        return 0
    else
        log_error "Deployment is unhealthy: $available_replicas/$desired_replicas replicas available"
        return 1
    fi
}

check_pod_health() {
    log_info "Checking pod health..."
    
    local unhealthy_pods=0
    while IFS= read -r pod; do
        local phase
        phase=$(kubectl get pod "$pod" -o jsonpath='{.status.phase}')
        local ready
        ready=$(kubectl get pod "$pod" -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
        
        if [[ "$phase" != "Running" || "$ready" != "True" ]]; then
            log_error "Pod $pod is unhealthy: phase=$phase, ready=$ready"
            ((unhealthy_pods++))
            
            if [[ "$VERBOSE" == "true" ]]; then
                kubectl describe pod "$pod"
            fi
        elif [[ "$VERBOSE" == "true" ]]; then
            log_success "Pod $pod is healthy"
        fi
    done < <(kubectl get pods -l app="$APP_NAME" -o jsonpath='{.items[*].metadata.name}')
    
    if [[ $unhealthy_pods -eq 0 ]]; then
        log_success "All pods are healthy"
        return 0
    else
        log_error "$unhealthy_pods unhealthy pods found"
        return 1
    fi
}

check_service_endpoints() {
    log_info "Checking service endpoints..."
    
    local service_name="${APP_NAME}-service"
    local endpoints
    endpoints=$(kubectl get endpoints "$service_name" -o jsonpath='{.subsets[0].addresses}' 2>/dev/null || echo "[]")
    local endpoint_count
    endpoint_count=$(echo "$endpoints" | jq length 2>/dev/null || echo "0")
    
    if [[ "$endpoint_count" -gt 0 ]]; then
        log_success "Service has $endpoint_count healthy endpoints"
        return 0
    else
        log_error "Service has no healthy endpoints"
        return 1
    fi
}

check_application_health() {
    log_info "Checking application health endpoints..."
    
    # Port-forward to check health endpoints
    local pod
    pod=$(kubectl get pods -l app="$APP_NAME" -o jsonpath='{.items[0].metadata.name}')
    
    if [[ -z "$pod" ]]; then
        log_error "No pods found for health check"
        return 1
    fi
    
    # Check startup endpoint
    if kubectl exec "$pod" -- curl -sf http://localhost:8080/health/startup &>/dev/null; then
        log_success "Startup health check passed"
    else
        log_error "Startup health check failed"
        return 1
    fi
    
    # Check liveness endpoint  
    if kubectl exec "$pod" -- curl -sf http://localhost:8080/health/live &>/dev/null; then
        log_success "Liveness health check passed"
    else
        log_error "Liveness health check failed"
        return 1
    fi
    
    # Check readiness endpoint
    if kubectl exec "$pod" -- curl -sf http://localhost:8080/health/ready &>/dev/null; then
        log_success "Readiness health check passed"
    else
        log_error "Readiness health check failed"
        return 1
    fi
    
    return 0
}

check_metrics() {
    log_info "Checking metrics endpoint..."
    
    local pod
    pod=$(kubectl get pods -l app="$APP_NAME" -o jsonpath='{.items[0].metadata.name}')
    
    if kubectl exec "$pod" -- curl -sf http://localhost:9090/metrics &>/dev/null; then
        log_success "Metrics endpoint is accessible"
        return 0
    else
        log_error "Metrics endpoint is not accessible"
        return 1
    fi
}

# Main health check
main() {
    local environment=""
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --verbose)
                VERBOSE=true
                shift
                ;;
            --timeout=*)
                TIMEOUT="${1#*=}"
                shift
                ;;
            -*)
                log_error "Unknown option: $1"
                exit 1
                ;;
            *)
                environment="$1"
                shift
                ;;
        esac
    done
    
    if [[ -z "$environment" ]]; then
        log_error "Environment is required"
        exit 1
    fi
    
    log_info "Starting health check for $APP_NAME in $environment environment"
    
    local checks_passed=0
    local total_checks=5
    
    check_deployment_status && ((checks_passed++))
    check_pod_health && ((checks_passed++))
    check_service_endpoints && ((checks_passed++))
    check_application_health && ((checks_passed++))
    check_metrics && ((checks_passed++))
    
    log_info "Health check summary: $checks_passed/$total_checks checks passed"
    
    if [[ $checks_passed -eq $total_checks ]]; then
        log_success "All health checks passed! Application is healthy."
        exit 0
    else
        log_error "Some health checks failed. Application may not be fully healthy."
        exit 1
    fi
}

main "$@"

---
# -----------------------------------------------------------------------------
# FILE: docs/README.md
# OWNER: Platform Team
# PURPOSE: Comprehensive documentation for team usage
# -----------------------------------------------------------------------------
        team: sre
        service: generic-app
      annotations:
        summary: "Generic App experiencing high latency"
        description: "95th percentile latency is {{ $value }}s for the last 5 minutes"
        runbook_url: "https://wiki.company.com/runbooks/generic-app/high-latency"
  
  - name: generic-app.infrastructure
    interval: 30s
    rules:
    # Infrastructure and resource alerts
    - alert: GenericAppPodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total{pod=~"generic-app-.*"}[15m]) > 0
      for: 5m
      labels:
        severity: critical
        team: sre
        service: generic-app
      annotations:
        summary: "Generic App pod is crash looping"
        description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"
    
    - alert: GenericAppHighMemoryUsage
      expr: |
        (
          container_memory_working_set_bytes{pod=~"generic-app-.*"} /
          container_spec_memory_limit_bytes{pod=~"generic-app-.*"}
        ) > 0.85
      for: 10m
      labels:
        severity: warning
        