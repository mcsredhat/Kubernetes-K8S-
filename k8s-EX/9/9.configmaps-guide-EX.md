# Complete Hands-On Guide to Kubernetes ConfigMaps
*From Foundation to Production-Ready Implementation*

## 🚀 Quick Start: Your First ConfigMap in 5 Minutes

Let's start with something you can try right now. This quick demo will give you immediate hands-on experience with ConfigMaps.

### Demo 1: Hello ConfigMap World

```bash
# 1. Create your first ConfigMap
kubectl create configmap hello-config \
  --from-literal=greeting="Hello from ConfigMap!" \
  --from-literal=app_version="1.0.0"

# 2. Verify it was created
kubectl get configmap hello-config -o yaml

# 3. Create a pod that uses this ConfigMap
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: hello-configmap-pod
spec:
  containers:
  - name: hello-app
    image: busybox
    command: ["sh", "-c", "echo \$GREETING && echo 'Version:' \$APP_VERSION && sleep 3600"]
    env:
    - name: GREETING
      valueFrom:
        configMapKeyRef:
          name: hello-config
          key: greeting
    - name: APP_VERSION
      valueFrom:
        configMapKeyRef:
          name: hello-config
          key: app_version
EOF

# 4. See it in action
kubectl logs hello-configmap-pod

# 5. Cleanup
kubectl delete pod hello-configmap-pod
kubectl delete configmap hello-config
```

**What you just learned:** You created a ConfigMap with key-value pairs, used them as environment variables in a pod, and saw how configuration data flows from ConfigMap to application.

## 🎯 Foundation Level: Core Concepts with Practical Examples

### Understanding the Problem ConfigMaps Solve

Before diving deeper, let's understand why ConfigMaps exist through a practical scenario. Imagine you're developing a web application that needs different settings for development, testing, and production environments.

**Without ConfigMaps (The Hard Way):**
```dockerfile
# This is what we want to avoid - hardcoded configuration
ENV DATABASE_URL=hardcoded-prod-db:5432
ENV LOG_LEVEL=info
ENV FEATURE_X_ENABLED=false
```

**With ConfigMaps (The Right Way):**
```bash
# Same container image, different behavior per environment
kubectl create configmap dev-config --from-literal=DATABASE_URL=dev-db:5432
kubectl create configmap prod-config --from-literal=DATABASE_URL=prod-db:5432
```

### Demo 2: Three Ways to Create ConfigMaps

Let's explore all three creation methods with practical examples you can follow along.

#### Method 1: From Literal Values (Key-Value Pairs)

```bash
# Create a realistic application configuration
kubectl create configmap webapp-config \
  --from-literal=database_host=postgres.mycompany.com \
  --from-literal=database_port=5432 \
  --from-literal=max_connections=100 \
  --from-literal=api_timeout=30 \
  --from-literal=log_level=info \
  --from-literal=debug_enabled=false \
  --from-literal=cache_ttl=3600

# Inspect what was created
kubectl describe configmap webapp-config
```

**When to use:** Simple configuration parameters, environment variables, feature flags.

#### Method 2: From Configuration Files

```bash
# Create a realistic database configuration file
mkdir -p configmap-demo && cd configmap-demo

cat > database.properties << EOF
# Database Configuration for Production
database.host=prod-postgres.internal
database.port=5432
database.name=myapp_production
database.ssl.enabled=true
database.connection.pool.size=20
database.connection.pool.max=50
database.connection.timeout=30000
database.query.timeout=60000

# Connection validation
database.validation.query=SELECT 1
database.validation.interval=30000
EOF

# Create ConfigMap from the file
kubectl create configmap db-properties --from-file=database.properties

# The file name becomes the key, content becomes the value
kubectl get configmap db-properties -o yaml
```

Now let's create a more complex configuration:

```bash
# Create an nginx configuration
cat > nginx.conf << EOF
events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # Logging format
    log_format main '\$remote_addr - \$remote_user [\$time_local] '
                   '"\$request" \$status \$body_bytes_sent '
                   '"\$http_referer" "\$http_user_agent"';
    
    # Gzip compression
    gzip on;
    gzip_types text/plain text/css application/json application/javascript;
    
    server {
        listen 80;
        server_name localhost;
        
        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
        
        location /api/ {
            proxy_pass http://backend-service:8080/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
        }
        
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
EOF

kubectl create configmap nginx-config --from-file=nginx.conf
```

**When to use:** Existing configuration files, complex structured configurations, when applications expect specific file formats.

#### Method 3: From Environment Files

```bash
# Create environment-style configuration
cat > application.env << EOF
# Application Configuration
APP_NAME=MyWebApp
APP_VERSION=2.1.0
APP_ENVIRONMENT=production
APP_PORT=8080

# Database Configuration
DATABASE_HOST=postgres.cluster.local
DATABASE_PORT=5432
DATABASE_NAME=myapp
DATABASE_SSL_MODE=require
DATABASE_MAX_CONNECTIONS=25

# Redis Configuration
REDIS_HOST=redis.cluster.local
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Feature Flags
FEATURE_NEW_UI=true
FEATURE_ANALYTICS=true
FEATURE_BETA_DASHBOARD=false

# Performance Tuning
HTTP_TIMEOUT=30
HTTP_RETRIES=3
CACHE_TTL=1800
RATE_LIMIT_PER_HOUR=1000
EOF

kubectl create configmap app-env-config --from-env-file=application.env

# Verify all variables were parsed correctly
kubectl get configmap app-env-config -o yaml
```

**When to use:** Environment variable patterns, when migrating from docker-compose or similar tools, bulk configuration loading.

### Demo 3: Consuming ConfigMaps - Environment Variables

Let's create a realistic web application that demonstrates different ways to consume ConfigMap data.

```bash
# Create a comprehensive test pod
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: config-consumer-demo
  labels:
    app: config-demo
spec:
  containers:
  - name: web-app
    image: nginx:alpine
    ports:
    - containerPort: 80
    env:
    # Method 1: Individual key mapping with custom names
    - name: DB_HOST  # Custom environment variable name
      valueFrom:
        configMapKeyRef:
          name: webapp-config
          key: database_host
    - name: DB_PORT
      valueFrom:
        configMapKeyRef:
          name: webapp-config
          key: database_port
    - name: CONNECTION_TIMEOUT
      valueFrom:
        configMapKeyRef:
          name: webapp-config
          key: api_timeout
    # Method 2: Load all keys from ConfigMap
    envFrom:
    - configMapRef:
        name: app-env-config
    # Method 3: Load with prefix to avoid naming conflicts
    - configMapRef:
        name: webapp-config
        prefix: WEBAPP_
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Configuration Loaded ==="
      echo "Database Host: $DB_HOST"
      echo "Database Port: $DB_PORT"
      echo "App Name: $APP_NAME"
      echo "App Version: $APP_VERSION"
      echo "WebApp Database Host (with prefix): $WEBAPP_database_host"
      echo "=== Environment Variables ==="
      env | grep -E "(DB_|APP_|WEBAPP_|FEATURE_)" | sort
      echo "=== Keeping container running ==="
      sleep 3600
EOF

# Test the configuration loading
kubectl logs config-consumer-demo
```

**Interactive Exercise:** Try modifying the ConfigMaps and see how it affects the pod:

```bash
# Update a ConfigMap value
kubectl patch configmap webapp-config -p '{"data":{"log_level":"debug"}}'

# Note: Environment variables don't update automatically
# You need to restart the pod to see changes in environment variables
kubectl delete pod config-consumer-demo
# Re-run the pod creation command above to see the updated value
```

### Demo 4: Consuming ConfigMaps - Volume Mounts

Now let's see how to use ConfigMaps as files in the filesystem:

```bash
# Create a pod that mounts ConfigMaps as files
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: config-files-demo
spec:
  containers:
  - name: app-container
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Configuration Files Available ==="
      echo "Files in /etc/config/:"
      ls -la /etc/config/
      echo
      echo "=== Database Properties Content ==="
      cat /etc/config/database.properties
      echo
      echo "=== Nginx Configuration Content (first 20 lines) ==="
      head -20 /etc/nginx/nginx.conf
      echo
      echo "=== Monitoring for changes (press Ctrl+C to stop) ==="
      while true; do
        echo "$(date): Checking config files..."
        ls -la /etc/config/ | grep database.properties
        sleep 30
      done
    volumeMounts:
    # Mount database properties
    - name: db-config-volume
      mountPath: /etc/config
      readOnly: true
    # Mount nginx configuration
    - name: nginx-config-volume
      mountPath: /etc/nginx/nginx.conf
      subPath: nginx.conf  # Mount only this file, not the whole ConfigMap
      readOnly: true
  volumes:
  - name: db-config-volume
    configMap:
      name: db-properties
      defaultMode: 0644  # File permissions
  - name: nginx-config-volume
    configMap:
      name: nginx-config
EOF

# View the mounted configuration
kubectl logs config-files-demo
```

**Key Insight:** Notice how `subPath` allows you to mount a single file from a ConfigMap, while without it, the entire ConfigMap becomes a directory with files for each key.

## 🔧 Intermediate Level: Practical Mini-Projects

### Mini-Project 1: Multi-Container Application with Shared Configuration

Let's build a realistic scenario: a web application with an Nginx proxy and a backend service, both sharing configuration.

```bash
# Step 1: Create shared configuration
kubectl create configmap shared-config \
  --from-literal=backend_host=backend-service \
  --from-literal=backend_port=8080 \
  --from-literal=proxy_timeout=60 \
  --from-literal=max_body_size=10m \
  --from-literal=log_level=info

# Step 2: Create backend-specific configuration
cat > backend-config.yaml << 'EOF'
server:
  port: 8080
  shutdown: graceful
  
database:
  host: postgres.default.svc.cluster.local
  port: 5432
  name: appdb
  
logging:
  level: info
  format: json
  
features:
  metrics: true
  health_check: true
  cors: true
EOF

kubectl create configmap backend-config --from-file=config.yaml=backend-config.yaml

# Step 3: Create nginx configuration template
cat > nginx-proxy.conf << 'EOF'
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server ${BACKEND_HOST}:${BACKEND_PORT};
    }
    
    server {
        listen 80;
        client_max_body_size ${MAX_BODY_SIZE};
        proxy_connect_timeout ${PROXY_TIMEOUT}s;
        proxy_send_timeout ${PROXY_TIMEOUT}s;
        proxy_read_timeout ${PROXY_TIMEOUT}s;
        
        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
        
        location /health {
            access_log off;
            return 200 "nginx healthy\n";
        }
    }
}
EOF

# We'll use an init container to process the template
kubectl create configmap nginx-template --from-file=nginx.conf.template=nginx-proxy.conf

# Step 4: Deploy the multi-container application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-container-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: multi-container-app
  template:
    metadata:
      labels:
        app: multi-container-app
    spec:
      # Init container to process nginx template
      initContainers:
      - name: nginx-config-processor
        image: busybox
        env:
        - name: BACKEND_HOST
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: backend_host
        - name: BACKEND_PORT
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: backend_port
        - name: PROXY_TIMEOUT
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: proxy_timeout
        - name: MAX_BODY_SIZE
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: max_body_size
        command: ["sh", "-c"]
        args:
        - |
          echo "Processing nginx configuration template..."
          sed -e "s/\${BACKEND_HOST}/$BACKEND_HOST/g" \
              -e "s/\${BACKEND_PORT}/$BACKEND_PORT/g" \
              -e "s/\${PROXY_TIMEOUT}/$PROXY_TIMEOUT/g" \
              -e "s/\${MAX_BODY_SIZE}/$MAX_BODY_SIZE/g" \
              /templates/nginx.conf.template > /processed-config/nginx.conf
          echo "Processed nginx configuration:"
          cat /processed-config/nginx.conf
        volumeMounts:
        - name: nginx-template-volume
          mountPath: /templates
        - name: processed-config
          mountPath: /processed-config
      containers:
      - name: webapp
        image: webapp  # This will be replaced by kustomization
        ports:
        - containerPort: 8080
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  labels:
    app: webapp
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 8080
    name: http
  type: ClusterIP
EOF

# Create deployment script for GitOps pipeline
cat > deploy-gitops.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[GITOPS]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

OVERLAY_PATH="environments/overlays/$ENVIRONMENT"
NAMESPACE="webapp-$ENVIRONMENT"

log_info "🚀 Deploying $ENVIRONMENT environment using GitOps pattern"
log_info "Overlay path: $OVERLAY_PATH"
log_info "Target namespace: $NAMESPACE"

# Check if kustomize is available
if ! command -v kustomize &> /dev/null; then
    log_error "kustomize is not installed. Please install it first."
    log_info "Install instructions: https://kubectl.docs.kubernetes.io/installation/kustomize/"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Preview what will be applied
log_info "📋 Preview of resources to be applied:"
echo "======================================"
kustomize build "$OVERLAY_PATH"
echo "======================================"

# Ask for confirmation (skip in CI/CD)
if [ "${CI:-false}" != "true" ]; then
    read -p "Do you want to apply these changes? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log_info "Deployment cancelled"
        exit 0
    fi
fi

# Apply configurations
log_info "🔧 Applying configurations..."
kustomize build "$OVERLAY_PATH" | kubectl apply -f -

# Apply application deployment
log_info "🚢 Applying application deployment..."
kubectl apply -f applications/webapp-deployment.yaml -n "$NAMESPACE"

# Wait for deployment to be ready
log_info "⏳ Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n "$NAMESPACE"

# Verify deployment
log_info "✅ Verifying deployment..."
READY_REPLICAS=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
DESIRED_REPLICAS=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')

if [ "$READY_REPLICAS" = "$DESIRED_REPLICAS" ]; then
    log_success "🎉 Deployment successful! ($READY_REPLICAS/$DESIRED_REPLICAS replicas ready)"
else
    log_error "❌ Deployment may have issues ($READY_REPLICAS/$DESIRED_REPLICAS replicas ready)"
fi

# Show deployed resources
log_info "📊 Deployed resources in namespace $NAMESPACE:"
kubectl get all,configmaps -n "$NAMESPACE" -l managed-by=kustomize

# Test the application
log_info "🧪 Testing application..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080/health &> /dev/null; then
    log_success "✅ Application health check passed"
else
    log_warning "⚠️  Application health check failed or not implemented"
fi

kill $PORT_FORWARD_PID 2>/dev/null || true

log_success "🎊 GitOps deployment completed for $ENVIRONMENT environment!"
log_info "💡 To see configuration differences between environments:"
echo "   kustomize build environments/overlays/development | grep -A 20 'kind: ConfigMap'"
echo "   kustomize build environments/overlays/production | grep -A 20 'kind: ConfigMap'"
EOF

chmod +x deploy-gitops.sh

# Demo the GitOps pipeline
log_info "🎭 Demonstrating GitOps Configuration Pipeline"

# Install kustomize if not available (for demo purposes)
if ! command -v kustomize &> /dev/null; then
    log_info "Installing kustomize for demo..."
    curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
    sudo mv kustomize /usr/local/bin/ 2>/dev/null || mv kustomize ~/bin/ 2>/dev/null || true
fi

# Deploy to different environments
for env in development staging production; do
    log_info "Deploying to $env environment..."
    CI=true ./deploy-gitops.sh "$env"
    
    # Show configuration differences
    log_info "Configuration for $env environment:"
    kustomize build "environments/overlays/$env" | grep -A 15 -B 2 "kind: ConfigMap" | head -20
    echo ""
done

log_success "🏆 GitOps Pipeline Demo Complete!"

cd .. # Go back to parent directory
EOF

## 🎯 Real-World Production Mini-Project

### Mini-Project 4: Complete Microservices Configuration Management

Let's build a complete microservices application with proper configuration management:

```bash
# Create comprehensive microservices project
mkdir -p microservices-config/{services,shared-config,monitoring,scripts}
cd microservices-config

# Step 1: Create shared infrastructure configuration
cat > shared-config/infrastructure.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: infrastructure-config
data:
  # Database Configuration
  POSTGRES_HOST: "postgres.default.svc.cluster.local"
  POSTGRES_PORT: "5432"
  POSTGRES_SSL_MODE: "require"
  POSTGRES_MAX_CONNECTIONS: "20"
  
  # Redis Configuration
  REDIS_HOST: "redis.default.svc.cluster.local"
  REDIS_PORT: "6379"
  REDIS_MAX_CONNECTIONS: "10"
  
  # Message Queue Configuration
  RABBITMQ_HOST: "rabbitmq.default.svc.cluster.local"
  RABBITMQ_PORT: "5672"
  RABBITMQ_VHOST: "/"
  
  # Monitoring Configuration
  JAEGER_AGENT_HOST: "jaeger-agent"
  JAEGER_AGENT_PORT: "6831"
  PROMETHEUS_GATEWAY: "prometheus-pushgateway:9091"
  
  # Service Mesh Configuration
  ISTIO_ENABLED: "false"
  CIRCUIT_BREAKER_ENABLED: "true"
  RATE_LIMITING_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  # Logging Configuration
  LOG_FORMAT: "json"
  LOG_TIMESTAMP_FORMAT: "rfc3339"
  
  # Metrics Configuration
  METRICS_ENABLED: "true"
  METRICS_PORT: "9090"
  METRICS_PATH: "/metrics"
  
  # Health Check Configuration
  HEALTH_CHECK_PORT: "8080"
  HEALTH_CHECK_PATH: "/health"
  READINESS_CHECK_PATH: "/ready"
  
  # Tracing Configuration
  TRACING_ENABLED: "true"
  TRACING_SAMPLE_RATE: "0.1"
EOF

# Step 2: Create service-specific configurations

# API Gateway Service
cat > services/api-gateway-config.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-gateway-config
data:
  SERVICE_NAME: "api-gateway"
  SERVICE_PORT: "8080"
  
  # Routing Configuration
  USER_SERVICE_URL: "http://user-service:8080"
  ORDER_SERVICE_URL: "http://order-service:8080"
  INVENTORY_SERVICE_URL: "http://inventory-service:8080"
  NOTIFICATION_SERVICE_URL: "http://notification-service:8080"
  
  # Security Configuration
  JWT_SECRET_KEY_ID: "jwt-secret"  # Reference to secret
  CORS_ENABLED: "true"
  CORS_ORIGINS: "*"
  
  # Rate Limiting
  RATE_LIMIT_PER_MINUTE: "1000"
  RATE_LIMIT_BURST: "100"
  
  # Timeout Configuration
  UPSTREAM_TIMEOUT: "30s"
  KEEP_ALIVE_TIMEOUT: "60s"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  labels:
    app: api-gateway
    tier: gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
        tier: gateway
    spec:
      containers:
      - name: api-gateway
        image: hashicorp/http-echo:latest
        args: 
        - -text=API Gateway - $(SERVICE_NAME) running on port $(SERVICE_PORT)
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        envFrom:
        - configMapRef:
            name: infrastructure-config
        - configMapRef:
            name: monitoring-config
        - configMapRef:
            name: api-gateway-config
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# User Service
cat > services/user-service-config.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
data:
  SERVICE_NAME: "user-service"
  SERVICE_PORT: "8080"
  
  # Database Configuration
  USER_DB_NAME: "users"
  USER_DB_POOL_SIZE: "10"
  USER_DB_TIMEOUT: "30s"
  
  # Cache Configuration
  USER_CACHE_TTL: "3600"
  USER_SESSION_TTL: "86400"
  
  # Authentication
  PASSWORD_HASH_ROUNDS: "12"
  SESSION_TOKEN_TTL: "3600"
  REFRESH_TOKEN_TTL: "604800"
  
  # Business Logic
  MAX_LOGIN_ATTEMPTS: "5"
  ACCOUNT_LOCKOUT_DURATION: "1800"
  PASSWORD_RESET_TTL: "3600"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  labels:
    app: user-service
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
        tier: backend
    spec:
      containers:
      - name: user-service
        image: hashicorp/http-echo:latest
        args:
        - -text=User Service - $(SERVICE_NAME) connected to $(USER_DB_NAME) database
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        envFrom:
        - configMapRef:
            name: infrastructure-config
        - configMapRef:
            name: monitoring-config
        - configMapRef:
            name: user-service-config
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Order Service  
cat > services/order-service-config.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: order-service-config
data:
  SERVICE_NAME: "order-service"
  SERVICE_PORT: "8080"
  
  # Database Configuration
  ORDER_DB_NAME: "orders"
  ORDER_DB_POOL_SIZE: "15"
  ORDER_DB_TIMEOUT: "45s"
  
  # External Service Integration
  PAYMENT_SERVICE_URL: "https://api.stripe.com"
  SHIPPING_SERVICE_URL: "https://api.fedex.com"
  INVENTORY_CHECK_TIMEOUT: "10s"
  
  # Business Rules
  ORDER_EXPIRY_MINUTES: "30"
  MAX_ORDER_ITEMS: "50"
  ORDER_PROCESSING_DELAY: "5s"
  
  # Queue Configuration
  ORDER_QUEUE_NAME: "orders"
  ORDER_DLQ_NAME: "orders-dlq"
  QUEUE_PREFETCH_COUNT: "10"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  labels:
    app: order-service
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
        tier: backend
    spec:
      containers:
      - name: order-service
        image: hashicorp/http-echo:latest
        args:
        - -text=Order Service - $(SERVICE_NAME) processing orders with $(MAX_ORDER_ITEMS) max items
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        envFrom:
        - configMapRef:
            name: infrastructure-config
        - configMapRef:
            name: monitoring-config
        - configMapRef:
            name: order-service-config
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Step 3: Create comprehensive monitoring and observability
cat > monitoring/observability-stack.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
    
    - job_name: 'microservices'
      static_configs:
      - targets: 
        - 'api-gateway:9090'
        - 'user-service:9090'  
        - 'order-service:9090'
      scrape_interval: 10s
      metrics_path: /metrics
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
data:
  microservices-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Microservices Overview",
        "tags": ["microservices"],
        "style": "dark",
        "timezone": "browser",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total[5m])",
                "legendFormat": "{{service}}"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              }
            ]
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "5s"
      }
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alerting-rules
data:
  microservices.rules: |
    groups:
    - name: microservices
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} for service {{ $labels.service }}"
      
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for service {{ $labels.service }}"
      
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "Service {{ $labels.instance }} is down"
EOF

# Step 4: Create deployment and management scripts
cat > scripts/deploy-microservices.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-microservices-$ENVIRONMENT}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[DEPLOY]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying microservices to $ENVIRONMENT environment"
log_info "Target namespace: $NAMESPACE"

# Create namespace
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
    kubectl label namespace "$NAMESPACE" environment="$ENVIRONMENT"
fi

# Deploy shared infrastructure configuration
log_info "📦 Deploying shared infrastructure configuration..."
kubectl apply -f shared-config/infrastructure.yaml -n "$NAMESPACE"

# Deploy monitoring configuration
log_info "📊 Deploying monitoring configuration..."
kubectl apply -f monitoring/observability-stack.yaml -n "$NAMESPACE"

# Deploy services
log_info "🚢 Deploying microservices..."
services=("api-gateway" "user-service" "order-service")

for service in "${services[@]}"; do
    log_info "Deploying $service..."
    kubectl apply -f "services/${service}-config.yaml" -n "$NAMESPACE"
    
    # Wait for deployment to be ready
    log_info "Waiting for $service deployment to be ready..."
    kubectl wait --for=condition=available --timeout=300s deployment/"$service" -n "$NAMESPACE"
    
    READY=$(kubectl get deployment "$service" -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment "$service" -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    
    if [ "$READY" = "$DESIRED" ]; then
        log_success "✅ $service deployed successfully ($READY/$DESIRED replicas)"
    else
        log_warning "⚠️  $service deployment may have issues ($READY/$DESIRED replicas)"
    fi
done

# Create services for inter-service communication
log_info "🔗 Creating services for inter-service communication..."
for service in "${services[@]}"; do
    cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: $service
  namespace: $NAMESPACE
  labels:
    app: $service
spec:
  selector:
    app: $service
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  - port: 9090
    targetPort: 9090
    name: metrics
EOF
done

# Verify deployment
log_info "🔍 Verifying deployment..."
kubectl get all -n "$NAMESPACE" -l tier

# Test service connectivity
log_info "🧪 Testing service connectivity..."
API_GATEWAY_POD=$(kubectl get pods -n "$NAMESPACE" -l app=api-gateway -o jsonpath='{.items[0].metadata.name}')

if [ -n "$API_GATEWAY_POD" ]; then
    log_info "Testing from API Gateway pod..."
    for service in "user-service" "order-service"; do
        if kubectl exec -n "$NAMESPACE" "$API_GATEWAY_POD" -- wget -qO- "http://$service:8080" --timeout=5 &> /dev/null; then
            log_success "✅ $service is reachable from API Gateway"
        else
            log_warning "⚠️  $service may not be reachable from API Gateway"
        fi
    done
fi

log_success "🎉 Microservices deployment completed!"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   Services deployed: ${services[*]}"
echo ""
log_info "🔍 To explore the deployment:"
echo "   kubectl get all -n $NAMESPACE"
echo "   kubectl logs deployment/api-gateway -n $NAMESPACE"
echo "   kubectl describe configmap infrastructure-config -n $NAMESPACE"
EOF

chmod +x scripts/deploy-microservices.sh

# Step 5: Create configuration testing and validation
cat > scripts/test-configurations.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-microservices-development}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing microservices configuration in namespace: $NAMESPACE"

# Test 1: Verify all ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
configmaps=("infrastructure-config" "monitoring-config" "api-gateway-config" "user-service-config" "order-service-config")

for cm in "${configmaps[@]}"; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Verify services can access configuration
log_info "Test 2: Verifying services can access configuration..."
services=("api-gateway" "user-service" "order-service")

for service in "${services[@]}"; do
    POD=$(kubectl get pods -n "$NAMESPACE" -l app="$service" -o jsonpath='{.items[0].metadata.name}')
    if [ -n "$POD" ]; then
        # Test environment variables are loaded
        if kubectl exec -n "$NAMESPACE" "$POD" -- env | grep -q "SERVICE_NAME"; then
            log_success "$service has SERVICE_NAME configured"
        else
            log_error "$service missing SERVICE_NAME configuration"
        fi
        
        # Test infrastructure config is loaded
        if kubectl exec -n "$NAMESPACE" "$POD" -- env | grep -q "POSTGRES_HOST"; then
            log_success "$service has infrastructure configuration loaded"
        else
            log_error "$service missing infrastructure configuration"
        fi
    else
        log_error "No pods found for service $service"
    fi
done

# Test 3: Test service-to-service configuration consistency
log_info "Test 3: Testing service-to-service configuration consistency..."
API_POD=$(kubectl get pods -n "$NAMESPACE" -l app=api-gateway -o jsonpath='{.items[0].metadata.name}')

if [ -n "$API_POD" ]; then
    USER_SERVICE_URL=$(kubectl exec -n "$NAMESPACE" "$API_POD" -- env | grep USER_SERVICE_URL | cut -d'=' -f2)
    ORDER_SERVICE_URL=$(kubectl exec -n "$NAMESPACE" "$API_POD" -- env | grep ORDER_SERVICE_URL | cut -d'=' -f2)
    
    if [[ "$USER_SERVICE_URL" == "http://user-service:8080" ]]; then
        log_success "User service URL correctly configured"
    else
        log_error "User service URL misconfigured: $USER_SERVICE_URL"
    fi
    
    if [[ "$ORDER_SERVICE_URL" == "http://order-service:8080" ]]; then
        log_success "Order service URL correctly configured"
    else
        log_error "Order service URL misconfigured: $ORDER_SERVICE_URL"
    fi
fi

# Test 4: Verify monitoring configuration
log_info "Test 4: Verifying monitoring configuration..."
for service in "${services[@]}"; do
    POD=$(kubectl get pods -n "$NAMESPACE" -l app="$service" -o jsonpath='{.items[0].metadata.name}')
    if [ -n "$POD" ]; then
        METRICS_ENABLED=$(kubectl exec -n "$NAMESPACE" "$POD" -- env | grep METRICS_ENABLED | cut -d'=' -f2)
        if [[ "$METRICS_ENABLED" == "true" ]]; then
            log_success "$service has metrics enabled"
        else
            log_error "$service metrics not enabled"
        fi
    fi
done

# Test 5: Configuration consistency across replicas
log_info "Test 5: Testing configuration consistency across replicas..."
for service in "${services[@]}"; do
    PODS=($(kubectl get pods -n "$NAMESPACE" -l app="$service" -o jsonpath='{.items[*].metadata.name}'))
    
    if [ ${#PODS[@]} -gt 1 ]; then
        log_info "Testing $service replicas..."
        FIRST_CONFIG=$(kubectl exec -n "$NAMESPACE" "${PODS[0]}" -- env | grep SERVICE_NAME | cut -d'=' -f2)
        
        for pod in "${PODS[@]:1}"; do
            POD_CONFIG=$(kubectl exec -n "$NAMESPACE" "$pod" -- env | grep SERVICE_NAME | cut -d'=' -f2)
            if [[ "$FIRST_CONFIG" == "$POD_CONFIG" ]]; then
                log_success "$service configuration consistent across replicas"
            else
                log_error "$service configuration inconsistent: $FIRST_CONFIG vs $POD_CONFIG"
            fi
        done
    fi
done

log_success "🎊 All configuration tests completed!"
EOF

chmod +x scripts/test-configurations.sh

# Step 6: Demo the complete microservices system
log_info "🎭 Demonstrating Complete Microservices Configuration Management"

# Deploy the microservices
./scripts/deploy-microservices.sh development

# Test the configuration
./scripts/test-configurations.sh microservices-development

# Show configuration hierarchy
log_info "📊 Configuration Hierarchy Visualization:"
echo "
🏗️  Infrastructure Layer (Shared)
├── Database connections (PostgreSQL, Redis)
├── Message queue (RabbitMQ)
├── Service discovery
└── Observability (Jaeger, Prometheus)

📊 Monitoring Layer (Shared)
├── Logging configuration  
├── Metrics collection
├── Health checks
└── Distributed tracing

🚪 API Gateway Service
├── Routing to microservices
├── Authentication & authorization
├── Rate limiting
└── CORS configuration

👥 User Service
├── User database settings
├── Authentication configuration
├── Session management
└── Security policies

📦 Order Service
├── Order database settings
├── Payment integration
├── Inventory checks
└── Queue processing
"

# Show how to manage configuration updates
log_info "💡 Configuration Management Examples:"

# Example 1: Update database connection pool
log_info "Example 1: Updating database connection pool size..."
kubectl patch configmap infrastructure-config -n microservices-development -p '{"data":{"POSTGRES_MAX_CONNECTIONS":"30"}}'

# Example 2: Toggle feature flag
log_info "Example 2: Updating service configuration..."
kubectl patch configmap order-service-config -n microservices-development -p '{"data":{"MAX_ORDER_ITEMS":"100"}}'

# Show restart is needed for env vars
log_info "Note: Restart deployments to pick up environment variable changes:"
echo "kubectl rollout restart deployment/user-service -n microservices-development"
echo "kubectl rollout restart deployment/order-service -n microservices-development"

log_success "🏆 Complete Microservices Configuration Management Demo Finished!"

cd .. # Go back to parent directory
```

## 🛠️ Troubleshooting and Best Practices

### Common ConfigMap Issues and Solutions

Let's create a comprehensive troubleshooting guide with interactive debugging scenarios:

```bash
# Create troubleshooting toolkit
mkdir -p troubleshooting-toolkit/{scenarios,tools,solutions}
cd troubleshooting-toolkit

# Scenario 1: ConfigMap Not Found Error
cat > scenarios/scenario1-missing-configmap.sh << 'EOF'
#!/bin/bash
echo "🚨 SCENARIO 1: ConfigMap Not Found Error"
echo "=========================================="

# Create a pod that references a non-existent ConfigMap
cat << 'YAML' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: broken-pod-missing-cm
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    envFrom:
    - configMapRef:
        name: missing-config  # This ConfigMap doesn't exist
YAML

echo "❌ Pod created with missing ConfigMap reference"
echo "🔍 Debugging steps:"
echo "1. Check pod status: kubectl get pod broken-pod-missing-cm"
echo "2. Check events: kubectl describe pod broken-pod-missing-cm"
echo "3. Look for ConfigMap: kubectl get configmaps"
echo ""
echo "🛠️  Solution: Create the missing ConfigMap or fix the reference"
EOF

# Scenario 2: ConfigMap Key Not Found
cat > scenarios/scenario2-missing-key.sh << 'EOF' 
#!/bin/bash
echo "🚨 SCENARIO 2: ConfigMap Key Not Found"
echo "======================================"

# Create a ConfigMap with limited keys
kubectl create configmap partial-config --from-literal=app_name=MyApp

# Create a pod that references a non-existent key
cat << 'YAML' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: broken-pod-missing-key
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    env:
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: partial-config
          key: app_name          # This exists
    - name: DATABASE_URL
      valueFrom:
        configMapKeyRef:
          name: partial-config
          key: database_url      # This doesn't exist
YAML

echo "❌ Pod created with missing key reference"
echo "🔍 Debugging steps:"
echo "1. Check pod status: kubectl get pod broken-pod-missing-key"
echo "2. Check ConfigMap contents: kubectl describe configmap partial-config"
echo "3. Compare required vs available keys"
echo ""
echo "🛠️  Solution: Add missing keys to ConfigMap or make them optional"
EOF

# Scenario 3: Configuration Not Updating
cat > scenarios/scenario3-config-not-updating.sh << 'EOF'
#!/bin/bash
echo "🚨 SCENARIO 3: Configuration Not Updating"
echo "========================================"

# Create initial ConfigMap
kubectl create configmap update-test-config \
  --from-literal=message="Original Message" \
  --from-literal=version="1.0"

# Create pod with environment variables
cat << 'YAML' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: env-update-test
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "while true; do echo 'Message: '$MESSAGE' Version: '$VERSION; sleep 30; done"]
    env:
    - name: MESSAGE
      valueFrom:
        configMapKeyRef:
          name: update-test-config
          key: message
    - name: VERSION
      valueFrom:
        configMapKeyRef:
          name: update-test-config
          key: version
YAML

# Create pod with volume mounts
cat << 'YAML' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: volume-update-test
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "while true; do echo 'File content:'; cat /config/message; sleep 30; done"]
    volumeMounts:
    - name: config-volume
      mountPath: /config
  volumes:
  - name: config-volume
    configMap:
      name: update-test-config
YAML

echo "✅ Created test pods for configuration update scenario"
echo "📝 Now update the ConfigMap:"
echo "   kubectl patch configmap update-test-config -p '{\"data\":{\"message\":\"Updated Message\",\"version\":\"2.0\"}}'"
echo ""
echo "🔍 Observe the behavior:"
echo "   kubectl logs env-update-test -f      # Environment variables DON'T update"
echo "   kubectl logs volume-update-test -f   # Volume mounts DO update (after ~60s)"
echo ""
echo "🛠️  Solution: Restart pods to pick up environment variable changes"
echo "   kubectl delete pod env-update-test"
EOF

# Create comprehensive debugging toolkit
cat > tools/configmap-debugger.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
POD_NAME=${2}
CONFIGMAP_NAME=${3}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[DEBUG]${NC} $1"; }
log_success() { echo -e "${GREEN}[OK]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

echo "🔍 ConfigMap Debugging Toolkit"
echo "=============================="

# Function to check ConfigMap existence and content
debug_configmap() {
    local cm_name=$1
    local namespace=$2
    
    log_info "Checking ConfigMap: $cm_name in namespace: $namespace"
    
    if kubectl get configmap "$cm_name" -n "$namespace" &> /dev/null; then
        log_success "ConfigMap '$cm_name' exists"
        
        # Show ConfigMap details
        echo ""
        log_info "ConfigMap contents:"
        kubectl describe configmap "$cm_name" -n "$namespace"
        
        # Check size
        local size=$(kubectl get configmap "$cm_name" -n "$namespace" -o json | jq -r '.data | to_entries | map(.value | length) | add')
        if [ "$size" -gt 1000000 ]; then  # 1MB limit
            log_warning "ConfigMap size is large ($size bytes). Consider splitting it."
        else
            log_success "ConfigMap size is acceptable ($size bytes)"
        fi
        
    else
        log_error "ConfigMap '$cm_name' not found"
        log_info "Available ConfigMaps in namespace '$namespace':"
        kubectl get configmaps -n "$namespace"
        return 1
    fi
}

# Function to debug pod ConfigMap usage
debug_pod_configmap() {
    local pod_name=$1
    local namespace=$2
    
    log_info "Debugging pod ConfigMap usage: $pod_name"
    
    if ! kubectl get pod "$pod_name" -n "$namespace" &> /dev/null; then
        log_error "Pod '$pod_name' not found in namespace '$namespace'"
        return 1
    fi
    
    # Check pod status
    local status=$(kubectl get pod "$pod_name" -n "$namespace" -o jsonpath='{.status.phase}')
    log_info "Pod status: $status"
    
    if [ "$status" != "Running" ]; then
        log_warning "Pod is not running. Checking events..."
        kubectl describe pod "$pod_name" -n "$namespace" | grep -A 20 "Events:"
    fi
    
    # Check environment variables
    log_info "Environment variables from ConfigMaps:"
    kubectl exec "$pod_name" -n "$namespace" -- env | grep -v '^PATH\|^HOME\|^PWD' | sort
    
    # Check mounted volumes
    log_info "Mounted ConfigMap volumes:"
    kubectl describe pod "$pod_name" -n "$namespace" | grep -A 10 "Mounts:"
    
    # List files in mounted volumes
    local mounts=$(kubectl get pod "$pod_name" -n "$namespace" -o json | jq -r '.spec.containers[0].volumeMounts[]? | select(.name | contains("config")) | .mountPath')
    
    for mount in $mounts; do
        log_info "Files in mounted volume $mount:"
        kubectl exec "$pod_name" -n "$namespace" -- ls -la "$mount" 2>/dev/null || log_warning "Cannot access $mount"
    done
}

# Function to validate ConfigMap references in pod spec
validate_configmap_references() {
    local pod_name=$1
    local namespace=$2
    
    log_info "Validating ConfigMap references in pod spec..."
    
    # Get pod spec
    local pod_spec=$(kubectl get pod "$pod_name" -n "$namespace" -o json)
    
    # Check environment variable references
    local env_configmaps=$(echo "$pod_spec" | jq -r '.spec.containers[].env[]? | select(.valueFrom.configMapKeyRef) | .valueFrom.configMapKeyRef.name' | sort -u)
    
    for cm in $env_configmaps; do
        if kubectl get configmap "$cm" -n "$namespace" &> /dev/null; then
            log_success "ConfigMap '$cm' (env reference) exists"
        else
            log_error "ConfigMap '$cm' (env reference) not found"
        fi
    done
    
    # Check envFrom references
    local envfrom_configmaps=$(echo "$pod_spec" | jq -r '.spec.containers[].envFrom[]? | select(.configMapRef) | .configMapRef.name' | sort -u)
    
    for cm in $envfrom_configmaps; do
        if kubectl get configmap "$cm" -n "$namespace" &> /dev/null; then
            log_success "ConfigMap '$cm' (envFrom reference) exists"
        else
            log_error "ConfigMap '$cm' (envFrom reference) not found"
        fi
    done
    
    # Check volume references
    local volume_configmaps=$(echo "$pod_spec" | jq -r '.spec.volumes[]? | select(.configMap) | .configMap.name' | sort -u)
    
    for cm in $volume_configmaps; do
        if kubectl get configmap "$cm" -n "$namespace" &> /dev/null; then
            log_success "ConfigMap '$cm' (volume reference) exists"
        else
            log_error "ConfigMap '$cm' (volume reference) not found"
        fi
    done
}

# Function to test ConfigMap key accessibility
test_configmap_keys() {
    local pod_name=$1
    local namespace=$2
    local configmap_name=$3
    
    log_info "Testing ConfigMap key accessibility..."
    
    # Get all keys from ConfigMap
    local keys=$(kubectl get configmap "$configmap_name" -n "$namespace" -o json | jq -r '.data | keys[]')
    
    for key in $keys; do
        # Check if key is accessible as environment variable (try common transformations)
        local env_var_patterns=("$key" "${key^^}" "${key//-/_}" "${key//./_}")
        local found=false
        
        for pattern in "${env_var_patterns[@]}"; do
            if kubectl exec "$pod_name" -n "$namespace" -- env | grep -q "^$pattern="; then
                log_success "Key '$key' accessible as environment variable '$pattern'"
                found=true
                break
            fi
        done
        
        if [ "$found" = false ]; then
            log_warning "Key '$key' not found as environment variable"
        fi
    done
}

# Main debugging flow
if [ -z "$POD_NAME" ] && [ -z "$CONFIGMAP_NAME" ]; then
    log_error "Usage: $0 <namespace> [pod-name] [configmap-name]"
    log_info "Examples:"
    echo "  $0 default                              # List all ConfigMaps in namespace"
    echo "  $0 default my-pod                      # Debug pod ConfigMap usage"
    echo "  $0 default my-pod my-configmap         # Debug specific pod and ConfigMap"
    exit 1
fi

# List all ConfigMaps in namespace
log_info "ConfigMaps in namespace '$NAMESPACE':"
kubectl get configmaps -n "$NAMESPACE"

if [ -n "$CONFIGMAP_NAME" ]; then
    debug_configmap "$CONFIGMAP_NAME" "$NAMESPACE"
fi

if [ -n "$POD_NAME" ]; then
    debug_pod_configmap "$POD_NAME" "$NAMESPACE"
    validate_configmap_references "$POD_NAME" "$NAMESPACE"
    
    if [ -n "$CONFIGMAP_NAME" ]; then
        test_configmap_keys "$POD_NAME" "$NAMESPACE" "$CONFIGMAP_NAME"
    fi
fi

log_success "🎉 Debugging session completed"
EOF

chmod +x tools/configmap-debugger.sh

# Create solutions for common problems
cat > solutions/common-solutions.md << 'EOF'
# ConfigMap Troubleshooting Solutions

## Problem 1: Pod Stuck in Pending State Due to Missing ConfigMap

**Symptoms:**
- Pod status shows "Pending"
- Events show "configmap not found" errors

**Diagnosis:**
```bash
kubectl describe pod <pod-name>
# Look for events mentioning ConfigMap issues
```

**Solutions:**
```bash
# Option 1: Create the missing ConfigMap
kubectl create configmap missing-config --from-literal=key=value

# Option 2: Make ConfigMap reference optional
spec:
  containers:
  - name: app
    envFrom:
    - configMapRef:
        name: missing-config
        optional: true  # Add this line
```

## Problem 2: Environment Variables Not Updating After ConfigMap Change

**Symptoms:**
- ConfigMap updated successfully
- Pod still shows old environment variable values

**Diagnosis:**
```bash
# Check if ConfigMap was updated
kubectl get configmap <name> -o yaml

# Check current environment variables in pod
kubectl exec <pod-name> -- env
```

**Solutions:**
```bash
# Solution 1: Restart the deployment
kubectl rollout restart deployment/<deployment-name>

# Solution 2: Delete and recreate pod
kubectl delete pod <pod-name>

# Solution 3: Use volume mounts instead (auto-updates)
volumeMounts:
- name: config
  mountPath: /etc/config
volumes:
- name: config
  configMap:
    name: my-config
```

## Problem 3: ConfigMap Size Limit Exceeded

**Symptoms:**
- Error: "ConfigMap too large"
- ConfigMap creation fails

**Diagnosis:**
```bash
# Check ConfigMap size
kubectl get configmap <name> -o json | jq '.data | to_entries | map(.value | length) | add'
```

**Solutions:**
```bash
# Solution 1: Split into multiple ConfigMaps
kubectl create configmap config-part1 --from-file=part1/
kubectl create configmap config-part2 --from-file=part2/

# Solution 2: Use external storage (S3, etc.) and reference in ConfigMap
# Solution 3: Compress large text files before storing
```

## Problem 4: Special Characters in ConfigMap Values

**Symptoms:**
- YAML parsing errors
- Unexpected behavior with environment variables

**Diagnosis:**
```bash
# Check for special characters
kubectl get configmap <name> -o yaml | grep -E "[\"'\\|>]"
```

**Solutions:**
```bash
# Use literal style for complex values
apiVersion: v1
kind: ConfigMap
data:
  config.yaml: |
    key: "value with special chars: $, %, @"
    
# Or escape properly in command line
kubectl create configmap test --from-literal='special=value$with%special@chars'
```

## Problem 5: File Permissions Issues with Mounted ConfigMaps

**Symptoms:**
- Application cannot read mounted config files
- Permission denied errors

**Diagnosis:**
```bash
# Check file permissions in pod
kubectl exec <pod-name> -- ls -la /mounted/path/
```

**Solutions:**
```bash
# Set default mode in volume definition
volumes:
- name: config
  configMap:
    name: my-config
    defaultMode: 0644  # or 0755 for executable

# Set specific permissions for individual files
volumes:
- name: config
  configMap:
    name: my-config
    items:
    - key: script.sh
      path: script.sh
      mode: 0755
```
EOF

# Create comprehensive testing scenarios
cat > scenarios/run-all-scenarios.sh << 'EOF'
#!/bin/bash
set -e

echo "🎭 ConfigMap Troubleshooting Scenarios"
echo "====================================="

# Clean up any existing test resources
cleanup() {
    echo "🧹 Cleaning up test resources..."
    kubectl delete pods,configmaps -l scenario=troubleshooting --ignore-not-found=true
    kubectl delete configmap partial-config update-test-config --ignore-not-found=true
}

# Set up cleanup trap
trap cleanup EXIT

echo "🚀 Running troubleshooting scenarios..."

# Scenario 1: Missing ConfigMap
echo -e "\n📋 Scenario 1: Missing ConfigMap"
bash scenarios/scenario1-missing-configmap.sh

sleep 3
echo "🔧 Demonstrating solution..."
kubectl create configmap missing-config --from-literal=app_name="Fixed App"
kubectl get pod broken-pod-missing-cm -o jsonpath='{.status.containerStatuses[0].state}'

# Scenario 2: Missing Key
echo -e "\n📋 Scenario 2: Missing Key" 
bash scenarios/scenario2-missing-key.sh

sleep 3
echo "🔧 Demonstrating solution..."
kubectl patch configmap partial-config -p '{"data":{"database_url":"postgres://localhost:5432/db"}}'
kubectl delete pod broken-pod-missing-key
echo "Pod will be recreated by controller and should start successfully"

# Scenario 3: Configuration Updates
echo -e "\n📋 Scenario 3: Configuration Updates"
bash scenarios/scenario3-config-not-updating.sh

# Wait for pods to be ready
kubectl wait --for=condition=ready pod/env-update-test --timeout=60s
kubectl wait --for=condition=ready pod/volume-update-test --timeout=60s

echo "📊 Initial configuration:"
kubectl logs env-update-test --tail=1
kubectl logs volume-update-test --tail=1

echo "🔄 Updating ConfigMap..."
kubectl patch configmap update-test-config -p '{"data":{"message":"Updated Message","version":"2.0"}}'

echo "⏳ Waiting for volume mount to update (60 seconds)..."
sleep 65

echo "📊 After update:"
kubectl logs env-update-test --tail=1
kubectl logs volume-update-test --tail=1

echo -e "\n✅ All scenarios completed!"
echo "🔍 Use the debugging toolkit: ./tools/configmap-debugger.sh default <pod-name>"
EOF

chmod +x scenarios/run-all-scenarios.sh

# Demo the troubleshooting toolkit
echo "🛠️ ConfigMap Troubleshooting Toolkit Demo"
echo "=========================================="

# Run the scenarios
bash scenarios/run-all-scenarios.sh

# Demonstrate the debugging toolkit
echo -e "\n🔍 Demonstrating debugging toolkit..."
./tools/configmap-debugger.sh default

echo -e "\n💡 Troubleshooting Best Practices:"
echo "1. Always check pod events: kubectl describe pod <name>"
echo "2. Verify ConfigMap exists: kubectl get configmap <name>"
echo "3. Check ConfigMap contents: kubectl describe configmap <name>"
echo "4. Test environment variables: kubectl exec <pod> -- env"
echo "5. Validate YAML syntax before applying"
echo "6. Use optional ConfigMap references when appropriate"
echo "7. Remember: env vars need pod restart, volumes auto-update"
echo "8. Monitor ConfigMap size (1MB limit)"
echo "9. Use proper escaping for special characters"
echo "10. Set appropriate file permissions for mounted configs"

cd .. # Go back to parent directory
```

## 📚 Production-Ready Best Practices

### Best Practice 1: Configuration Validation Pipeline

```bash
# Create production configuration validation system
mkdir -p production-practices/{validation,templates,policies}
cd production-practices

# Create configuration schema validator
cat > validation/config-schema-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-schema-validator
data:
  validate.py: |
    #!/usr/bin/env python3
    import json
    import sys
    import re
    import os
    
    # Configuration schema definitions
    SCHEMAS = {
        'database': {
            'required': ['host', 'port', 'name'],
            'optional': ['ssl_mode', 'timeout', 'pool_size'],
            'validators': {
                'host': r'^[a-zA-Z0-9.-]+
      # Mock backend container
      - name: backend
        image: hashicorp/http-echo:latest
        args:
        - -text=Hello from backend configured via ConfigMap!
        - -listen=:8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: log_level
        volumeMounts:
        - name: backend-config-volume
          mountPath: /etc/config
      # Nginx proxy container
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: processed-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 30
      volumes:
      - name: shared-config-volume
        configMap:
          name: shared-config
      - name: backend-config-volume
        configMap:
          name: backend-config
      - name: nginx-template-volume
        configMap:
          name: nginx-template
      - name: processed-config
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: multi-container-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Step 5: Test the deployment
kubectl get pods -l app=multi-container-app
kubectl logs deployment/multi-container-app -c nginx-config-processor
kubectl logs deployment/multi-container-app -c backend
kubectl logs deployment/multi-container-app -c nginx

# Test the application
kubectl port-forward service/multi-container-service 8080:80 &
curl http://localhost:8080
# Stop port forwarding with: kill %1
```

**What you learned:** How to share configuration between multiple containers, use init containers to process configuration templates, and create realistic multi-tier applications with ConfigMaps.

### Mini-Project 2: Dynamic Configuration Updates

Let's explore ConfigMap hot-reloading with a practical monitoring scenario:

```bash
# Step 1: Create a monitoring configuration
kubectl create configmap monitoring-config \
  --from-literal=scrape_interval=15s \
  --from-literal=evaluation_interval=15s \
  --from-literal=alert_threshold=80 \
  --from-literal=retention_days=30 \
  --from-literal=log_level=info

# Step 2: Create a Prometheus-style configuration file
cat > prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml

# Step 3: Create a configuration-aware application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-watcher-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: config-watcher
  template:
    metadata:
      labels:
        app: config-watcher
    spec:
      containers:
      - name: config-watcher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Configuration Watcher Started"
          echo "Monitoring config files for changes..."
          
          # Function to display current configuration
          show_config() {
            echo "=== Current Configuration ($(date)) ==="
            echo "Environment Variables:"
            env | grep -E "(SCRAPE_|EVALUATION_|ALERT_|RETENTION_|LOG_)" | sort
            echo
            echo "Configuration Files:"
            if [ -f /etc/config/scrape_interval ]; then
              echo "Scrape Interval: $(cat /etc/config/scrape_interval)"
            fi
            if [ -f /etc/config/alert_threshold ]; then
              echo "Alert Threshold: $(cat /etc/config/alert_threshold)"
            fi
            if [ -f /etc/prometheus/prometheus.yml ]; then
              echo "Prometheus Config Present: Yes"
              echo "Config size: $(wc -l < /etc/prometheus/prometheus.yml) lines"
            fi
            echo "================================"
          }
          
          # Show initial configuration
          show_config
          
          # Monitor for file changes
          while true; do
            sleep 30
            show_config
          done
        env:
        - name: SCRAPE_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: scrape_interval
        - name: EVALUATION_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: evaluation_interval
        - name: ALERT_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: alert_threshold
        - name: RETENTION_DAYS
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: retention_days
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: log_level
        volumeMounts:
        - name: monitoring-config-volume
          mountPath: /etc/config
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: monitoring-config-volume
        configMap:
          name: monitoring-config
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
EOF

# Step 4: Watch the initial configuration
kubectl logs deployment/config-watcher-app -f &

# Step 5: Update the configuration and observe changes
echo "Updating scrape interval..."
kubectl patch configmap monitoring-config -p '{"data":{"scrape_interval":"30s","alert_threshold":"90"}}'

echo "Waiting for changes to propagate (up to 60 seconds)..."
sleep 65

# Update the Prometheus configuration
cat > prometheus-updated.yml << 'EOF'
global:
  scrape_interval: 30s  # Updated
  evaluation_interval: 30s  # Updated

rule_files:
  - "alerts.yml"
  - "recording.yml"  # Added

scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 10s  # Override global
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
      
  - job_name: 'kubernetes-services'  # New job
    kubernetes_sd_configs:
    - role: service

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093', 'alertmanager-backup:9093']  # Added backup
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml=prometheus-updated.yml --dry-run=client -o yaml | kubectl apply -f -

echo "Configuration updated. Check the logs for changes..."
# Stop log following with: kill %1
```

**Key Insights from this project:**
- Environment variables from ConfigMaps don't update automatically (require pod restart)
- Volume-mounted ConfigMaps update automatically (with up to 60-second delay)
- Applications should be designed to handle configuration changes gracefully

### Mini-Project 3: Environment-Specific Deployment Pipeline

Let's create a realistic deployment pipeline that manages configurations across different environments:

```bash
# Create directory structure for the project
mkdir -p configmap-pipeline/{environments,applications,scripts}
cd configmap-pipeline

# Step 1: Create environment-specific configurations
cat > environments/development.env << 'EOF'
# Development Environment Configuration
APP_ENVIRONMENT=development
LOG_LEVEL=debug
DATABASE_URL=dev-postgres:5432/myapp_dev
REDIS_URL=dev-redis:6379/0
API_RATE_LIMIT=100
CACHE_TTL=60
EXTERNAL_API_TIMEOUT=5
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

cat > environments/staging.env << 'EOF'
# Staging Environment Configuration
APP_ENVIRONMENT=staging
LOG_LEVEL=info
DATABASE_URL=staging-postgres:5432/myapp_staging
REDIS_URL=staging-redis:6379/0
API_RATE_LIMIT=1000
CACHE_TTL=300
EXTERNAL_API_TIMEOUT=15
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

cat > environments/production.env << 'EOF'
# Production Environment Configuration
APP_ENVIRONMENT=production
LOG_LEVEL=warn
DATABASE_URL=prod-postgres:5432/myapp_production
REDIS_URL=prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Step 2: Create feature flag configurations
cat > environments/dev-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=true
FEATURE_EXPERIMENTAL_API=true
FEATURE_A_B_TESTING=true
EOF

cat > environments/staging-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=true
EOF

cat > environments/prod-features.env << 'EOF'
FEATURE_NEW_UI=false
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=false
EOF

# Step 3: Create deployment script
cat > scripts/deploy-environment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}
APP_VERSION=${3:-latest}

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying to environment: $ENVIRONMENT in namespace: $NAMESPACE"

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Function to create or update ConfigMap
create_or_update_configmap() {
    local name=$1
    local file=$2
    local description=$3
    
    log_info "Creating/updating ConfigMap: $name ($description)"
    
    if kubectl get configmap "$name" -n "$NAMESPACE" &> /dev/null; then
        log_warning "ConfigMap $name exists, updating..."
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl apply -f -
    else
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE"
    fi
}

# Deploy environment-specific configurations
create_or_update_configmap "app-config" "environments/$ENVIRONMENT.env" "Application Configuration"
create_or_update_configmap "feature-flags" "environments/$ENVIRONMENT-features.env" "Feature Flags"

# Create version info ConfigMap
log_info "Creating version information ConfigMap..."
kubectl create configmap version-info \
    --from-literal=version="$APP_VERSION" \
    --from-literal=environment="$ENVIRONMENT" \
    --from-literal=deployed_at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --from-literal=deployed_by="$(whoami)" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

log_success "✅ Configuration deployment complete for $ENVIRONMENT environment"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   App Version: $APP_VERSION"
echo "   ConfigMaps created:"
echo "     - app-config"
echo "     - feature-flags"
echo "     - version-info"

log_info "🔍 To view configurations:"
echo "   kubectl get configmaps -n $NAMESPACE"
echo "   kubectl describe configmap app-config -n $NAMESPACE"
echo "   kubectl describe configmap feature-flags -n $NAMESPACE"
EOF

chmod +x scripts/deploy-environment.sh

# Step 4: Create the application deployment
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: hashicorp/http-echo:latest
        ports:
        - containerPort: 5678
        # Load environment configuration
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        - configMapRef:
            name: version-info
            prefix: VERSION_
        # Custom startup message based on configuration
        args:
        - -text=Hello from $(VERSION_environment) environment! App version $(VERSION_version) deployed at $(VERSION_deployed_at)
        livenessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 5678
  type: ClusterIP
---
# ConfigMap for application monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*webapp*.log
        Parser            docker
        Tag               webapp.*
        Refresh_Interval  5
    
    [OUTPUT]
        Name  stdout
        Match *
---
# Health check and debugging pod
apiVersion: v1
kind: Pod
metadata:
  name: environment-inspector
  labels:
    app: inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "🔍 Environment Inspector Started"
      echo "=================================="
      
      show_environment_info() {
        echo "📊 Environment Information ($(date))"
        echo "Environment: $APP_ENVIRONMENT"
        echo "Version: $VERSION_version"
        echo "Log Level: $LOG_LEVEL"
        echo "Database: $DATABASE_URL"
        echo "Cache TTL: $CACHE_TTL seconds"
        echo
        echo "🚩 Feature Flags:"
        env | grep "FEATURE_" | sort
        echo
        echo "⚙️  System Configuration:"
        echo "API Rate Limit: $API_RATE_LIMIT"
        echo "External API Timeout: $EXTERNAL_API_TIMEOUT seconds"
        echo "Debug Mode: $DEBUG_MODE"
        echo "Metrics Enabled: $METRICS_ENABLED"
        echo "=================================="
      }
      
      # Show initial state
      show_environment_info
      
      # Monitor for changes every 60 seconds
      while true; do
        sleep 60
        show_environment_info
      done
    envFrom:
    - configMapRef:
        name: app-config
    - configMapRef:
        name: feature-flags
        prefix: FEATURE_
    - configMapRef:
        name: version-info
        prefix: VERSION_
EOF

# Step 5: Create testing script
cat > scripts/test-deployment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}

# Color output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing deployment in $ENVIRONMENT environment"

# Test 1: Check ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
for cm in app-config feature-flags version-info; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Check application is running
log_info "Test 2: Verifying application pods are running..."
if kubectl get deployment webapp -n "$NAMESPACE" &> /dev/null; then
    READY=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    if [ "$READY" = "$DESIRED" ]; then
        log_success "Application deployment ready ($READY/$DESIRED pods)"
    else
        log_error "Application deployment not ready ($READY/$DESIRED pods)"
    fi
else
    log_error "Application deployment not found"
fi

# Test 3: Test application response
log_info "Test 3: Testing application response..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080 | grep -q "$ENVIRONMENT"; then
    log_success "Application responds with correct environment"
else
    log_error "Application response incorrect or unreachable"
fi

kill $PORT_FORWARD_PID 2>/dev/null

# Test 4: Check environment inspector
log_info "Test 4: Checking environment inspector..."
if kubectl get pod environment-inspector -n "$NAMESPACE" &> /dev/null; then
    log_success "Environment inspector pod exists"
    log_info "Recent inspector logs:"
    kubectl logs environment-inspector -n "$NAMESPACE" --tail=10
else
    log_error "Environment inspector pod not found"
fi

log_success "🎉 All tests completed for $ENVIRONMENT environment"
EOF

chmod +x scripts/test-deployment.sh

# Step 6: Demo the pipeline
log_info() { echo -e "\033[0;34m[INFO]\033[0m $1"; }

log_info "🚀 Starting Environment Pipeline Demo"

# Deploy to development
log_info "Deploying to development environment..."
./scripts/deploy-environment.sh development webapp-dev v1.0.0

# Apply the application
kubectl apply -f applications/webapp-deployment.yaml -n webapp-dev

# Wait for deployment
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-dev

# Test the deployment
./scripts/test-deployment.sh development webapp-dev

log_info "✅ Development deployment complete!"

# Now let's demonstrate staging deployment
log_info "Deploying to staging environment..."
./scripts/deploy-environment.sh staging webapp-staging v1.0.0

# Apply application to staging
kubectl apply -f applications/webapp-deployment.yaml -n webapp-staging

log_info "Waiting for staging deployment..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-staging

# Test staging
./scripts/test-deployment.sh staging webapp-staging

log_info "✅ Staging deployment complete!"

# Cleanup function
cd .. # Go back to parent directory
```

**What you learned from this project:**
- How to organize environment-specific configurations
- Automated deployment pipelines with ConfigMaps
- Testing and validation strategies
- Real-world deployment patterns

## 🎓 Advanced Level: Production-Ready Patterns

### Advanced Demo 1: ConfigMap Validation and Rollback Strategy

Let's implement a production-ready configuration management system with validation and rollback capabilities:

```bash
# Create advanced configuration management system
mkdir -p advanced-configmaps/{configs,validators,scripts}
cd advanced-configmaps

# Step 1: Create configuration validator
cat > validators/config-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-validator-script
data:
  validate-config.sh: |
    #!/bin/bash
    set -e
    
    CONFIG_FILE=${1:-/config/app.env}
    ERRORS=0
    
    echo "🔍 Validating configuration file: $CONFIG_FILE"
    
    # Check if file exists
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "❌ Configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    # Validation rules
    validate_url() {
        local var_name=$1
        local url=$2
        if [[ ! "$url" =~ ^[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9.-]+:[0-9]+.*$ ]]; then
            echo "❌ Invalid URL format for $var_name: $url"
            ((ERRORS++))
        else
            echo "✅ Valid URL for $var_name"
        fi
    }
    
    validate_number() {
        local var_name=$1
        local number=$2
        local min=${3:-0}
        local max=${4:-999999}
        if ! [[ "$number" =~ ^[0-9]+$ ]] || [ "$number" -lt "$min" ] || [ "$number" -gt "$max" ]; then
            echo "❌ Invalid number for $var_name: $number (must be $min-$max)"
            ((ERRORS++))
        else
            echo "✅ Valid number for $var_name"
        fi
    }
    
    validate_boolean() {
        local var_name=$1
        local bool=$2
        if [[ ! "$bool" =~ ^(true|false)$ ]]; then
            echo "❌ Invalid boolean for $var_name: $bool (must be true/false)"
            ((ERRORS++))
        else
            echo "✅ Valid boolean for $var_name"
        fi
    }
    
    validate_log_level() {
        local level=$1
        if [[ ! "$level" =~ ^(debug|info|warn|error)$ ]]; then
            echo "❌ Invalid log level: $level (must be debug|info|warn|error)"
            ((ERRORS++))
        else
            echo "✅ Valid log level"
        fi
    }
    
    # Parse and validate configuration
    while IFS='=' read -r key value; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ ]] && continue
        [[ -z "$key" ]] && continue
        
        case "$key" in
            "DATABASE_URL"|"REDIS_URL")
                validate_url "$key" "$value"
                ;;
            "DATABASE_PORT"|"REDIS_PORT"|"APP_PORT")
                validate_number "$key" "$value" 1 65535
                ;;
            "API_RATE_LIMIT")
                validate_number "$key" "$value" 1 100000
                ;;
            "CACHE_TTL"|"EXTERNAL_API_TIMEOUT")
                validate_number "$key" "$value" 1 86400
                ;;
            "DEBUG_MODE"|"METRICS_ENABLED"|"PROFILING_ENABLED")
                validate_boolean "$key" "$value"
                ;;
            "LOG_LEVEL")
                validate_log_level "$value"
                ;;
        esac
    done < "$CONFIG_FILE"
    
    echo "📊 Validation complete. Errors found: $ERRORS"
    
    if [ $ERRORS -eq 0 ]; then
        echo "🎉 Configuration validation passed!"
        exit 0
    else
        echo "💥 Configuration validation failed!"
        exit 1
    fi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator
spec:
  template:
    spec:
      containers:
      - name: validator
        image: busybox
        command: ["sh", "/scripts/validate-config.sh"]
        volumeMounts:
        - name: config-to-validate
          mountPath: /config
        - name: validator-script
          mountPath: /scripts
      volumes:
      - name: config-to-validate
        configMap:
          name: app-config-candidate  # The config we want to validate
      - name: validator-script
        configMap:
          name: config-validator-script
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3
EOF

# Step 2: Create configuration rollback system
cat > scripts/config-rollback-manager.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_NAME=${2:-app-config}
ACTION=${3:-backup}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

case "$ACTION" in
    "backup")
        log_info "🔄 Creating backup of ConfigMap: $CONFIG_NAME"
        
        # Get current timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_NAME="${CONFIG_NAME}-backup-${TIMESTAMP}"
        
        # Check if ConfigMap exists
        if ! kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" &> /dev/null; then
            log_error "ConfigMap $CONFIG_NAME not found in namespace $NAMESPACE"
            exit 1
        fi
        
        # Create backup
        kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $CONFIG_NAME/name: $BACKUP_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        kubectl apply -f -
        
        log_success "✅ Backup created: $BACKUP_NAME"
        
        # Label the backup for easy identification
        kubectl label configmap "$BACKUP_NAME" -n "$NAMESPACE" \
            backup-of="$CONFIG_NAME" \
            backup-timestamp="$TIMESTAMP"
        
        echo "Backup ConfigMap: $BACKUP_NAME"
        ;;
        
    "list-backups")
        log_info "📋 Listing backups for ConfigMap: $CONFIG_NAME"
        kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o custom-columns="NAME:.metadata.name,TIMESTAMP:.metadata.labels.backup-timestamp,AGE:.metadata.creationTimestamp"
        ;;
        
    "rollback")
        BACKUP_NAME=$4
        if [ -z "$BACKUP_NAME" ]; then
            log_error "Backup name required for rollback"
            log_info "Usage: $0 <namespace> <config-name> rollback <backup-name>"
            log_info "Use 'list-backups' to see available backups"
            exit 1
        fi
        
        log_warning "⚠️  Rolling back $CONFIG_NAME to backup: $BACKUP_NAME"
        read -p "Are you sure? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Rollback cancelled"
            exit 0
        fi
        
        # Create backup of current state before rollback
        ./config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup
        
        # Perform rollback
        kubectl get configmap "$BACKUP_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $BACKUP_NAME/name: $CONFIG_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        sed '/backup-of:/d' | \
        sed '/backup-timestamp:/d' | \
        kubectl apply -f -
        
        log_success "✅ Rollback completed: $CONFIG_NAME restored from $BACKUP_NAME"
        ;;
        
    "cleanup")
        KEEP_COUNT=${4:-5}
        log_info "🧹 Cleaning up old backups, keeping $KEEP_COUNT most recent"
        
        # Get backup ConfigMaps sorted by timestamp (oldest first)
        BACKUPS=$(kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o jsonpath='{.items[*].metadata.name}')
        
        BACKUP_ARRAY=($BACKUPS)
        TOTAL_BACKUPS=${#BACKUP_ARRAY[@]}
        
        if [ $TOTAL_BACKUPS -le $KEEP_COUNT ]; then
            log_info "Found $TOTAL_BACKUPS backups, keeping all (within limit of $KEEP_COUNT)"
            exit 0
        fi
        
        DELETE_COUNT=$((TOTAL_BACKUPS - KEEP_COUNT))
        log_warning "Found $TOTAL_BACKUPS backups, will delete $DELETE_COUNT oldest"
        
        for ((i=0; i<DELETE_COUNT; i++)); do
            BACKUP_TO_DELETE=${BACKUP_ARRAY[$i]}
            log_info "Deleting backup: $BACKUP_TO_DELETE"
            kubectl delete configmap "$BACKUP_TO_DELETE" -n "$NAMESPACE"
        done
        
        log_success "✅ Cleanup completed, kept $KEEP_COUNT most recent backups"
        ;;
        
    *)
        log_error "Unknown action: $ACTION"
        log_info "Available actions: backup, list-backups, rollback, cleanup"
        exit 1
        ;;
esac
EOF

chmod +x scripts/config-rollback-manager.sh

# Step 3: Create safe configuration update system
cat > scripts/safe-config-update.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_FILE=${2}
CONFIG_NAME=${3:-app-config}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <namespace> <config-file> [config-name]"
    echo "Example: $0 default new-config.env app-config"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[SAFE-UPDATE]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🛡️  Starting safe configuration update process"
log_info "Target: $CONFIG_NAME in namespace $NAMESPACE"
log_info "Source: $CONFIG_FILE"

# Step 1: Validate the configuration file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Step 2: Create backup of current configuration
log_info "📦 Creating backup of current configuration..."
./scripts/config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup

# Step 3: Create candidate ConfigMap for validation
log_info "🧪 Creating candidate configuration for validation..."
kubectl create configmap app-config-candidate \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 4: Run validation
log_info "✅ Running configuration validation..."
kubectl apply -f validators/config-validator.yaml -n "$NAMESPACE"

# Wait for validation job to complete
log_info "Waiting for validation to complete..."
kubectl wait --for=condition=complete --timeout=300s job/config-validator -n "$NAMESPACE"

# Check validation result
if kubectl get job config-validator -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' | grep -q "Complete"; then
    log_success "🎉 Configuration validation passed!"
else
    log_error "💥 Configuration validation failed!"
    kubectl logs job/config-validator -n "$NAMESPACE"
    
    # Cleanup candidate ConfigMap
    kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
    kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found
    exit 1
fi

# Step 5: Apply the validated configuration
log_info "🚀 Applying validated configuration..."
kubectl create configmap "$CONFIG_NAME" \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 6: Verify applications are still healthy
log_info "🏥 Checking application health after configuration update..."
sleep 10

# Check if we have deployments using this ConfigMap
DEPLOYMENTS=$(kubectl get deployments -n "$NAMESPACE" -o json | \
    jq -r '.items[] | select(.spec.template.spec.containers[].envFrom[]?.configMapRef.name == "'$CONFIG_NAME'") | .metadata.name')

if [ -n "$DEPLOYMENTS" ]; then
    for deployment in $DEPLOYMENTS; do
        log_info "Checking deployment: $deployment"
        if kubectl get deployment "$deployment" -n "$NAMESPACE" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_success "✅ Deployment $deployment is healthy"
        else
            log_warning "⚠️  Deployment $deployment may have issues"
        fi
    done
else
    log_info "No deployments found using this ConfigMap"
fi

# Step 7: Cleanup
log_info "🧹 Cleaning up validation resources..."
kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found

log_success "🎉 Safe configuration update completed successfully!"
log_info "💡 To rollback if needed, use:"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME list-backups"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME rollback <backup-name>"
EOF

chmod +x scripts/safe-config-update.sh

# Step 4: Demo the advanced system
log_info "🎭 Demonstrating Advanced Configuration Management"

# Create a namespace for our demo
kubectl create namespace config-advanced --dry-run=client -o yaml | kubectl apply -f -

# Create initial configuration
cat > configs/initial-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=info
DATABASE_URL=postgres://prod-db:5432/myapp
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Apply initial configuration
kubectl create configmap app-config \
    --from-env-file=configs/initial-config.env \
    --namespace=config-advanced

# Create an application that uses this config
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-demo-app
  namespace: config-advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: config-demo
  template:
    metadata:
      labels:
        app: config-demo
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - -text=Config Demo App - Environment: $(APP_ENVIRONMENT), Log Level: $(LOG_LEVEL)
        ports:
        - containerPort: 5678
        envFrom:
        - configMapRef:
            name: app-config
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Wait for deployment
kubectl wait --for=condition=available --timeout=300s deployment/config-demo-app -n config-advanced

# Test the system
log_info "🧪 Testing the advanced configuration management system"

# 1. Create a backup
log_info "Step 1: Creating backup..."
./scripts/config-rollback-manager.sh config-advanced app-config backup

# 2. List backups
log_info "Step 2: Listing backups..."
./scripts/config-rollback-manager.sh config-advanced app-config list-backups

# 3. Create an invalid configuration to test validation
cat > configs/invalid-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=invalid_level
DATABASE_URL=not-a-valid-url
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=not_a_number
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=maybe
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

log_info "Step 3: Testing with invalid configuration (should fail)..."
if ./scripts/safe-config-update.sh config-advanced configs/invalid-config.env app-config; then
    log_error "❌ Validation should have failed!"
else
    log_success "✅ Validation correctly failed for invalid configuration"
fi

# 4. Create a valid updated configuration
cat > configs/updated-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=debug
DATABASE_URL=postgres://new-prod-db:5432/myapp
REDIS_URL=redis://new-prod-redis:6379/0
API_RATE_LIMIT=10000
CACHE_TTL=7200
EXTERNAL_API_TIMEOUT=60
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

log_info "Step 4: Applying valid updated configuration..."
./scripts/safe-config-update.sh config-advanced configs/updated-config.env app-config

# 5. Verify the update
log_info "Step 5: Verifying configuration update..."
kubectl get configmap app-config -n config-advanced -o yaml | grep -A 20 "data:"

# 6. Test rollback
log_info "Step 6: Testing rollback functionality..."
BACKUP_NAME=$(./scripts/config-rollback-manager.sh config-advanced app-config list-backups | tail -1 | awk '{print $1}')
if [ -n "$BACKUP_NAME" ] && [ "$BACKUP_NAME" != "NAME" ]; then
    echo "y" | ./scripts/config-rollback-manager.sh config-advanced app-config rollback "$BACKUP_NAME"
    log_success "✅ Rollback test completed"
fi

log_success "🎉 Advanced Configuration Management Demo Complete!"

cd .. # Go back to parent directory
```

### Advanced Demo 2: Multi-Environment GitOps Configuration Pipeline

```bash
# Create GitOps-style configuration management
mkdir -p gitops-configs/{environments,applications,kustomization}
cd gitops-configs

# Base configuration (shared across all environments)
mkdir -p environments/base
cat > environments/base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: "MyWebApp"
  APP_PORT: "8080"
  METRICS_ENABLED: "true"
  HEALTH_CHECK_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_METRICS_DASHBOARD: "true"
  FEATURE_HEALTH_CHECKS: "true"
EOF

cat > environments/base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- configmap.yaml

commonLabels:
  managed-by: kustomize
  component: configuration
EOF

# Development overlay
mkdir -p environments/overlays/development
cat > environments/overlays/development/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "development"
  LOG_LEVEL: "debug"
  DATABASE_URL: "postgres://dev-db:5432/myapp_dev"
  REDIS_URL: "redis://dev-redis:6379/0"
  API_RATE_LIMIT: "100"
  CACHE_TTL: "60"
  DEBUG_MODE: "true"
  EXTERNAL_API_TIMEOUT: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "true"
  FEATURE_EXPERIMENTAL_API: "true"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/development/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-dev

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: development
  
images:
- name: webapp
  newTag: latest
EOF

# Staging overlay
mkdir -p environments/overlays/staging
cat > environments/overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "staging"
  LOG_LEVEL: "info"
  DATABASE_URL: "postgres://staging-db:5432/myapp_staging"
  REDIS_URL: "redis://staging-redis:6379/0"
  API_RATE_LIMIT: "1000"
  CACHE_TTL: "300"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "15"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-staging

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: staging
  
images:
- name: webapp
  newTag: v1.0.0-rc
EOF

# Production overlay
mkdir -p environments/overlays/production
cat > environments/overlays/production/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "production"
  LOG_LEVEL: "warn"
  DATABASE_URL: "postgres://prod-db:5432/myapp_production"
  REDIS_URL: "redis://prod-redis:6379/0"
  API_RATE_LIMIT: "5000"
  CACHE_TTL: "3600"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "30"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "false"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "false"
EOF

cat > environments/overlays/production/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-prod

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: production
  
images:
- name: webapp
  newTag: v1.0.0
EOF

# Application deployment template
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      
  ,
                'port': lambda x: 1 <= int(x) <= 65535,
                'ssl_mode': lambda x: x in ['disable', 'require', 'verify-ca'],
                'timeout': lambda x: 1 <= int(x) <= 300,
                'pool_size': lambda x: 1 <= int(x) <= 100
            }
        },
        'application': {
            'required': ['name', 'environment'],
            'optional': ['debug', 'log_level', 'port'],
            'validators': {
                'name': r'^[a-zA-Z][a-zA-Z0-9-]*
      # Mock backend container
      - name: backend
        image: hashicorp/http-echo:latest
        args:
        - -text=Hello from backend configured via ConfigMap!
        - -listen=:8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: log_level
        volumeMounts:
        - name: backend-config-volume
          mountPath: /etc/config
      # Nginx proxy container
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: processed-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 30
      volumes:
      - name: shared-config-volume
        configMap:
          name: shared-config
      - name: backend-config-volume
        configMap:
          name: backend-config
      - name: nginx-template-volume
        configMap:
          name: nginx-template
      - name: processed-config
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: multi-container-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Step 5: Test the deployment
kubectl get pods -l app=multi-container-app
kubectl logs deployment/multi-container-app -c nginx-config-processor
kubectl logs deployment/multi-container-app -c backend
kubectl logs deployment/multi-container-app -c nginx

# Test the application
kubectl port-forward service/multi-container-service 8080:80 &
curl http://localhost:8080
# Stop port forwarding with: kill %1
```

**What you learned:** How to share configuration between multiple containers, use init containers to process configuration templates, and create realistic multi-tier applications with ConfigMaps.

### Mini-Project 2: Dynamic Configuration Updates

Let's explore ConfigMap hot-reloading with a practical monitoring scenario:

```bash
# Step 1: Create a monitoring configuration
kubectl create configmap monitoring-config \
  --from-literal=scrape_interval=15s \
  --from-literal=evaluation_interval=15s \
  --from-literal=alert_threshold=80 \
  --from-literal=retention_days=30 \
  --from-literal=log_level=info

# Step 2: Create a Prometheus-style configuration file
cat > prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml

# Step 3: Create a configuration-aware application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-watcher-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: config-watcher
  template:
    metadata:
      labels:
        app: config-watcher
    spec:
      containers:
      - name: config-watcher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Configuration Watcher Started"
          echo "Monitoring config files for changes..."
          
          # Function to display current configuration
          show_config() {
            echo "=== Current Configuration ($(date)) ==="
            echo "Environment Variables:"
            env | grep -E "(SCRAPE_|EVALUATION_|ALERT_|RETENTION_|LOG_)" | sort
            echo
            echo "Configuration Files:"
            if [ -f /etc/config/scrape_interval ]; then
              echo "Scrape Interval: $(cat /etc/config/scrape_interval)"
            fi
            if [ -f /etc/config/alert_threshold ]; then
              echo "Alert Threshold: $(cat /etc/config/alert_threshold)"
            fi
            if [ -f /etc/prometheus/prometheus.yml ]; then
              echo "Prometheus Config Present: Yes"
              echo "Config size: $(wc -l < /etc/prometheus/prometheus.yml) lines"
            fi
            echo "================================"
          }
          
          # Show initial configuration
          show_config
          
          # Monitor for file changes
          while true; do
            sleep 30
            show_config
          done
        env:
        - name: SCRAPE_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: scrape_interval
        - name: EVALUATION_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: evaluation_interval
        - name: ALERT_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: alert_threshold
        - name: RETENTION_DAYS
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: retention_days
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: log_level
        volumeMounts:
        - name: monitoring-config-volume
          mountPath: /etc/config
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: monitoring-config-volume
        configMap:
          name: monitoring-config
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
EOF

# Step 4: Watch the initial configuration
kubectl logs deployment/config-watcher-app -f &

# Step 5: Update the configuration and observe changes
echo "Updating scrape interval..."
kubectl patch configmap monitoring-config -p '{"data":{"scrape_interval":"30s","alert_threshold":"90"}}'

echo "Waiting for changes to propagate (up to 60 seconds)..."
sleep 65

# Update the Prometheus configuration
cat > prometheus-updated.yml << 'EOF'
global:
  scrape_interval: 30s  # Updated
  evaluation_interval: 30s  # Updated

rule_files:
  - "alerts.yml"
  - "recording.yml"  # Added

scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 10s  # Override global
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
      
  - job_name: 'kubernetes-services'  # New job
    kubernetes_sd_configs:
    - role: service

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093', 'alertmanager-backup:9093']  # Added backup
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml=prometheus-updated.yml --dry-run=client -o yaml | kubectl apply -f -

echo "Configuration updated. Check the logs for changes..."
# Stop log following with: kill %1
```

**Key Insights from this project:**
- Environment variables from ConfigMaps don't update automatically (require pod restart)
- Volume-mounted ConfigMaps update automatically (with up to 60-second delay)
- Applications should be designed to handle configuration changes gracefully

### Mini-Project 3: Environment-Specific Deployment Pipeline

Let's create a realistic deployment pipeline that manages configurations across different environments:

```bash
# Create directory structure for the project
mkdir -p configmap-pipeline/{environments,applications,scripts}
cd configmap-pipeline

# Step 1: Create environment-specific configurations
cat > environments/development.env << 'EOF'
# Development Environment Configuration
APP_ENVIRONMENT=development
LOG_LEVEL=debug
DATABASE_URL=dev-postgres:5432/myapp_dev
REDIS_URL=dev-redis:6379/0
API_RATE_LIMIT=100
CACHE_TTL=60
EXTERNAL_API_TIMEOUT=5
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

cat > environments/staging.env << 'EOF'
# Staging Environment Configuration
APP_ENVIRONMENT=staging
LOG_LEVEL=info
DATABASE_URL=staging-postgres:5432/myapp_staging
REDIS_URL=staging-redis:6379/0
API_RATE_LIMIT=1000
CACHE_TTL=300
EXTERNAL_API_TIMEOUT=15
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

cat > environments/production.env << 'EOF'
# Production Environment Configuration
APP_ENVIRONMENT=production
LOG_LEVEL=warn
DATABASE_URL=prod-postgres:5432/myapp_production
REDIS_URL=prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Step 2: Create feature flag configurations
cat > environments/dev-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=true
FEATURE_EXPERIMENTAL_API=true
FEATURE_A_B_TESTING=true
EOF

cat > environments/staging-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=true
EOF

cat > environments/prod-features.env << 'EOF'
FEATURE_NEW_UI=false
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=false
EOF

# Step 3: Create deployment script
cat > scripts/deploy-environment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}
APP_VERSION=${3:-latest}

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying to environment: $ENVIRONMENT in namespace: $NAMESPACE"

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Function to create or update ConfigMap
create_or_update_configmap() {
    local name=$1
    local file=$2
    local description=$3
    
    log_info "Creating/updating ConfigMap: $name ($description)"
    
    if kubectl get configmap "$name" -n "$NAMESPACE" &> /dev/null; then
        log_warning "ConfigMap $name exists, updating..."
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl apply -f -
    else
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE"
    fi
}

# Deploy environment-specific configurations
create_or_update_configmap "app-config" "environments/$ENVIRONMENT.env" "Application Configuration"
create_or_update_configmap "feature-flags" "environments/$ENVIRONMENT-features.env" "Feature Flags"

# Create version info ConfigMap
log_info "Creating version information ConfigMap..."
kubectl create configmap version-info \
    --from-literal=version="$APP_VERSION" \
    --from-literal=environment="$ENVIRONMENT" \
    --from-literal=deployed_at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --from-literal=deployed_by="$(whoami)" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

log_success "✅ Configuration deployment complete for $ENVIRONMENT environment"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   App Version: $APP_VERSION"
echo "   ConfigMaps created:"
echo "     - app-config"
echo "     - feature-flags"
echo "     - version-info"

log_info "🔍 To view configurations:"
echo "   kubectl get configmaps -n $NAMESPACE"
echo "   kubectl describe configmap app-config -n $NAMESPACE"
echo "   kubectl describe configmap feature-flags -n $NAMESPACE"
EOF

chmod +x scripts/deploy-environment.sh

# Step 4: Create the application deployment
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: hashicorp/http-echo:latest
        ports:
        - containerPort: 5678
        # Load environment configuration
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        - configMapRef:
            name: version-info
            prefix: VERSION_
        # Custom startup message based on configuration
        args:
        - -text=Hello from $(VERSION_environment) environment! App version $(VERSION_version) deployed at $(VERSION_deployed_at)
        livenessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 5678
  type: ClusterIP
---
# ConfigMap for application monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*webapp*.log
        Parser            docker
        Tag               webapp.*
        Refresh_Interval  5
    
    [OUTPUT]
        Name  stdout
        Match *
---
# Health check and debugging pod
apiVersion: v1
kind: Pod
metadata:
  name: environment-inspector
  labels:
    app: inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "🔍 Environment Inspector Started"
      echo "=================================="
      
      show_environment_info() {
        echo "📊 Environment Information ($(date))"
        echo "Environment: $APP_ENVIRONMENT"
        echo "Version: $VERSION_version"
        echo "Log Level: $LOG_LEVEL"
        echo "Database: $DATABASE_URL"
        echo "Cache TTL: $CACHE_TTL seconds"
        echo
        echo "🚩 Feature Flags:"
        env | grep "FEATURE_" | sort
        echo
        echo "⚙️  System Configuration:"
        echo "API Rate Limit: $API_RATE_LIMIT"
        echo "External API Timeout: $EXTERNAL_API_TIMEOUT seconds"
        echo "Debug Mode: $DEBUG_MODE"
        echo "Metrics Enabled: $METRICS_ENABLED"
        echo "=================================="
      }
      
      # Show initial state
      show_environment_info
      
      # Monitor for changes every 60 seconds
      while true; do
        sleep 60
        show_environment_info
      done
    envFrom:
    - configMapRef:
        name: app-config
    - configMapRef:
        name: feature-flags
        prefix: FEATURE_
    - configMapRef:
        name: version-info
        prefix: VERSION_
EOF

# Step 5: Create testing script
cat > scripts/test-deployment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}

# Color output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing deployment in $ENVIRONMENT environment"

# Test 1: Check ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
for cm in app-config feature-flags version-info; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Check application is running
log_info "Test 2: Verifying application pods are running..."
if kubectl get deployment webapp -n "$NAMESPACE" &> /dev/null; then
    READY=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    if [ "$READY" = "$DESIRED" ]; then
        log_success "Application deployment ready ($READY/$DESIRED pods)"
    else
        log_error "Application deployment not ready ($READY/$DESIRED pods)"
    fi
else
    log_error "Application deployment not found"
fi

# Test 3: Test application response
log_info "Test 3: Testing application response..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080 | grep -q "$ENVIRONMENT"; then
    log_success "Application responds with correct environment"
else
    log_error "Application response incorrect or unreachable"
fi

kill $PORT_FORWARD_PID 2>/dev/null

# Test 4: Check environment inspector
log_info "Test 4: Checking environment inspector..."
if kubectl get pod environment-inspector -n "$NAMESPACE" &> /dev/null; then
    log_success "Environment inspector pod exists"
    log_info "Recent inspector logs:"
    kubectl logs environment-inspector -n "$NAMESPACE" --tail=10
else
    log_error "Environment inspector pod not found"
fi

log_success "🎉 All tests completed for $ENVIRONMENT environment"
EOF

chmod +x scripts/test-deployment.sh

# Step 6: Demo the pipeline
log_info() { echo -e "\033[0;34m[INFO]\033[0m $1"; }

log_info "🚀 Starting Environment Pipeline Demo"

# Deploy to development
log_info "Deploying to development environment..."
./scripts/deploy-environment.sh development webapp-dev v1.0.0

# Apply the application
kubectl apply -f applications/webapp-deployment.yaml -n webapp-dev

# Wait for deployment
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-dev

# Test the deployment
./scripts/test-deployment.sh development webapp-dev

log_info "✅ Development deployment complete!"

# Now let's demonstrate staging deployment
log_info "Deploying to staging environment..."
./scripts/deploy-environment.sh staging webapp-staging v1.0.0

# Apply application to staging
kubectl apply -f applications/webapp-deployment.yaml -n webapp-staging

log_info "Waiting for staging deployment..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-staging

# Test staging
./scripts/test-deployment.sh staging webapp-staging

log_info "✅ Staging deployment complete!"

# Cleanup function
cd .. # Go back to parent directory
```

**What you learned from this project:**
- How to organize environment-specific configurations
- Automated deployment pipelines with ConfigMaps
- Testing and validation strategies
- Real-world deployment patterns

## 🎓 Advanced Level: Production-Ready Patterns

### Advanced Demo 1: ConfigMap Validation and Rollback Strategy

Let's implement a production-ready configuration management system with validation and rollback capabilities:

```bash
# Create advanced configuration management system
mkdir -p advanced-configmaps/{configs,validators,scripts}
cd advanced-configmaps

# Step 1: Create configuration validator
cat > validators/config-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-validator-script
data:
  validate-config.sh: |
    #!/bin/bash
    set -e
    
    CONFIG_FILE=${1:-/config/app.env}
    ERRORS=0
    
    echo "🔍 Validating configuration file: $CONFIG_FILE"
    
    # Check if file exists
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "❌ Configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    # Validation rules
    validate_url() {
        local var_name=$1
        local url=$2
        if [[ ! "$url" =~ ^[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9.-]+:[0-9]+.*$ ]]; then
            echo "❌ Invalid URL format for $var_name: $url"
            ((ERRORS++))
        else
            echo "✅ Valid URL for $var_name"
        fi
    }
    
    validate_number() {
        local var_name=$1
        local number=$2
        local min=${3:-0}
        local max=${4:-999999}
        if ! [[ "$number" =~ ^[0-9]+$ ]] || [ "$number" -lt "$min" ] || [ "$number" -gt "$max" ]; then
            echo "❌ Invalid number for $var_name: $number (must be $min-$max)"
            ((ERRORS++))
        else
            echo "✅ Valid number for $var_name"
        fi
    }
    
    validate_boolean() {
        local var_name=$1
        local bool=$2
        if [[ ! "$bool" =~ ^(true|false)$ ]]; then
            echo "❌ Invalid boolean for $var_name: $bool (must be true/false)"
            ((ERRORS++))
        else
            echo "✅ Valid boolean for $var_name"
        fi
    }
    
    validate_log_level() {
        local level=$1
        if [[ ! "$level" =~ ^(debug|info|warn|error)$ ]]; then
            echo "❌ Invalid log level: $level (must be debug|info|warn|error)"
            ((ERRORS++))
        else
            echo "✅ Valid log level"
        fi
    }
    
    # Parse and validate configuration
    while IFS='=' read -r key value; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ ]] && continue
        [[ -z "$key" ]] && continue
        
        case "$key" in
            "DATABASE_URL"|"REDIS_URL")
                validate_url "$key" "$value"
                ;;
            "DATABASE_PORT"|"REDIS_PORT"|"APP_PORT")
                validate_number "$key" "$value" 1 65535
                ;;
            "API_RATE_LIMIT")
                validate_number "$key" "$value" 1 100000
                ;;
            "CACHE_TTL"|"EXTERNAL_API_TIMEOUT")
                validate_number "$key" "$value" 1 86400
                ;;
            "DEBUG_MODE"|"METRICS_ENABLED"|"PROFILING_ENABLED")
                validate_boolean "$key" "$value"
                ;;
            "LOG_LEVEL")
                validate_log_level "$value"
                ;;
        esac
    done < "$CONFIG_FILE"
    
    echo "📊 Validation complete. Errors found: $ERRORS"
    
    if [ $ERRORS -eq 0 ]; then
        echo "🎉 Configuration validation passed!"
        exit 0
    else
        echo "💥 Configuration validation failed!"
        exit 1
    fi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator
spec:
  template:
    spec:
      containers:
      - name: validator
        image: busybox
        command: ["sh", "/scripts/validate-config.sh"]
        volumeMounts:
        - name: config-to-validate
          mountPath: /config
        - name: validator-script
          mountPath: /scripts
      volumes:
      - name: config-to-validate
        configMap:
          name: app-config-candidate  # The config we want to validate
      - name: validator-script
        configMap:
          name: config-validator-script
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3
EOF

# Step 2: Create configuration rollback system
cat > scripts/config-rollback-manager.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_NAME=${2:-app-config}
ACTION=${3:-backup}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

case "$ACTION" in
    "backup")
        log_info "🔄 Creating backup of ConfigMap: $CONFIG_NAME"
        
        # Get current timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_NAME="${CONFIG_NAME}-backup-${TIMESTAMP}"
        
        # Check if ConfigMap exists
        if ! kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" &> /dev/null; then
            log_error "ConfigMap $CONFIG_NAME not found in namespace $NAMESPACE"
            exit 1
        fi
        
        # Create backup
        kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $CONFIG_NAME/name: $BACKUP_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        kubectl apply -f -
        
        log_success "✅ Backup created: $BACKUP_NAME"
        
        # Label the backup for easy identification
        kubectl label configmap "$BACKUP_NAME" -n "$NAMESPACE" \
            backup-of="$CONFIG_NAME" \
            backup-timestamp="$TIMESTAMP"
        
        echo "Backup ConfigMap: $BACKUP_NAME"
        ;;
        
    "list-backups")
        log_info "📋 Listing backups for ConfigMap: $CONFIG_NAME"
        kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o custom-columns="NAME:.metadata.name,TIMESTAMP:.metadata.labels.backup-timestamp,AGE:.metadata.creationTimestamp"
        ;;
        
    "rollback")
        BACKUP_NAME=$4
        if [ -z "$BACKUP_NAME" ]; then
            log_error "Backup name required for rollback"
            log_info "Usage: $0 <namespace> <config-name> rollback <backup-name>"
            log_info "Use 'list-backups' to see available backups"
            exit 1
        fi
        
        log_warning "⚠️  Rolling back $CONFIG_NAME to backup: $BACKUP_NAME"
        read -p "Are you sure? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Rollback cancelled"
            exit 0
        fi
        
        # Create backup of current state before rollback
        ./config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup
        
        # Perform rollback
        kubectl get configmap "$BACKUP_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $BACKUP_NAME/name: $CONFIG_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        sed '/backup-of:/d' | \
        sed '/backup-timestamp:/d' | \
        kubectl apply -f -
        
        log_success "✅ Rollback completed: $CONFIG_NAME restored from $BACKUP_NAME"
        ;;
        
    "cleanup")
        KEEP_COUNT=${4:-5}
        log_info "🧹 Cleaning up old backups, keeping $KEEP_COUNT most recent"
        
        # Get backup ConfigMaps sorted by timestamp (oldest first)
        BACKUPS=$(kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o jsonpath='{.items[*].metadata.name}')
        
        BACKUP_ARRAY=($BACKUPS)
        TOTAL_BACKUPS=${#BACKUP_ARRAY[@]}
        
        if [ $TOTAL_BACKUPS -le $KEEP_COUNT ]; then
            log_info "Found $TOTAL_BACKUPS backups, keeping all (within limit of $KEEP_COUNT)"
            exit 0
        fi
        
        DELETE_COUNT=$((TOTAL_BACKUPS - KEEP_COUNT))
        log_warning "Found $TOTAL_BACKUPS backups, will delete $DELETE_COUNT oldest"
        
        for ((i=0; i<DELETE_COUNT; i++)); do
            BACKUP_TO_DELETE=${BACKUP_ARRAY[$i]}
            log_info "Deleting backup: $BACKUP_TO_DELETE"
            kubectl delete configmap "$BACKUP_TO_DELETE" -n "$NAMESPACE"
        done
        
        log_success "✅ Cleanup completed, kept $KEEP_COUNT most recent backups"
        ;;
        
    *)
        log_error "Unknown action: $ACTION"
        log_info "Available actions: backup, list-backups, rollback, cleanup"
        exit 1
        ;;
esac
EOF

chmod +x scripts/config-rollback-manager.sh

# Step 3: Create safe configuration update system
cat > scripts/safe-config-update.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_FILE=${2}
CONFIG_NAME=${3:-app-config}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <namespace> <config-file> [config-name]"
    echo "Example: $0 default new-config.env app-config"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[SAFE-UPDATE]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🛡️  Starting safe configuration update process"
log_info "Target: $CONFIG_NAME in namespace $NAMESPACE"
log_info "Source: $CONFIG_FILE"

# Step 1: Validate the configuration file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Step 2: Create backup of current configuration
log_info "📦 Creating backup of current configuration..."
./scripts/config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup

# Step 3: Create candidate ConfigMap for validation
log_info "🧪 Creating candidate configuration for validation..."
kubectl create configmap app-config-candidate \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 4: Run validation
log_info "✅ Running configuration validation..."
kubectl apply -f validators/config-validator.yaml -n "$NAMESPACE"

# Wait for validation job to complete
log_info "Waiting for validation to complete..."
kubectl wait --for=condition=complete --timeout=300s job/config-validator -n "$NAMESPACE"

# Check validation result
if kubectl get job config-validator -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' | grep -q "Complete"; then
    log_success "🎉 Configuration validation passed!"
else
    log_error "💥 Configuration validation failed!"
    kubectl logs job/config-validator -n "$NAMESPACE"
    
    # Cleanup candidate ConfigMap
    kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
    kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found
    exit 1
fi

# Step 5: Apply the validated configuration
log_info "🚀 Applying validated configuration..."
kubectl create configmap "$CONFIG_NAME" \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 6: Verify applications are still healthy
log_info "🏥 Checking application health after configuration update..."
sleep 10

# Check if we have deployments using this ConfigMap
DEPLOYMENTS=$(kubectl get deployments -n "$NAMESPACE" -o json | \
    jq -r '.items[] | select(.spec.template.spec.containers[].envFrom[]?.configMapRef.name == "'$CONFIG_NAME'") | .metadata.name')

if [ -n "$DEPLOYMENTS" ]; then
    for deployment in $DEPLOYMENTS; do
        log_info "Checking deployment: $deployment"
        if kubectl get deployment "$deployment" -n "$NAMESPACE" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_success "✅ Deployment $deployment is healthy"
        else
            log_warning "⚠️  Deployment $deployment may have issues"
        fi
    done
else
    log_info "No deployments found using this ConfigMap"
fi

# Step 7: Cleanup
log_info "🧹 Cleaning up validation resources..."
kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found

log_success "🎉 Safe configuration update completed successfully!"
log_info "💡 To rollback if needed, use:"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME list-backups"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME rollback <backup-name>"
EOF

chmod +x scripts/safe-config-update.sh

# Step 4: Demo the advanced system
log_info "🎭 Demonstrating Advanced Configuration Management"

# Create a namespace for our demo
kubectl create namespace config-advanced --dry-run=client -o yaml | kubectl apply -f -

# Create initial configuration
cat > configs/initial-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=info
DATABASE_URL=postgres://prod-db:5432/myapp
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Apply initial configuration
kubectl create configmap app-config \
    --from-env-file=configs/initial-config.env \
    --namespace=config-advanced

# Create an application that uses this config
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-demo-app
  namespace: config-advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: config-demo
  template:
    metadata:
      labels:
        app: config-demo
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - -text=Config Demo App - Environment: $(APP_ENVIRONMENT), Log Level: $(LOG_LEVEL)
        ports:
        - containerPort: 5678
        envFrom:
        - configMapRef:
            name: app-config
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Wait for deployment
kubectl wait --for=condition=available --timeout=300s deployment/config-demo-app -n config-advanced

# Test the system
log_info "🧪 Testing the advanced configuration management system"

# 1. Create a backup
log_info "Step 1: Creating backup..."
./scripts/config-rollback-manager.sh config-advanced app-config backup

# 2. List backups
log_info "Step 2: Listing backups..."
./scripts/config-rollback-manager.sh config-advanced app-config list-backups

# 3. Create an invalid configuration to test validation
cat > configs/invalid-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=invalid_level
DATABASE_URL=not-a-valid-url
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=not_a_number
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=maybe
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

log_info "Step 3: Testing with invalid configuration (should fail)..."
if ./scripts/safe-config-update.sh config-advanced configs/invalid-config.env app-config; then
    log_error "❌ Validation should have failed!"
else
    log_success "✅ Validation correctly failed for invalid configuration"
fi

# 4. Create a valid updated configuration
cat > configs/updated-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=debug
DATABASE_URL=postgres://new-prod-db:5432/myapp
REDIS_URL=redis://new-prod-redis:6379/0
API_RATE_LIMIT=10000
CACHE_TTL=7200
EXTERNAL_API_TIMEOUT=60
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

log_info "Step 4: Applying valid updated configuration..."
./scripts/safe-config-update.sh config-advanced configs/updated-config.env app-config

# 5. Verify the update
log_info "Step 5: Verifying configuration update..."
kubectl get configmap app-config -n config-advanced -o yaml | grep -A 20 "data:"

# 6. Test rollback
log_info "Step 6: Testing rollback functionality..."
BACKUP_NAME=$(./scripts/config-rollback-manager.sh config-advanced app-config list-backups | tail -1 | awk '{print $1}')
if [ -n "$BACKUP_NAME" ] && [ "$BACKUP_NAME" != "NAME" ]; then
    echo "y" | ./scripts/config-rollback-manager.sh config-advanced app-config rollback "$BACKUP_NAME"
    log_success "✅ Rollback test completed"
fi

log_success "🎉 Advanced Configuration Management Demo Complete!"

cd .. # Go back to parent directory
```

### Advanced Demo 2: Multi-Environment GitOps Configuration Pipeline

```bash
# Create GitOps-style configuration management
mkdir -p gitops-configs/{environments,applications,kustomization}
cd gitops-configs

# Base configuration (shared across all environments)
mkdir -p environments/base
cat > environments/base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: "MyWebApp"
  APP_PORT: "8080"
  METRICS_ENABLED: "true"
  HEALTH_CHECK_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_METRICS_DASHBOARD: "true"
  FEATURE_HEALTH_CHECKS: "true"
EOF

cat > environments/base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- configmap.yaml

commonLabels:
  managed-by: kustomize
  component: configuration
EOF

# Development overlay
mkdir -p environments/overlays/development
cat > environments/overlays/development/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "development"
  LOG_LEVEL: "debug"
  DATABASE_URL: "postgres://dev-db:5432/myapp_dev"
  REDIS_URL: "redis://dev-redis:6379/0"
  API_RATE_LIMIT: "100"
  CACHE_TTL: "60"
  DEBUG_MODE: "true"
  EXTERNAL_API_TIMEOUT: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "true"
  FEATURE_EXPERIMENTAL_API: "true"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/development/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-dev

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: development
  
images:
- name: webapp
  newTag: latest
EOF

# Staging overlay
mkdir -p environments/overlays/staging
cat > environments/overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "staging"
  LOG_LEVEL: "info"
  DATABASE_URL: "postgres://staging-db:5432/myapp_staging"
  REDIS_URL: "redis://staging-redis:6379/0"
  API_RATE_LIMIT: "1000"
  CACHE_TTL: "300"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "15"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-staging

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: staging
  
images:
- name: webapp
  newTag: v1.0.0-rc
EOF

# Production overlay
mkdir -p environments/overlays/production
cat > environments/overlays/production/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "production"
  LOG_LEVEL: "warn"
  DATABASE_URL: "postgres://prod-db:5432/myapp_production"
  REDIS_URL: "redis://prod-redis:6379/0"
  API_RATE_LIMIT: "5000"
  CACHE_TTL: "3600"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "30"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "false"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "false"
EOF

cat > environments/overlays/production/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-prod

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: production
  
images:
- name: webapp
  newTag: v1.0.0
EOF

# Application deployment template
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      
  ,
                'environment': lambda x: x in ['development', 'staging', 'production'],
                'debug': lambda x: x.lower() in ['true', 'false'],
                'log_level': lambda x: x in ['debug', 'info', 'warn', 'error'],
                'port': lambda x: 1024 <= int(x) <= 65535
            }
        }
    }
    
    def validate_config(config_data, schema_name):
        """Validate configuration against schema"""
        if schema_name not in SCHEMAS:
            print(f"❌ Unknown schema: {schema_name}")
            return False
            
        schema = SCHEMAS[schema_name]
        errors = []
        
        # Check required fields
        for field in schema['required']:
            if field not in config_data:
                errors.append(f"Missing required field: {field}")
        
        # Validate field values
        for field, value in config_data.items():
            if field in schema['validators']:
                validator = schema['validators'][field]
                try:
                    if callable(validator):
                        if not validator(value):
                            errors.append(f"Invalid value for {field}: {value}")
                    else:  # regex pattern
                        if not re.match(validator, value):
                            errors.append(f"Invalid format for {field}: {value}")
                except Exception as e:
                    errors.append(f"Validation error for {field}: {str(e)}")
        
        if errors:
            print("❌ Validation errors:")
            for error in errors:
                print(f"  - {error}")
            return False
        else:
            print("✅ Configuration validation passed")
            return True
    
    def main():
        if len(sys.argv) != 3:
            print("Usage: validate.py <config-file> <schema-name>")
            sys.exit(1)
            
        config_file = sys.argv[1]
        schema_name = sys.argv[2]
        
        if not os.path.exists(config_file):
            print(f"❌ Configuration file not found: {config_file}")
            sys.exit(1)
        
        # Parse configuration file (assume key=value format)
        config_data = {}
        with open(config_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        config_data[key.strip().lower()] = value.strip()
        
        print(f"🔍 Validating {config_file} against {schema_name} schema")
        success = validate_config(config_data, schema_name)
        sys.exit(0 if success else 1)
    
    if __name__ == '__main__':
        main()
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator-job
spec:
  template:
    spec:
      containers:
      - name: validator
        image: python:3.9-slim
        command: ["python3", "/scripts/validate.py", "/config/app.env", "application"]
        volumeMounts:
        - name: validator-script
          mountPath: /scripts
        - name: config-to-validate
          mountPath: /config
      volumes:
      - name: validator-script
        configMap:
          name: config-schema-validator
          defaultMode: 0755
      - name: config-to-validate
        configMap:
          name: config-candidate
      restartPolicy: Never
  backoffLimit: 3
EOF

# Create configuration templates
cat > templates/application-config-template.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.configName }}
  namespace: {{ .Values.namespace }}
  labels:
    app: {{ .Values.appName }}
    environment: {{ .Values.environment }}
    config-version: {{ .Values.configVersion }}
data:
  # Application Configuration
  APP_NAME: {{ .Values.app.name | quote }}
  APP_ENVIRONMENT: {{ .Values.environment | quote }}
  APP_VERSION: {{ .Values.app.version | quote }}
  APP_PORT: {{ .Values.app.port | quote }}
  
  # Database Configuration
  {{- if .Values.database }}
  DATABASE_HOST: {{ .Values.database.host | quote }}
  DATABASE_PORT: {{ .Values.database.port | quote }}
  DATABASE_NAME: {{ .Values.database.name | quote }}
  DATABASE_SSL_MODE: {{ .Values.database.sslMode | default "require" | quote }}
  DATABASE_POOL_SIZE: {{ .Values.database.poolSize | default "10" | quote }}
  {{- end }}
  
  # Cache Configuration  
  {{- if .Values.redis }}
  REDIS_HOST: {{ .Values.redis.host | quote }}
  REDIS_PORT: {{ .Values.redis.port | quote }}
  REDIS_DB: {{ .Values.redis.database | default "0" | quote }}
  {{- end }}
  
  # Feature Flags
  {{- range $key, $value := .Values.features }}
  FEATURE_{{ $key | upper }}: {{ $value | quote }}
  {{- end }}
  
  # Environment-specific overrides
  {{- if eq .Values.environment "development" }}
  LOG_LEVEL: "debug"
  DEBUG_MODE: "true" 
  METRICS_ENABLED: "true"
  {{- else if eq .Values.environment "staging" }}
  LOG_LEVEL: "info"
  DEBUG_MODE: "false"
  METRICS_ENABLED: "true"
  {{- else if eq .Values.environment "production" }}
  LOG_LEVEL: "warn"
  DEBUG_MODE: "false"
  METRICS_ENABLED: "true"
  {{- end }}
EOF

# Create policy enforcement
cat > policies/configmap-policies.yaml << 'EOF'
# OPA (Open Policy Agent) policies for ConfigMap governance
apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap-policies
data:
  configmap-naming.rego: |
    package configmap.naming
    
    # ConfigMap names must follow naming convention
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        not regex.match("^[a-z][a-z0-9-]*-config$", input.metadata.name)
        msg := sprintf("ConfigMap name '%s' doesn't follow naming convention (must end with -config)", [input.metadata.name])
    }
    
  configmap-labels.rego: |
    package configmap.labels
    
    required_labels := ["app", "environment", "config-version"]
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        required := required_labels[_]
        not input.metadata.labels[required]
        msg := sprintf("ConfigMap missing required label: %s", [required])
    }
    
  configmap-size.rego: |
    package configmap.size
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        total_size := sum([count(input.data[key]) | key := input.data[_]])
        total_size > 1048576  # 1MB limit
        msg := sprintf("ConfigMap size %d bytes exceeds 1MB limit", [total_size])
    }
    
  configmap-environment.rego: |
    package configmap.environment
    
    # Production ConfigMaps must have additional validation
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        input.metadata.labels.environment == "production"
        not input.metadata.labels["config-version"]
        msg := "Production ConfigMaps must have config-version label"
    }
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap" 
        input.metadata.labels.environment == "production"
        input.data[key]
        contains(input.data[key], "localhost")
        msg := sprintf("Production ConfigMap contains localhost reference in key: %s", [key])
    }
EOF

# Create comprehensive deployment script
cat > deploy-with-validation.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
CONFIG_FILE=${2}
NAMESPACE=${3:-default}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <environment> <config-file> [namespace]"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[DEPLOY]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying configuration with validation pipeline"
log_info "Environment: $ENVIRONMENT"
log_info "Config file: $CONFIG_FILE"
log_info "Namespace: $NAMESPACE"

# Step 1: Pre-deployment validation
log_info "📋 Step 1: Pre-deployment validation"

# Validate file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Validate environment
if [[ ! "$
      # Mock backend container
      - name: backend
        image: hashicorp/http-echo:latest
        args:
        - -text=Hello from backend configured via ConfigMap!
        - -listen=:8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: log_level
        volumeMounts:
        - name: backend-config-volume
          mountPath: /etc/config
      # Nginx proxy container
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: processed-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 30
      volumes:
      - name: shared-config-volume
        configMap:
          name: shared-config
      - name: backend-config-volume
        configMap:
          name: backend-config
      - name: nginx-template-volume
        configMap:
          name: nginx-template
      - name: processed-config
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: multi-container-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Step 5: Test the deployment
kubectl get pods -l app=multi-container-app
kubectl logs deployment/multi-container-app -c nginx-config-processor
kubectl logs deployment/multi-container-app -c backend
kubectl logs deployment/multi-container-app -c nginx

# Test the application
kubectl port-forward service/multi-container-service 8080:80 &
curl http://localhost:8080
# Stop port forwarding with: kill %1
```

**What you learned:** How to share configuration between multiple containers, use init containers to process configuration templates, and create realistic multi-tier applications with ConfigMaps.

### Mini-Project 2: Dynamic Configuration Updates

Let's explore ConfigMap hot-reloading with a practical monitoring scenario:

```bash
# Step 1: Create a monitoring configuration
kubectl create configmap monitoring-config \
  --from-literal=scrape_interval=15s \
  --from-literal=evaluation_interval=15s \
  --from-literal=alert_threshold=80 \
  --from-literal=retention_days=30 \
  --from-literal=log_level=info

# Step 2: Create a Prometheus-style configuration file
cat > prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml

# Step 3: Create a configuration-aware application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-watcher-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: config-watcher
  template:
    metadata:
      labels:
        app: config-watcher
    spec:
      containers:
      - name: config-watcher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Configuration Watcher Started"
          echo "Monitoring config files for changes..."
          
          # Function to display current configuration
          show_config() {
            echo "=== Current Configuration ($(date)) ==="
            echo "Environment Variables:"
            env | grep -E "(SCRAPE_|EVALUATION_|ALERT_|RETENTION_|LOG_)" | sort
            echo
            echo "Configuration Files:"
            if [ -f /etc/config/scrape_interval ]; then
              echo "Scrape Interval: $(cat /etc/config/scrape_interval)"
            fi
            if [ -f /etc/config/alert_threshold ]; then
              echo "Alert Threshold: $(cat /etc/config/alert_threshold)"
            fi
            if [ -f /etc/prometheus/prometheus.yml ]; then
              echo "Prometheus Config Present: Yes"
              echo "Config size: $(wc -l < /etc/prometheus/prometheus.yml) lines"
            fi
            echo "================================"
          }
          
          # Show initial configuration
          show_config
          
          # Monitor for file changes
          while true; do
            sleep 30
            show_config
          done
        env:
        - name: SCRAPE_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: scrape_interval
        - name: EVALUATION_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: evaluation_interval
        - name: ALERT_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: alert_threshold
        - name: RETENTION_DAYS
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: retention_days
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: log_level
        volumeMounts:
        - name: monitoring-config-volume
          mountPath: /etc/config
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: monitoring-config-volume
        configMap:
          name: monitoring-config
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
EOF

# Step 4: Watch the initial configuration
kubectl logs deployment/config-watcher-app -f &

# Step 5: Update the configuration and observe changes
echo "Updating scrape interval..."
kubectl patch configmap monitoring-config -p '{"data":{"scrape_interval":"30s","alert_threshold":"90"}}'

echo "Waiting for changes to propagate (up to 60 seconds)..."
sleep 65

# Update the Prometheus configuration
cat > prometheus-updated.yml << 'EOF'
global:
  scrape_interval: 30s  # Updated
  evaluation_interval: 30s  # Updated

rule_files:
  - "alerts.yml"
  - "recording.yml"  # Added

scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 10s  # Override global
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
      
  - job_name: 'kubernetes-services'  # New job
    kubernetes_sd_configs:
    - role: service

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093', 'alertmanager-backup:9093']  # Added backup
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml=prometheus-updated.yml --dry-run=client -o yaml | kubectl apply -f -

echo "Configuration updated. Check the logs for changes..."
# Stop log following with: kill %1
```

**Key Insights from this project:**
- Environment variables from ConfigMaps don't update automatically (require pod restart)
- Volume-mounted ConfigMaps update automatically (with up to 60-second delay)
- Applications should be designed to handle configuration changes gracefully

### Mini-Project 3: Environment-Specific Deployment Pipeline

Let's create a realistic deployment pipeline that manages configurations across different environments:

```bash
# Create directory structure for the project
mkdir -p configmap-pipeline/{environments,applications,scripts}
cd configmap-pipeline

# Step 1: Create environment-specific configurations
cat > environments/development.env << 'EOF'
# Development Environment Configuration
APP_ENVIRONMENT=development
LOG_LEVEL=debug
DATABASE_URL=dev-postgres:5432/myapp_dev
REDIS_URL=dev-redis:6379/0
API_RATE_LIMIT=100
CACHE_TTL=60
EXTERNAL_API_TIMEOUT=5
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

cat > environments/staging.env << 'EOF'
# Staging Environment Configuration
APP_ENVIRONMENT=staging
LOG_LEVEL=info
DATABASE_URL=staging-postgres:5432/myapp_staging
REDIS_URL=staging-redis:6379/0
API_RATE_LIMIT=1000
CACHE_TTL=300
EXTERNAL_API_TIMEOUT=15
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

cat > environments/production.env << 'EOF'
# Production Environment Configuration
APP_ENVIRONMENT=production
LOG_LEVEL=warn
DATABASE_URL=prod-postgres:5432/myapp_production
REDIS_URL=prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Step 2: Create feature flag configurations
cat > environments/dev-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=true
FEATURE_EXPERIMENTAL_API=true
FEATURE_A_B_TESTING=true
EOF

cat > environments/staging-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=true
EOF

cat > environments/prod-features.env << 'EOF'
FEATURE_NEW_UI=false
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=false
EOF

# Step 3: Create deployment script
cat > scripts/deploy-environment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}
APP_VERSION=${3:-latest}

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying to environment: $ENVIRONMENT in namespace: $NAMESPACE"

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Function to create or update ConfigMap
create_or_update_configmap() {
    local name=$1
    local file=$2
    local description=$3
    
    log_info "Creating/updating ConfigMap: $name ($description)"
    
    if kubectl get configmap "$name" -n "$NAMESPACE" &> /dev/null; then
        log_warning "ConfigMap $name exists, updating..."
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl apply -f -
    else
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE"
    fi
}

# Deploy environment-specific configurations
create_or_update_configmap "app-config" "environments/$ENVIRONMENT.env" "Application Configuration"
create_or_update_configmap "feature-flags" "environments/$ENVIRONMENT-features.env" "Feature Flags"

# Create version info ConfigMap
log_info "Creating version information ConfigMap..."
kubectl create configmap version-info \
    --from-literal=version="$APP_VERSION" \
    --from-literal=environment="$ENVIRONMENT" \
    --from-literal=deployed_at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --from-literal=deployed_by="$(whoami)" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

log_success "✅ Configuration deployment complete for $ENVIRONMENT environment"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   App Version: $APP_VERSION"
echo "   ConfigMaps created:"
echo "     - app-config"
echo "     - feature-flags"
echo "     - version-info"

log_info "🔍 To view configurations:"
echo "   kubectl get configmaps -n $NAMESPACE"
echo "   kubectl describe configmap app-config -n $NAMESPACE"
echo "   kubectl describe configmap feature-flags -n $NAMESPACE"
EOF

chmod +x scripts/deploy-environment.sh

# Step 4: Create the application deployment
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: hashicorp/http-echo:latest
        ports:
        - containerPort: 5678
        # Load environment configuration
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        - configMapRef:
            name: version-info
            prefix: VERSION_
        # Custom startup message based on configuration
        args:
        - -text=Hello from $(VERSION_environment) environment! App version $(VERSION_version) deployed at $(VERSION_deployed_at)
        livenessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 5678
  type: ClusterIP
---
# ConfigMap for application monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*webapp*.log
        Parser            docker
        Tag               webapp.*
        Refresh_Interval  5
    
    [OUTPUT]
        Name  stdout
        Match *
---
# Health check and debugging pod
apiVersion: v1
kind: Pod
metadata:
  name: environment-inspector
  labels:
    app: inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "🔍 Environment Inspector Started"
      echo "=================================="
      
      show_environment_info() {
        echo "📊 Environment Information ($(date))"
        echo "Environment: $APP_ENVIRONMENT"
        echo "Version: $VERSION_version"
        echo "Log Level: $LOG_LEVEL"
        echo "Database: $DATABASE_URL"
        echo "Cache TTL: $CACHE_TTL seconds"
        echo
        echo "🚩 Feature Flags:"
        env | grep "FEATURE_" | sort
        echo
        echo "⚙️  System Configuration:"
        echo "API Rate Limit: $API_RATE_LIMIT"
        echo "External API Timeout: $EXTERNAL_API_TIMEOUT seconds"
        echo "Debug Mode: $DEBUG_MODE"
        echo "Metrics Enabled: $METRICS_ENABLED"
        echo "=================================="
      }
      
      # Show initial state
      show_environment_info
      
      # Monitor for changes every 60 seconds
      while true; do
        sleep 60
        show_environment_info
      done
    envFrom:
    - configMapRef:
        name: app-config
    - configMapRef:
        name: feature-flags
        prefix: FEATURE_
    - configMapRef:
        name: version-info
        prefix: VERSION_
EOF

# Step 5: Create testing script
cat > scripts/test-deployment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}

# Color output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing deployment in $ENVIRONMENT environment"

# Test 1: Check ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
for cm in app-config feature-flags version-info; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Check application is running
log_info "Test 2: Verifying application pods are running..."
if kubectl get deployment webapp -n "$NAMESPACE" &> /dev/null; then
    READY=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    if [ "$READY" = "$DESIRED" ]; then
        log_success "Application deployment ready ($READY/$DESIRED pods)"
    else
        log_error "Application deployment not ready ($READY/$DESIRED pods)"
    fi
else
    log_error "Application deployment not found"
fi

# Test 3: Test application response
log_info "Test 3: Testing application response..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080 | grep -q "$ENVIRONMENT"; then
    log_success "Application responds with correct environment"
else
    log_error "Application response incorrect or unreachable"
fi

kill $PORT_FORWARD_PID 2>/dev/null

# Test 4: Check environment inspector
log_info "Test 4: Checking environment inspector..."
if kubectl get pod environment-inspector -n "$NAMESPACE" &> /dev/null; then
    log_success "Environment inspector pod exists"
    log_info "Recent inspector logs:"
    kubectl logs environment-inspector -n "$NAMESPACE" --tail=10
else
    log_error "Environment inspector pod not found"
fi

log_success "🎉 All tests completed for $ENVIRONMENT environment"
EOF

chmod +x scripts/test-deployment.sh

# Step 6: Demo the pipeline
log_info() { echo -e "\033[0;34m[INFO]\033[0m $1"; }

log_info "🚀 Starting Environment Pipeline Demo"

# Deploy to development
log_info "Deploying to development environment..."
./scripts/deploy-environment.sh development webapp-dev v1.0.0

# Apply the application
kubectl apply -f applications/webapp-deployment.yaml -n webapp-dev

# Wait for deployment
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-dev

# Test the deployment
./scripts/test-deployment.sh development webapp-dev

log_info "✅ Development deployment complete!"

# Now let's demonstrate staging deployment
log_info "Deploying to staging environment..."
./scripts/deploy-environment.sh staging webapp-staging v1.0.0

# Apply application to staging
kubectl apply -f applications/webapp-deployment.yaml -n webapp-staging

log_info "Waiting for staging deployment..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-staging

# Test staging
./scripts/test-deployment.sh staging webapp-staging

log_info "✅ Staging deployment complete!"

# Cleanup function
cd .. # Go back to parent directory
```

**What you learned from this project:**
- How to organize environment-specific configurations
- Automated deployment pipelines with ConfigMaps
- Testing and validation strategies
- Real-world deployment patterns

## 🎓 Advanced Level: Production-Ready Patterns

### Advanced Demo 1: ConfigMap Validation and Rollback Strategy

Let's implement a production-ready configuration management system with validation and rollback capabilities:

```bash
# Create advanced configuration management system
mkdir -p advanced-configmaps/{configs,validators,scripts}
cd advanced-configmaps

# Step 1: Create configuration validator
cat > validators/config-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-validator-script
data:
  validate-config.sh: |
    #!/bin/bash
    set -e
    
    CONFIG_FILE=${1:-/config/app.env}
    ERRORS=0
    
    echo "🔍 Validating configuration file: $CONFIG_FILE"
    
    # Check if file exists
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "❌ Configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    # Validation rules
    validate_url() {
        local var_name=$1
        local url=$2
        if [[ ! "$url" =~ ^[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9.-]+:[0-9]+.*$ ]]; then
            echo "❌ Invalid URL format for $var_name: $url"
            ((ERRORS++))
        else
            echo "✅ Valid URL for $var_name"
        fi
    }
    
    validate_number() {
        local var_name=$1
        local number=$2
        local min=${3:-0}
        local max=${4:-999999}
        if ! [[ "$number" =~ ^[0-9]+$ ]] || [ "$number" -lt "$min" ] || [ "$number" -gt "$max" ]; then
            echo "❌ Invalid number for $var_name: $number (must be $min-$max)"
            ((ERRORS++))
        else
            echo "✅ Valid number for $var_name"
        fi
    }
    
    validate_boolean() {
        local var_name=$1
        local bool=$2
        if [[ ! "$bool" =~ ^(true|false)$ ]]; then
            echo "❌ Invalid boolean for $var_name: $bool (must be true/false)"
            ((ERRORS++))
        else
            echo "✅ Valid boolean for $var_name"
        fi
    }
    
    validate_log_level() {
        local level=$1
        if [[ ! "$level" =~ ^(debug|info|warn|error)$ ]]; then
            echo "❌ Invalid log level: $level (must be debug|info|warn|error)"
            ((ERRORS++))
        else
            echo "✅ Valid log level"
        fi
    }
    
    # Parse and validate configuration
    while IFS='=' read -r key value; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ ]] && continue
        [[ -z "$key" ]] && continue
        
        case "$key" in
            "DATABASE_URL"|"REDIS_URL")
                validate_url "$key" "$value"
                ;;
            "DATABASE_PORT"|"REDIS_PORT"|"APP_PORT")
                validate_number "$key" "$value" 1 65535
                ;;
            "API_RATE_LIMIT")
                validate_number "$key" "$value" 1 100000
                ;;
            "CACHE_TTL"|"EXTERNAL_API_TIMEOUT")
                validate_number "$key" "$value" 1 86400
                ;;
            "DEBUG_MODE"|"METRICS_ENABLED"|"PROFILING_ENABLED")
                validate_boolean "$key" "$value"
                ;;
            "LOG_LEVEL")
                validate_log_level "$value"
                ;;
        esac
    done < "$CONFIG_FILE"
    
    echo "📊 Validation complete. Errors found: $ERRORS"
    
    if [ $ERRORS -eq 0 ]; then
        echo "🎉 Configuration validation passed!"
        exit 0
    else
        echo "💥 Configuration validation failed!"
        exit 1
    fi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator
spec:
  template:
    spec:
      containers:
      - name: validator
        image: busybox
        command: ["sh", "/scripts/validate-config.sh"]
        volumeMounts:
        - name: config-to-validate
          mountPath: /config
        - name: validator-script
          mountPath: /scripts
      volumes:
      - name: config-to-validate
        configMap:
          name: app-config-candidate  # The config we want to validate
      - name: validator-script
        configMap:
          name: config-validator-script
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3
EOF

# Step 2: Create configuration rollback system
cat > scripts/config-rollback-manager.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_NAME=${2:-app-config}
ACTION=${3:-backup}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

case "$ACTION" in
    "backup")
        log_info "🔄 Creating backup of ConfigMap: $CONFIG_NAME"
        
        # Get current timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_NAME="${CONFIG_NAME}-backup-${TIMESTAMP}"
        
        # Check if ConfigMap exists
        if ! kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" &> /dev/null; then
            log_error "ConfigMap $CONFIG_NAME not found in namespace $NAMESPACE"
            exit 1
        fi
        
        # Create backup
        kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $CONFIG_NAME/name: $BACKUP_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        kubectl apply -f -
        
        log_success "✅ Backup created: $BACKUP_NAME"
        
        # Label the backup for easy identification
        kubectl label configmap "$BACKUP_NAME" -n "$NAMESPACE" \
            backup-of="$CONFIG_NAME" \
            backup-timestamp="$TIMESTAMP"
        
        echo "Backup ConfigMap: $BACKUP_NAME"
        ;;
        
    "list-backups")
        log_info "📋 Listing backups for ConfigMap: $CONFIG_NAME"
        kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o custom-columns="NAME:.metadata.name,TIMESTAMP:.metadata.labels.backup-timestamp,AGE:.metadata.creationTimestamp"
        ;;
        
    "rollback")
        BACKUP_NAME=$4
        if [ -z "$BACKUP_NAME" ]; then
            log_error "Backup name required for rollback"
            log_info "Usage: $0 <namespace> <config-name> rollback <backup-name>"
            log_info "Use 'list-backups' to see available backups"
            exit 1
        fi
        
        log_warning "⚠️  Rolling back $CONFIG_NAME to backup: $BACKUP_NAME"
        read -p "Are you sure? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Rollback cancelled"
            exit 0
        fi
        
        # Create backup of current state before rollback
        ./config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup
        
        # Perform rollback
        kubectl get configmap "$BACKUP_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $BACKUP_NAME/name: $CONFIG_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        sed '/backup-of:/d' | \
        sed '/backup-timestamp:/d' | \
        kubectl apply -f -
        
        log_success "✅ Rollback completed: $CONFIG_NAME restored from $BACKUP_NAME"
        ;;
        
    "cleanup")
        KEEP_COUNT=${4:-5}
        log_info "🧹 Cleaning up old backups, keeping $KEEP_COUNT most recent"
        
        # Get backup ConfigMaps sorted by timestamp (oldest first)
        BACKUPS=$(kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o jsonpath='{.items[*].metadata.name}')
        
        BACKUP_ARRAY=($BACKUPS)
        TOTAL_BACKUPS=${#BACKUP_ARRAY[@]}
        
        if [ $TOTAL_BACKUPS -le $KEEP_COUNT ]; then
            log_info "Found $TOTAL_BACKUPS backups, keeping all (within limit of $KEEP_COUNT)"
            exit 0
        fi
        
        DELETE_COUNT=$((TOTAL_BACKUPS - KEEP_COUNT))
        log_warning "Found $TOTAL_BACKUPS backups, will delete $DELETE_COUNT oldest"
        
        for ((i=0; i<DELETE_COUNT; i++)); do
            BACKUP_TO_DELETE=${BACKUP_ARRAY[$i]}
            log_info "Deleting backup: $BACKUP_TO_DELETE"
            kubectl delete configmap "$BACKUP_TO_DELETE" -n "$NAMESPACE"
        done
        
        log_success "✅ Cleanup completed, kept $KEEP_COUNT most recent backups"
        ;;
        
    *)
        log_error "Unknown action: $ACTION"
        log_info "Available actions: backup, list-backups, rollback, cleanup"
        exit 1
        ;;
esac
EOF

chmod +x scripts/config-rollback-manager.sh

# Step 3: Create safe configuration update system
cat > scripts/safe-config-update.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_FILE=${2}
CONFIG_NAME=${3:-app-config}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <namespace> <config-file> [config-name]"
    echo "Example: $0 default new-config.env app-config"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[SAFE-UPDATE]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🛡️  Starting safe configuration update process"
log_info "Target: $CONFIG_NAME in namespace $NAMESPACE"
log_info "Source: $CONFIG_FILE"

# Step 1: Validate the configuration file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Step 2: Create backup of current configuration
log_info "📦 Creating backup of current configuration..."
./scripts/config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup

# Step 3: Create candidate ConfigMap for validation
log_info "🧪 Creating candidate configuration for validation..."
kubectl create configmap app-config-candidate \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 4: Run validation
log_info "✅ Running configuration validation..."
kubectl apply -f validators/config-validator.yaml -n "$NAMESPACE"

# Wait for validation job to complete
log_info "Waiting for validation to complete..."
kubectl wait --for=condition=complete --timeout=300s job/config-validator -n "$NAMESPACE"

# Check validation result
if kubectl get job config-validator -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' | grep -q "Complete"; then
    log_success "🎉 Configuration validation passed!"
else
    log_error "💥 Configuration validation failed!"
    kubectl logs job/config-validator -n "$NAMESPACE"
    
    # Cleanup candidate ConfigMap
    kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
    kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found
    exit 1
fi

# Step 5: Apply the validated configuration
log_info "🚀 Applying validated configuration..."
kubectl create configmap "$CONFIG_NAME" \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 6: Verify applications are still healthy
log_info "🏥 Checking application health after configuration update..."
sleep 10

# Check if we have deployments using this ConfigMap
DEPLOYMENTS=$(kubectl get deployments -n "$NAMESPACE" -o json | \
    jq -r '.items[] | select(.spec.template.spec.containers[].envFrom[]?.configMapRef.name == "'$CONFIG_NAME'") | .metadata.name')

if [ -n "$DEPLOYMENTS" ]; then
    for deployment in $DEPLOYMENTS; do
        log_info "Checking deployment: $deployment"
        if kubectl get deployment "$deployment" -n "$NAMESPACE" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_success "✅ Deployment $deployment is healthy"
        else
            log_warning "⚠️  Deployment $deployment may have issues"
        fi
    done
else
    log_info "No deployments found using this ConfigMap"
fi

# Step 7: Cleanup
log_info "🧹 Cleaning up validation resources..."
kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found

log_success "🎉 Safe configuration update completed successfully!"
log_info "💡 To rollback if needed, use:"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME list-backups"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME rollback <backup-name>"
EOF

chmod +x scripts/safe-config-update.sh

# Step 4: Demo the advanced system
log_info "🎭 Demonstrating Advanced Configuration Management"

# Create a namespace for our demo
kubectl create namespace config-advanced --dry-run=client -o yaml | kubectl apply -f -

# Create initial configuration
cat > configs/initial-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=info
DATABASE_URL=postgres://prod-db:5432/myapp
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Apply initial configuration
kubectl create configmap app-config \
    --from-env-file=configs/initial-config.env \
    --namespace=config-advanced

# Create an application that uses this config
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-demo-app
  namespace: config-advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: config-demo
  template:
    metadata:
      labels:
        app: config-demo
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - -text=Config Demo App - Environment: $(APP_ENVIRONMENT), Log Level: $(LOG_LEVEL)
        ports:
        - containerPort: 5678
        envFrom:
        - configMapRef:
            name: app-config
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Wait for deployment
kubectl wait --for=condition=available --timeout=300s deployment/config-demo-app -n config-advanced

# Test the system
log_info "🧪 Testing the advanced configuration management system"

# 1. Create a backup
log_info "Step 1: Creating backup..."
./scripts/config-rollback-manager.sh config-advanced app-config backup

# 2. List backups
log_info "Step 2: Listing backups..."
./scripts/config-rollback-manager.sh config-advanced app-config list-backups

# 3. Create an invalid configuration to test validation
cat > configs/invalid-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=invalid_level
DATABASE_URL=not-a-valid-url
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=not_a_number
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=maybe
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

log_info "Step 3: Testing with invalid configuration (should fail)..."
if ./scripts/safe-config-update.sh config-advanced configs/invalid-config.env app-config; then
    log_error "❌ Validation should have failed!"
else
    log_success "✅ Validation correctly failed for invalid configuration"
fi

# 4. Create a valid updated configuration
cat > configs/updated-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=debug
DATABASE_URL=postgres://new-prod-db:5432/myapp
REDIS_URL=redis://new-prod-redis:6379/0
API_RATE_LIMIT=10000
CACHE_TTL=7200
EXTERNAL_API_TIMEOUT=60
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

log_info "Step 4: Applying valid updated configuration..."
./scripts/safe-config-update.sh config-advanced configs/updated-config.env app-config

# 5. Verify the update
log_info "Step 5: Verifying configuration update..."
kubectl get configmap app-config -n config-advanced -o yaml | grep -A 20 "data:"

# 6. Test rollback
log_info "Step 6: Testing rollback functionality..."
BACKUP_NAME=$(./scripts/config-rollback-manager.sh config-advanced app-config list-backups | tail -1 | awk '{print $1}')
if [ -n "$BACKUP_NAME" ] && [ "$BACKUP_NAME" != "NAME" ]; then
    echo "y" | ./scripts/config-rollback-manager.sh config-advanced app-config rollback "$BACKUP_NAME"
    log_success "✅ Rollback test completed"
fi

log_success "🎉 Advanced Configuration Management Demo Complete!"

cd .. # Go back to parent directory
```

### Advanced Demo 2: Multi-Environment GitOps Configuration Pipeline

```bash
# Create GitOps-style configuration management
mkdir -p gitops-configs/{environments,applications,kustomization}
cd gitops-configs

# Base configuration (shared across all environments)
mkdir -p environments/base
cat > environments/base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: "MyWebApp"
  APP_PORT: "8080"
  METRICS_ENABLED: "true"
  HEALTH_CHECK_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_METRICS_DASHBOARD: "true"
  FEATURE_HEALTH_CHECKS: "true"
EOF

cat > environments/base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- configmap.yaml

commonLabels:
  managed-by: kustomize
  component: configuration
EOF

# Development overlay
mkdir -p environments/overlays/development
cat > environments/overlays/development/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "development"
  LOG_LEVEL: "debug"
  DATABASE_URL: "postgres://dev-db:5432/myapp_dev"
  REDIS_URL: "redis://dev-redis:6379/0"
  API_RATE_LIMIT: "100"
  CACHE_TTL: "60"
  DEBUG_MODE: "true"
  EXTERNAL_API_TIMEOUT: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "true"
  FEATURE_EXPERIMENTAL_API: "true"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/development/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-dev

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: development
  
images:
- name: webapp
  newTag: latest
EOF

# Staging overlay
mkdir -p environments/overlays/staging
cat > environments/overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "staging"
  LOG_LEVEL: "info"
  DATABASE_URL: "postgres://staging-db:5432/myapp_staging"
  REDIS_URL: "redis://staging-redis:6379/0"
  API_RATE_LIMIT: "1000"
  CACHE_TTL: "300"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "15"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-staging

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: staging
  
images:
- name: webapp
  newTag: v1.0.0-rc
EOF

# Production overlay
mkdir -p environments/overlays/production
cat > environments/overlays/production/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "production"
  LOG_LEVEL: "warn"
  DATABASE_URL: "postgres://prod-db:5432/myapp_production"
  REDIS_URL: "redis://prod-redis:6379/0"
  API_RATE_LIMIT: "5000"
  CACHE_TTL: "3600"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "30"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "false"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "false"
EOF

cat > environments/overlays/production/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-prod

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: production
  
images:
- name: webapp
  newTag: v1.0.0
EOF

# Application deployment template
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      
  ,
                'port': lambda x: 1 <= int(x) <= 65535,
                'ssl_mode': lambda x: x in ['disable', 'require', 'verify-ca'],
                'timeout': lambda x: 1 <= int(x) <= 300,
                'pool_size': lambda x: 1 <= int(x) <= 100
            }
        },
        'application': {
            'required': ['name', 'environment'],
            'optional': ['debug', 'log_level', 'port'],
            'validators': {
                'name': r'^[a-zA-Z][a-zA-Z0-9-]*
      # Mock backend container
      - name: backend
        image: hashicorp/http-echo:latest
        args:
        - -text=Hello from backend configured via ConfigMap!
        - -listen=:8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: log_level
        volumeMounts:
        - name: backend-config-volume
          mountPath: /etc/config
      # Nginx proxy container
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: processed-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 30
      volumes:
      - name: shared-config-volume
        configMap:
          name: shared-config
      - name: backend-config-volume
        configMap:
          name: backend-config
      - name: nginx-template-volume
        configMap:
          name: nginx-template
      - name: processed-config
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: multi-container-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Step 5: Test the deployment
kubectl get pods -l app=multi-container-app
kubectl logs deployment/multi-container-app -c nginx-config-processor
kubectl logs deployment/multi-container-app -c backend
kubectl logs deployment/multi-container-app -c nginx

# Test the application
kubectl port-forward service/multi-container-service 8080:80 &
curl http://localhost:8080
# Stop port forwarding with: kill %1
```

**What you learned:** How to share configuration between multiple containers, use init containers to process configuration templates, and create realistic multi-tier applications with ConfigMaps.

### Mini-Project 2: Dynamic Configuration Updates

Let's explore ConfigMap hot-reloading with a practical monitoring scenario:

```bash
# Step 1: Create a monitoring configuration
kubectl create configmap monitoring-config \
  --from-literal=scrape_interval=15s \
  --from-literal=evaluation_interval=15s \
  --from-literal=alert_threshold=80 \
  --from-literal=retention_days=30 \
  --from-literal=log_level=info

# Step 2: Create a Prometheus-style configuration file
cat > prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml

# Step 3: Create a configuration-aware application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-watcher-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: config-watcher
  template:
    metadata:
      labels:
        app: config-watcher
    spec:
      containers:
      - name: config-watcher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Configuration Watcher Started"
          echo "Monitoring config files for changes..."
          
          # Function to display current configuration
          show_config() {
            echo "=== Current Configuration ($(date)) ==="
            echo "Environment Variables:"
            env | grep -E "(SCRAPE_|EVALUATION_|ALERT_|RETENTION_|LOG_)" | sort
            echo
            echo "Configuration Files:"
            if [ -f /etc/config/scrape_interval ]; then
              echo "Scrape Interval: $(cat /etc/config/scrape_interval)"
            fi
            if [ -f /etc/config/alert_threshold ]; then
              echo "Alert Threshold: $(cat /etc/config/alert_threshold)"
            fi
            if [ -f /etc/prometheus/prometheus.yml ]; then
              echo "Prometheus Config Present: Yes"
              echo "Config size: $(wc -l < /etc/prometheus/prometheus.yml) lines"
            fi
            echo "================================"
          }
          
          # Show initial configuration
          show_config
          
          # Monitor for file changes
          while true; do
            sleep 30
            show_config
          done
        env:
        - name: SCRAPE_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: scrape_interval
        - name: EVALUATION_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: evaluation_interval
        - name: ALERT_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: alert_threshold
        - name: RETENTION_DAYS
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: retention_days
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: log_level
        volumeMounts:
        - name: monitoring-config-volume
          mountPath: /etc/config
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: monitoring-config-volume
        configMap:
          name: monitoring-config
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
EOF

# Step 4: Watch the initial configuration
kubectl logs deployment/config-watcher-app -f &

# Step 5: Update the configuration and observe changes
echo "Updating scrape interval..."
kubectl patch configmap monitoring-config -p '{"data":{"scrape_interval":"30s","alert_threshold":"90"}}'

echo "Waiting for changes to propagate (up to 60 seconds)..."
sleep 65

# Update the Prometheus configuration
cat > prometheus-updated.yml << 'EOF'
global:
  scrape_interval: 30s  # Updated
  evaluation_interval: 30s  # Updated

rule_files:
  - "alerts.yml"
  - "recording.yml"  # Added

scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 10s  # Override global
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
      
  - job_name: 'kubernetes-services'  # New job
    kubernetes_sd_configs:
    - role: service

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093', 'alertmanager-backup:9093']  # Added backup
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml=prometheus-updated.yml --dry-run=client -o yaml | kubectl apply -f -

echo "Configuration updated. Check the logs for changes..."
# Stop log following with: kill %1
```

**Key Insights from this project:**
- Environment variables from ConfigMaps don't update automatically (require pod restart)
- Volume-mounted ConfigMaps update automatically (with up to 60-second delay)
- Applications should be designed to handle configuration changes gracefully

### Mini-Project 3: Environment-Specific Deployment Pipeline

Let's create a realistic deployment pipeline that manages configurations across different environments:

```bash
# Create directory structure for the project
mkdir -p configmap-pipeline/{environments,applications,scripts}
cd configmap-pipeline

# Step 1: Create environment-specific configurations
cat > environments/development.env << 'EOF'
# Development Environment Configuration
APP_ENVIRONMENT=development
LOG_LEVEL=debug
DATABASE_URL=dev-postgres:5432/myapp_dev
REDIS_URL=dev-redis:6379/0
API_RATE_LIMIT=100
CACHE_TTL=60
EXTERNAL_API_TIMEOUT=5
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

cat > environments/staging.env << 'EOF'
# Staging Environment Configuration
APP_ENVIRONMENT=staging
LOG_LEVEL=info
DATABASE_URL=staging-postgres:5432/myapp_staging
REDIS_URL=staging-redis:6379/0
API_RATE_LIMIT=1000
CACHE_TTL=300
EXTERNAL_API_TIMEOUT=15
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

cat > environments/production.env << 'EOF'
# Production Environment Configuration
APP_ENVIRONMENT=production
LOG_LEVEL=warn
DATABASE_URL=prod-postgres:5432/myapp_production
REDIS_URL=prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Step 2: Create feature flag configurations
cat > environments/dev-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=true
FEATURE_EXPERIMENTAL_API=true
FEATURE_A_B_TESTING=true
EOF

cat > environments/staging-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=true
EOF

cat > environments/prod-features.env << 'EOF'
FEATURE_NEW_UI=false
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=false
EOF

# Step 3: Create deployment script
cat > scripts/deploy-environment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}
APP_VERSION=${3:-latest}

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying to environment: $ENVIRONMENT in namespace: $NAMESPACE"

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Function to create or update ConfigMap
create_or_update_configmap() {
    local name=$1
    local file=$2
    local description=$3
    
    log_info "Creating/updating ConfigMap: $name ($description)"
    
    if kubectl get configmap "$name" -n "$NAMESPACE" &> /dev/null; then
        log_warning "ConfigMap $name exists, updating..."
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl apply -f -
    else
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE"
    fi
}

# Deploy environment-specific configurations
create_or_update_configmap "app-config" "environments/$ENVIRONMENT.env" "Application Configuration"
create_or_update_configmap "feature-flags" "environments/$ENVIRONMENT-features.env" "Feature Flags"

# Create version info ConfigMap
log_info "Creating version information ConfigMap..."
kubectl create configmap version-info \
    --from-literal=version="$APP_VERSION" \
    --from-literal=environment="$ENVIRONMENT" \
    --from-literal=deployed_at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --from-literal=deployed_by="$(whoami)" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

log_success "✅ Configuration deployment complete for $ENVIRONMENT environment"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   App Version: $APP_VERSION"
echo "   ConfigMaps created:"
echo "     - app-config"
echo "     - feature-flags"
echo "     - version-info"

log_info "🔍 To view configurations:"
echo "   kubectl get configmaps -n $NAMESPACE"
echo "   kubectl describe configmap app-config -n $NAMESPACE"
echo "   kubectl describe configmap feature-flags -n $NAMESPACE"
EOF

chmod +x scripts/deploy-environment.sh

# Step 4: Create the application deployment
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: hashicorp/http-echo:latest
        ports:
        - containerPort: 5678
        # Load environment configuration
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        - configMapRef:
            name: version-info
            prefix: VERSION_
        # Custom startup message based on configuration
        args:
        - -text=Hello from $(VERSION_environment) environment! App version $(VERSION_version) deployed at $(VERSION_deployed_at)
        livenessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 5678
  type: ClusterIP
---
# ConfigMap for application monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*webapp*.log
        Parser            docker
        Tag               webapp.*
        Refresh_Interval  5
    
    [OUTPUT]
        Name  stdout
        Match *
---
# Health check and debugging pod
apiVersion: v1
kind: Pod
metadata:
  name: environment-inspector
  labels:
    app: inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "🔍 Environment Inspector Started"
      echo "=================================="
      
      show_environment_info() {
        echo "📊 Environment Information ($(date))"
        echo "Environment: $APP_ENVIRONMENT"
        echo "Version: $VERSION_version"
        echo "Log Level: $LOG_LEVEL"
        echo "Database: $DATABASE_URL"
        echo "Cache TTL: $CACHE_TTL seconds"
        echo
        echo "🚩 Feature Flags:"
        env | grep "FEATURE_" | sort
        echo
        echo "⚙️  System Configuration:"
        echo "API Rate Limit: $API_RATE_LIMIT"
        echo "External API Timeout: $EXTERNAL_API_TIMEOUT seconds"
        echo "Debug Mode: $DEBUG_MODE"
        echo "Metrics Enabled: $METRICS_ENABLED"
        echo "=================================="
      }
      
      # Show initial state
      show_environment_info
      
      # Monitor for changes every 60 seconds
      while true; do
        sleep 60
        show_environment_info
      done
    envFrom:
    - configMapRef:
        name: app-config
    - configMapRef:
        name: feature-flags
        prefix: FEATURE_
    - configMapRef:
        name: version-info
        prefix: VERSION_
EOF

# Step 5: Create testing script
cat > scripts/test-deployment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}

# Color output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing deployment in $ENVIRONMENT environment"

# Test 1: Check ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
for cm in app-config feature-flags version-info; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Check application is running
log_info "Test 2: Verifying application pods are running..."
if kubectl get deployment webapp -n "$NAMESPACE" &> /dev/null; then
    READY=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    if [ "$READY" = "$DESIRED" ]; then
        log_success "Application deployment ready ($READY/$DESIRED pods)"
    else
        log_error "Application deployment not ready ($READY/$DESIRED pods)"
    fi
else
    log_error "Application deployment not found"
fi

# Test 3: Test application response
log_info "Test 3: Testing application response..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080 | grep -q "$ENVIRONMENT"; then
    log_success "Application responds with correct environment"
else
    log_error "Application response incorrect or unreachable"
fi

kill $PORT_FORWARD_PID 2>/dev/null

# Test 4: Check environment inspector
log_info "Test 4: Checking environment inspector..."
if kubectl get pod environment-inspector -n "$NAMESPACE" &> /dev/null; then
    log_success "Environment inspector pod exists"
    log_info "Recent inspector logs:"
    kubectl logs environment-inspector -n "$NAMESPACE" --tail=10
else
    log_error "Environment inspector pod not found"
fi

log_success "🎉 All tests completed for $ENVIRONMENT environment"
EOF

chmod +x scripts/test-deployment.sh

# Step 6: Demo the pipeline
log_info() { echo -e "\033[0;34m[INFO]\033[0m $1"; }

log_info "🚀 Starting Environment Pipeline Demo"

# Deploy to development
log_info "Deploying to development environment..."
./scripts/deploy-environment.sh development webapp-dev v1.0.0

# Apply the application
kubectl apply -f applications/webapp-deployment.yaml -n webapp-dev

# Wait for deployment
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-dev

# Test the deployment
./scripts/test-deployment.sh development webapp-dev

log_info "✅ Development deployment complete!"

# Now let's demonstrate staging deployment
log_info "Deploying to staging environment..."
./scripts/deploy-environment.sh staging webapp-staging v1.0.0

# Apply application to staging
kubectl apply -f applications/webapp-deployment.yaml -n webapp-staging

log_info "Waiting for staging deployment..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-staging

# Test staging
./scripts/test-deployment.sh staging webapp-staging

log_info "✅ Staging deployment complete!"

# Cleanup function
cd .. # Go back to parent directory
```

**What you learned from this project:**
- How to organize environment-specific configurations
- Automated deployment pipelines with ConfigMaps
- Testing and validation strategies
- Real-world deployment patterns

## 🎓 Advanced Level: Production-Ready Patterns

### Advanced Demo 1: ConfigMap Validation and Rollback Strategy

Let's implement a production-ready configuration management system with validation and rollback capabilities:

```bash
# Create advanced configuration management system
mkdir -p advanced-configmaps/{configs,validators,scripts}
cd advanced-configmaps

# Step 1: Create configuration validator
cat > validators/config-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-validator-script
data:
  validate-config.sh: |
    #!/bin/bash
    set -e
    
    CONFIG_FILE=${1:-/config/app.env}
    ERRORS=0
    
    echo "🔍 Validating configuration file: $CONFIG_FILE"
    
    # Check if file exists
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "❌ Configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    # Validation rules
    validate_url() {
        local var_name=$1
        local url=$2
        if [[ ! "$url" =~ ^[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9.-]+:[0-9]+.*$ ]]; then
            echo "❌ Invalid URL format for $var_name: $url"
            ((ERRORS++))
        else
            echo "✅ Valid URL for $var_name"
        fi
    }
    
    validate_number() {
        local var_name=$1
        local number=$2
        local min=${3:-0}
        local max=${4:-999999}
        if ! [[ "$number" =~ ^[0-9]+$ ]] || [ "$number" -lt "$min" ] || [ "$number" -gt "$max" ]; then
            echo "❌ Invalid number for $var_name: $number (must be $min-$max)"
            ((ERRORS++))
        else
            echo "✅ Valid number for $var_name"
        fi
    }
    
    validate_boolean() {
        local var_name=$1
        local bool=$2
        if [[ ! "$bool" =~ ^(true|false)$ ]]; then
            echo "❌ Invalid boolean for $var_name: $bool (must be true/false)"
            ((ERRORS++))
        else
            echo "✅ Valid boolean for $var_name"
        fi
    }
    
    validate_log_level() {
        local level=$1
        if [[ ! "$level" =~ ^(debug|info|warn|error)$ ]]; then
            echo "❌ Invalid log level: $level (must be debug|info|warn|error)"
            ((ERRORS++))
        else
            echo "✅ Valid log level"
        fi
    }
    
    # Parse and validate configuration
    while IFS='=' read -r key value; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ ]] && continue
        [[ -z "$key" ]] && continue
        
        case "$key" in
            "DATABASE_URL"|"REDIS_URL")
                validate_url "$key" "$value"
                ;;
            "DATABASE_PORT"|"REDIS_PORT"|"APP_PORT")
                validate_number "$key" "$value" 1 65535
                ;;
            "API_RATE_LIMIT")
                validate_number "$key" "$value" 1 100000
                ;;
            "CACHE_TTL"|"EXTERNAL_API_TIMEOUT")
                validate_number "$key" "$value" 1 86400
                ;;
            "DEBUG_MODE"|"METRICS_ENABLED"|"PROFILING_ENABLED")
                validate_boolean "$key" "$value"
                ;;
            "LOG_LEVEL")
                validate_log_level "$value"
                ;;
        esac
    done < "$CONFIG_FILE"
    
    echo "📊 Validation complete. Errors found: $ERRORS"
    
    if [ $ERRORS -eq 0 ]; then
        echo "🎉 Configuration validation passed!"
        exit 0
    else
        echo "💥 Configuration validation failed!"
        exit 1
    fi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator
spec:
  template:
    spec:
      containers:
      - name: validator
        image: busybox
        command: ["sh", "/scripts/validate-config.sh"]
        volumeMounts:
        - name: config-to-validate
          mountPath: /config
        - name: validator-script
          mountPath: /scripts
      volumes:
      - name: config-to-validate
        configMap:
          name: app-config-candidate  # The config we want to validate
      - name: validator-script
        configMap:
          name: config-validator-script
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3
EOF

# Step 2: Create configuration rollback system
cat > scripts/config-rollback-manager.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_NAME=${2:-app-config}
ACTION=${3:-backup}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

case "$ACTION" in
    "backup")
        log_info "🔄 Creating backup of ConfigMap: $CONFIG_NAME"
        
        # Get current timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_NAME="${CONFIG_NAME}-backup-${TIMESTAMP}"
        
        # Check if ConfigMap exists
        if ! kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" &> /dev/null; then
            log_error "ConfigMap $CONFIG_NAME not found in namespace $NAMESPACE"
            exit 1
        fi
        
        # Create backup
        kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $CONFIG_NAME/name: $BACKUP_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        kubectl apply -f -
        
        log_success "✅ Backup created: $BACKUP_NAME"
        
        # Label the backup for easy identification
        kubectl label configmap "$BACKUP_NAME" -n "$NAMESPACE" \
            backup-of="$CONFIG_NAME" \
            backup-timestamp="$TIMESTAMP"
        
        echo "Backup ConfigMap: $BACKUP_NAME"
        ;;
        
    "list-backups")
        log_info "📋 Listing backups for ConfigMap: $CONFIG_NAME"
        kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o custom-columns="NAME:.metadata.name,TIMESTAMP:.metadata.labels.backup-timestamp,AGE:.metadata.creationTimestamp"
        ;;
        
    "rollback")
        BACKUP_NAME=$4
        if [ -z "$BACKUP_NAME" ]; then
            log_error "Backup name required for rollback"
            log_info "Usage: $0 <namespace> <config-name> rollback <backup-name>"
            log_info "Use 'list-backups' to see available backups"
            exit 1
        fi
        
        log_warning "⚠️  Rolling back $CONFIG_NAME to backup: $BACKUP_NAME"
        read -p "Are you sure? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Rollback cancelled"
            exit 0
        fi
        
        # Create backup of current state before rollback
        ./config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup
        
        # Perform rollback
        kubectl get configmap "$BACKUP_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $BACKUP_NAME/name: $CONFIG_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        sed '/backup-of:/d' | \
        sed '/backup-timestamp:/d' | \
        kubectl apply -f -
        
        log_success "✅ Rollback completed: $CONFIG_NAME restored from $BACKUP_NAME"
        ;;
        
    "cleanup")
        KEEP_COUNT=${4:-5}
        log_info "🧹 Cleaning up old backups, keeping $KEEP_COUNT most recent"
        
        # Get backup ConfigMaps sorted by timestamp (oldest first)
        BACKUPS=$(kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o jsonpath='{.items[*].metadata.name}')
        
        BACKUP_ARRAY=($BACKUPS)
        TOTAL_BACKUPS=${#BACKUP_ARRAY[@]}
        
        if [ $TOTAL_BACKUPS -le $KEEP_COUNT ]; then
            log_info "Found $TOTAL_BACKUPS backups, keeping all (within limit of $KEEP_COUNT)"
            exit 0
        fi
        
        DELETE_COUNT=$((TOTAL_BACKUPS - KEEP_COUNT))
        log_warning "Found $TOTAL_BACKUPS backups, will delete $DELETE_COUNT oldest"
        
        for ((i=0; i<DELETE_COUNT; i++)); do
            BACKUP_TO_DELETE=${BACKUP_ARRAY[$i]}
            log_info "Deleting backup: $BACKUP_TO_DELETE"
            kubectl delete configmap "$BACKUP_TO_DELETE" -n "$NAMESPACE"
        done
        
        log_success "✅ Cleanup completed, kept $KEEP_COUNT most recent backups"
        ;;
        
    *)
        log_error "Unknown action: $ACTION"
        log_info "Available actions: backup, list-backups, rollback, cleanup"
        exit 1
        ;;
esac
EOF

chmod +x scripts/config-rollback-manager.sh

# Step 3: Create safe configuration update system
cat > scripts/safe-config-update.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_FILE=${2}
CONFIG_NAME=${3:-app-config}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <namespace> <config-file> [config-name]"
    echo "Example: $0 default new-config.env app-config"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[SAFE-UPDATE]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🛡️  Starting safe configuration update process"
log_info "Target: $CONFIG_NAME in namespace $NAMESPACE"
log_info "Source: $CONFIG_FILE"

# Step 1: Validate the configuration file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Step 2: Create backup of current configuration
log_info "📦 Creating backup of current configuration..."
./scripts/config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup

# Step 3: Create candidate ConfigMap for validation
log_info "🧪 Creating candidate configuration for validation..."
kubectl create configmap app-config-candidate \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 4: Run validation
log_info "✅ Running configuration validation..."
kubectl apply -f validators/config-validator.yaml -n "$NAMESPACE"

# Wait for validation job to complete
log_info "Waiting for validation to complete..."
kubectl wait --for=condition=complete --timeout=300s job/config-validator -n "$NAMESPACE"

# Check validation result
if kubectl get job config-validator -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' | grep -q "Complete"; then
    log_success "🎉 Configuration validation passed!"
else
    log_error "💥 Configuration validation failed!"
    kubectl logs job/config-validator -n "$NAMESPACE"
    
    # Cleanup candidate ConfigMap
    kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
    kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found
    exit 1
fi

# Step 5: Apply the validated configuration
log_info "🚀 Applying validated configuration..."
kubectl create configmap "$CONFIG_NAME" \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 6: Verify applications are still healthy
log_info "🏥 Checking application health after configuration update..."
sleep 10

# Check if we have deployments using this ConfigMap
DEPLOYMENTS=$(kubectl get deployments -n "$NAMESPACE" -o json | \
    jq -r '.items[] | select(.spec.template.spec.containers[].envFrom[]?.configMapRef.name == "'$CONFIG_NAME'") | .metadata.name')

if [ -n "$DEPLOYMENTS" ]; then
    for deployment in $DEPLOYMENTS; do
        log_info "Checking deployment: $deployment"
        if kubectl get deployment "$deployment" -n "$NAMESPACE" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_success "✅ Deployment $deployment is healthy"
        else
            log_warning "⚠️  Deployment $deployment may have issues"
        fi
    done
else
    log_info "No deployments found using this ConfigMap"
fi

# Step 7: Cleanup
log_info "🧹 Cleaning up validation resources..."
kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found

log_success "🎉 Safe configuration update completed successfully!"
log_info "💡 To rollback if needed, use:"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME list-backups"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME rollback <backup-name>"
EOF

chmod +x scripts/safe-config-update.sh

# Step 4: Demo the advanced system
log_info "🎭 Demonstrating Advanced Configuration Management"

# Create a namespace for our demo
kubectl create namespace config-advanced --dry-run=client -o yaml | kubectl apply -f -

# Create initial configuration
cat > configs/initial-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=info
DATABASE_URL=postgres://prod-db:5432/myapp
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Apply initial configuration
kubectl create configmap app-config \
    --from-env-file=configs/initial-config.env \
    --namespace=config-advanced

# Create an application that uses this config
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-demo-app
  namespace: config-advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: config-demo
  template:
    metadata:
      labels:
        app: config-demo
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - -text=Config Demo App - Environment: $(APP_ENVIRONMENT), Log Level: $(LOG_LEVEL)
        ports:
        - containerPort: 5678
        envFrom:
        - configMapRef:
            name: app-config
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Wait for deployment
kubectl wait --for=condition=available --timeout=300s deployment/config-demo-app -n config-advanced

# Test the system
log_info "🧪 Testing the advanced configuration management system"

# 1. Create a backup
log_info "Step 1: Creating backup..."
./scripts/config-rollback-manager.sh config-advanced app-config backup

# 2. List backups
log_info "Step 2: Listing backups..."
./scripts/config-rollback-manager.sh config-advanced app-config list-backups

# 3. Create an invalid configuration to test validation
cat > configs/invalid-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=invalid_level
DATABASE_URL=not-a-valid-url
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=not_a_number
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=maybe
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

log_info "Step 3: Testing with invalid configuration (should fail)..."
if ./scripts/safe-config-update.sh config-advanced configs/invalid-config.env app-config; then
    log_error "❌ Validation should have failed!"
else
    log_success "✅ Validation correctly failed for invalid configuration"
fi

# 4. Create a valid updated configuration
cat > configs/updated-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=debug
DATABASE_URL=postgres://new-prod-db:5432/myapp
REDIS_URL=redis://new-prod-redis:6379/0
API_RATE_LIMIT=10000
CACHE_TTL=7200
EXTERNAL_API_TIMEOUT=60
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

log_info "Step 4: Applying valid updated configuration..."
./scripts/safe-config-update.sh config-advanced configs/updated-config.env app-config

# 5. Verify the update
log_info "Step 5: Verifying configuration update..."
kubectl get configmap app-config -n config-advanced -o yaml | grep -A 20 "data:"

# 6. Test rollback
log_info "Step 6: Testing rollback functionality..."
BACKUP_NAME=$(./scripts/config-rollback-manager.sh config-advanced app-config list-backups | tail -1 | awk '{print $1}')
if [ -n "$BACKUP_NAME" ] && [ "$BACKUP_NAME" != "NAME" ]; then
    echo "y" | ./scripts/config-rollback-manager.sh config-advanced app-config rollback "$BACKUP_NAME"
    log_success "✅ Rollback test completed"
fi

log_success "🎉 Advanced Configuration Management Demo Complete!"

cd .. # Go back to parent directory
```

### Advanced Demo 2: Multi-Environment GitOps Configuration Pipeline

```bash
# Create GitOps-style configuration management
mkdir -p gitops-configs/{environments,applications,kustomization}
cd gitops-configs

# Base configuration (shared across all environments)
mkdir -p environments/base
cat > environments/base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: "MyWebApp"
  APP_PORT: "8080"
  METRICS_ENABLED: "true"
  HEALTH_CHECK_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_METRICS_DASHBOARD: "true"
  FEATURE_HEALTH_CHECKS: "true"
EOF

cat > environments/base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- configmap.yaml

commonLabels:
  managed-by: kustomize
  component: configuration
EOF

# Development overlay
mkdir -p environments/overlays/development
cat > environments/overlays/development/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "development"
  LOG_LEVEL: "debug"
  DATABASE_URL: "postgres://dev-db:5432/myapp_dev"
  REDIS_URL: "redis://dev-redis:6379/0"
  API_RATE_LIMIT: "100"
  CACHE_TTL: "60"
  DEBUG_MODE: "true"
  EXTERNAL_API_TIMEOUT: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "true"
  FEATURE_EXPERIMENTAL_API: "true"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/development/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-dev

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: development
  
images:
- name: webapp
  newTag: latest
EOF

# Staging overlay
mkdir -p environments/overlays/staging
cat > environments/overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "staging"
  LOG_LEVEL: "info"
  DATABASE_URL: "postgres://staging-db:5432/myapp_staging"
  REDIS_URL: "redis://staging-redis:6379/0"
  API_RATE_LIMIT: "1000"
  CACHE_TTL: "300"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "15"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-staging

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: staging
  
images:
- name: webapp
  newTag: v1.0.0-rc
EOF

# Production overlay
mkdir -p environments/overlays/production
cat > environments/overlays/production/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "production"
  LOG_LEVEL: "warn"
  DATABASE_URL: "postgres://prod-db:5432/myapp_production"
  REDIS_URL: "redis://prod-redis:6379/0"
  API_RATE_LIMIT: "5000"
  CACHE_TTL: "3600"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "30"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "false"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "false"
EOF

cat > environments/overlays/production/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-prod

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: production
  
images:
- name: webapp
  newTag: v1.0.0
EOF

# Application deployment template
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      
  ,
                'environment': lambda x: x in ['development', 'staging', 'production'],
                'debug': lambda x: x.lower() in ['true', 'false'],
                'log_level': lambda x: x in ['debug', 'info', 'warn', 'error'],
                'port': lambda x: 1024 <= int(x) <= 65535
            }
        }
    }
    
    def validate_config(config_data, schema_name):
        """Validate configuration against schema"""
        if schema_name not in SCHEMAS:
            print(f"❌ Unknown schema: {schema_name}")
            return False
            
        schema = SCHEMAS[schema_name]
        errors = []
        
        # Check required fields
        for field in schema['required']:
            if field not in config_data:
                errors.append(f"Missing required field: {field}")
        
        # Validate field values
        for field, value in config_data.items():
            if field in schema['validators']:
                validator = schema['validators'][field]
                try:
                    if callable(validator):
                        if not validator(value):
                            errors.append(f"Invalid value for {field}: {value}")
                    else:  # regex pattern
                        if not re.match(validator, value):
                            errors.append(f"Invalid format for {field}: {value}")
                except Exception as e:
                    errors.append(f"Validation error for {field}: {str(e)}")
        
        if errors:
            print("❌ Validation errors:")
            for error in errors:
                print(f"  - {error}")
            return False
        else:
            print("✅ Configuration validation passed")
            return True
    
    def main():
        if len(sys.argv) != 3:
            print("Usage: validate.py <config-file> <schema-name>")
            sys.exit(1)
            
        config_file = sys.argv[1]
        schema_name = sys.argv[2]
        
        if not os.path.exists(config_file):
            print(f"❌ Configuration file not found: {config_file}")
            sys.exit(1)
        
        # Parse configuration file (assume key=value format)
        config_data = {}
        with open(config_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        config_data[key.strip().lower()] = value.strip()
        
        print(f"🔍 Validating {config_file} against {schema_name} schema")
        success = validate_config(config_data, schema_name)
        sys.exit(0 if success else 1)
    
    if __name__ == '__main__':
        main()
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator-job
spec:
  template:
    spec:
      containers:
      - name: validator
        image: python:3.9-slim
        command: ["python3", "/scripts/validate.py", "/config/app.env", "application"]
        volumeMounts:
        - name: validator-script
          mountPath: /scripts
        - name: config-to-validate
          mountPath: /config
      volumes:
      - name: validator-script
        configMap:
          name: config-schema-validator
          defaultMode: 0755
      - name: config-to-validate
        configMap:
          name: config-candidate
      restartPolicy: Never
  backoffLimit: 3
EOF

# Create configuration templates
cat > templates/application-config-template.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.configName }}
  namespace: {{ .Values.namespace }}
  labels:
    app: {{ .Values.appName }}
    environment: {{ .Values.environment }}
    config-version: {{ .Values.configVersion }}
data:
  # Application Configuration
  APP_NAME: {{ .Values.app.name | quote }}
  APP_ENVIRONMENT: {{ .Values.environment | quote }}
  APP_VERSION: {{ .Values.app.version | quote }}
  APP_PORT: {{ .Values.app.port | quote }}
  
  # Database Configuration
  {{- if .Values.database }}
  DATABASE_HOST: {{ .Values.database.host | quote }}
  DATABASE_PORT: {{ .Values.database.port | quote }}
  DATABASE_NAME: {{ .Values.database.name | quote }}
  DATABASE_SSL_MODE: {{ .Values.database.sslMode | default "require" | quote }}
  DATABASE_POOL_SIZE: {{ .Values.database.poolSize | default "10" | quote }}
  {{- end }}
  
  # Cache Configuration  
  {{- if .Values.redis }}
  REDIS_HOST: {{ .Values.redis.host | quote }}
  REDIS_PORT: {{ .Values.redis.port | quote }}
  REDIS_DB: {{ .Values.redis.database | default "0" | quote }}
  {{- end }}
  
  # Feature Flags
  {{- range $key, $value := .Values.features }}
  FEATURE_{{ $key | upper }}: {{ $value | quote }}
  {{- end }}
  
  # Environment-specific overrides
  {{- if eq .Values.environment "development" }}
  LOG_LEVEL: "debug"
  DEBUG_MODE: "true" 
  METRICS_ENABLED: "true"
  {{- else if eq .Values.environment "staging" }}
  LOG_LEVEL: "info"
  DEBUG_MODE: "false"
  METRICS_ENABLED: "true"
  {{- else if eq .Values.environment "production" }}
  LOG_LEVEL: "warn"
  DEBUG_MODE: "false"
  METRICS_ENABLED: "true"
  {{- end }}
EOF

# Create policy enforcement
cat > policies/configmap-policies.yaml << 'EOF'
# OPA (Open Policy Agent) policies for ConfigMap governance
apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap-policies
data:
  configmap-naming.rego: |
    package configmap.naming
    
    # ConfigMap names must follow naming convention
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        not regex.match("^[a-z][a-z0-9-]*-config$", input.metadata.name)
        msg := sprintf("ConfigMap name '%s' doesn't follow naming convention (must end with -config)", [input.metadata.name])
    }
    
  configmap-labels.rego: |
    package configmap.labels
    
    required_labels := ["app", "environment", "config-version"]
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        required := required_labels[_]
        not input.metadata.labels[required]
        msg := sprintf("ConfigMap missing required label: %s", [required])
    }
    
  configmap-size.rego: |
    package configmap.size
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        total_size := sum([count(input.data[key]) | key := input.data[_]])
        total_size > 1048576  # 1MB limit
        msg := sprintf("ConfigMap size %d bytes exceeds 1MB limit", [total_size])
    }
    
  configmap-environment.rego: |
    package configmap.environment
    
    # Production ConfigMaps must have additional validation
    violation[{"msg": msg}] {
        input.kind == "ConfigMap"
        input.metadata.labels.environment == "production"
        not input.metadata.labels["config-version"]
        msg := "Production ConfigMaps must have config-version label"
    }
    
    violation[{"msg": msg}] {
        input.kind == "ConfigMap" 
        input.metadata.labels.environment == "production"
        input.data[key]
        contains(input.data[key], "localhost")
        msg := sprintf("Production ConfigMap contains localhost reference in key: %s", [key])
    }
EOF

# Create comprehensive deployment script
cat > deploy-with-validation.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
CONFIG_FILE=${2}
NAMESPACE=${3:-default}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <environment> <config-file> [namespace]"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[DEPLOY]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying configuration with validation pipeline"
log_info "Environment: $ENVIRONMENT"
log_info "Config file: $CONFIG_FILE"
log_info "Namespace: $NAMESPACE"

# Step 1: Pre-deployment validation
log_info "📋 Step 1: Pre-deployment validation"

# Validate file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Validate environment
if [[ ! "$
      # Mock backend container
      - name: backend
        image: hashicorp/http-echo:latest
        args:
        - -text=Hello from backend configured via ConfigMap!
        - -listen=:8080
        ports:
        - containerPort: 8080
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: shared-config
              key: log_level
        volumeMounts:
        - name: backend-config-volume
          mountPath: /etc/config
      # Nginx proxy container
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: processed-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 30
      volumes:
      - name: shared-config-volume
        configMap:
          name: shared-config
      - name: backend-config-volume
        configMap:
          name: backend-config
      - name: nginx-template-volume
        configMap:
          name: nginx-template
      - name: processed-config
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: multi-container-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Step 5: Test the deployment
kubectl get pods -l app=multi-container-app
kubectl logs deployment/multi-container-app -c nginx-config-processor
kubectl logs deployment/multi-container-app -c backend
kubectl logs deployment/multi-container-app -c nginx

# Test the application
kubectl port-forward service/multi-container-service 8080:80 &
curl http://localhost:8080
# Stop port forwarding with: kill %1
```

**What you learned:** How to share configuration between multiple containers, use init containers to process configuration templates, and create realistic multi-tier applications with ConfigMaps.

### Mini-Project 2: Dynamic Configuration Updates

Let's explore ConfigMap hot-reloading with a practical monitoring scenario:

```bash
# Step 1: Create a monitoring configuration
kubectl create configmap monitoring-config \
  --from-literal=scrape_interval=15s \
  --from-literal=evaluation_interval=15s \
  --from-literal=alert_threshold=80 \
  --from-literal=retention_days=30 \
  --from-literal=log_level=info

# Step 2: Create a Prometheus-style configuration file
cat > prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml

# Step 3: Create a configuration-aware application
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-watcher-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: config-watcher
  template:
    metadata:
      labels:
        app: config-watcher
    spec:
      containers:
      - name: config-watcher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Configuration Watcher Started"
          echo "Monitoring config files for changes..."
          
          # Function to display current configuration
          show_config() {
            echo "=== Current Configuration ($(date)) ==="
            echo "Environment Variables:"
            env | grep -E "(SCRAPE_|EVALUATION_|ALERT_|RETENTION_|LOG_)" | sort
            echo
            echo "Configuration Files:"
            if [ -f /etc/config/scrape_interval ]; then
              echo "Scrape Interval: $(cat /etc/config/scrape_interval)"
            fi
            if [ -f /etc/config/alert_threshold ]; then
              echo "Alert Threshold: $(cat /etc/config/alert_threshold)"
            fi
            if [ -f /etc/prometheus/prometheus.yml ]; then
              echo "Prometheus Config Present: Yes"
              echo "Config size: $(wc -l < /etc/prometheus/prometheus.yml) lines"
            fi
            echo "================================"
          }
          
          # Show initial configuration
          show_config
          
          # Monitor for file changes
          while true; do
            sleep 30
            show_config
          done
        env:
        - name: SCRAPE_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: scrape_interval
        - name: EVALUATION_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: evaluation_interval
        - name: ALERT_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: alert_threshold
        - name: RETENTION_DAYS
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: retention_days
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: monitoring-config
              key: log_level
        volumeMounts:
        - name: monitoring-config-volume
          mountPath: /etc/config
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: monitoring-config-volume
        configMap:
          name: monitoring-config
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
EOF

# Step 4: Watch the initial configuration
kubectl logs deployment/config-watcher-app -f &

# Step 5: Update the configuration and observe changes
echo "Updating scrape interval..."
kubectl patch configmap monitoring-config -p '{"data":{"scrape_interval":"30s","alert_threshold":"90"}}'

echo "Waiting for changes to propagate (up to 60 seconds)..."
sleep 65

# Update the Prometheus configuration
cat > prometheus-updated.yml << 'EOF'
global:
  scrape_interval: 30s  # Updated
  evaluation_interval: 30s  # Updated

rule_files:
  - "alerts.yml"
  - "recording.yml"  # Added

scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 10s  # Override global
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
      
  - job_name: 'kubernetes-services'  # New job
    kubernetes_sd_configs:
    - role: service

alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093', 'alertmanager-backup:9093']  # Added backup
EOF

kubectl create configmap prometheus-config --from-file=prometheus.yml=prometheus-updated.yml --dry-run=client -o yaml | kubectl apply -f -

echo "Configuration updated. Check the logs for changes..."
# Stop log following with: kill %1
```

**Key Insights from this project:**
- Environment variables from ConfigMaps don't update automatically (require pod restart)
- Volume-mounted ConfigMaps update automatically (with up to 60-second delay)
- Applications should be designed to handle configuration changes gracefully

### Mini-Project 3: Environment-Specific Deployment Pipeline

Let's create a realistic deployment pipeline that manages configurations across different environments:

```bash
# Create directory structure for the project
mkdir -p configmap-pipeline/{environments,applications,scripts}
cd configmap-pipeline

# Step 1: Create environment-specific configurations
cat > environments/development.env << 'EOF'
# Development Environment Configuration
APP_ENVIRONMENT=development
LOG_LEVEL=debug
DATABASE_URL=dev-postgres:5432/myapp_dev
REDIS_URL=dev-redis:6379/0
API_RATE_LIMIT=100
CACHE_TTL=60
EXTERNAL_API_TIMEOUT=5
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

cat > environments/staging.env << 'EOF'
# Staging Environment Configuration
APP_ENVIRONMENT=staging
LOG_LEVEL=info
DATABASE_URL=staging-postgres:5432/myapp_staging
REDIS_URL=staging-redis:6379/0
API_RATE_LIMIT=1000
CACHE_TTL=300
EXTERNAL_API_TIMEOUT=15
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

cat > environments/production.env << 'EOF'
# Production Environment Configuration
APP_ENVIRONMENT=production
LOG_LEVEL=warn
DATABASE_URL=prod-postgres:5432/myapp_production
REDIS_URL=prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Step 2: Create feature flag configurations
cat > environments/dev-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=true
FEATURE_EXPERIMENTAL_API=true
FEATURE_A_B_TESTING=true
EOF

cat > environments/staging-features.env << 'EOF'
FEATURE_NEW_UI=true
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=true
EOF

cat > environments/prod-features.env << 'EOF'
FEATURE_NEW_UI=false
FEATURE_ADVANCED_SEARCH=true
FEATURE_BETA_DASHBOARD=false
FEATURE_EXPERIMENTAL_API=false
FEATURE_A_B_TESTING=false
EOF

# Step 3: Create deployment script
cat > scripts/deploy-environment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}
APP_VERSION=${3:-latest}

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🚀 Deploying to environment: $ENVIRONMENT in namespace: $NAMESPACE"

# Validate environment
if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
    log_error "Invalid environment: $ENVIRONMENT"
    log_info "Valid environments: development, staging, production"
    exit 1
fi

# Create namespace if it doesn't exist
if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace "$NAMESPACE"
fi

# Function to create or update ConfigMap
create_or_update_configmap() {
    local name=$1
    local file=$2
    local description=$3
    
    log_info "Creating/updating ConfigMap: $name ($description)"
    
    if kubectl get configmap "$name" -n "$NAMESPACE" &> /dev/null; then
        log_warning "ConfigMap $name exists, updating..."
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl apply -f -
    else
        kubectl create configmap "$name" \
            --from-env-file="$file" \
            --namespace="$NAMESPACE"
    fi
}

# Deploy environment-specific configurations
create_or_update_configmap "app-config" "environments/$ENVIRONMENT.env" "Application Configuration"
create_or_update_configmap "feature-flags" "environments/$ENVIRONMENT-features.env" "Feature Flags"

# Create version info ConfigMap
log_info "Creating version information ConfigMap..."
kubectl create configmap version-info \
    --from-literal=version="$APP_VERSION" \
    --from-literal=environment="$ENVIRONMENT" \
    --from-literal=deployed_at="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --from-literal=deployed_by="$(whoami)" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

log_success "✅ Configuration deployment complete for $ENVIRONMENT environment"
log_info "📋 Summary:"
echo "   Environment: $ENVIRONMENT"
echo "   Namespace: $NAMESPACE"
echo "   App Version: $APP_VERSION"
echo "   ConfigMaps created:"
echo "     - app-config"
echo "     - feature-flags"
echo "     - version-info"

log_info "🔍 To view configurations:"
echo "   kubectl get configmaps -n $NAMESPACE"
echo "   kubectl describe configmap app-config -n $NAMESPACE"
echo "   kubectl describe configmap feature-flags -n $NAMESPACE"
EOF

chmod +x scripts/deploy-environment.sh

# Step 4: Create the application deployment
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: hashicorp/http-echo:latest
        ports:
        - containerPort: 5678
        # Load environment configuration
        envFrom:
        - configMapRef:
            name: app-config
        - configMapRef:
            name: feature-flags
            prefix: FEATURE_
        - configMapRef:
            name: version-info
            prefix: VERSION_
        # Custom startup message based on configuration
        args:
        - -text=Hello from $(VERSION_environment) environment! App version $(VERSION_version) deployed at $(VERSION_deployed_at)
        livenessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 5678
  type: ClusterIP
---
# ConfigMap for application monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*webapp*.log
        Parser            docker
        Tag               webapp.*
        Refresh_Interval  5
    
    [OUTPUT]
        Name  stdout
        Match *
---
# Health check and debugging pod
apiVersion: v1
kind: Pod
metadata:
  name: environment-inspector
  labels:
    app: inspector
spec:
  containers:
  - name: inspector
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "🔍 Environment Inspector Started"
      echo "=================================="
      
      show_environment_info() {
        echo "📊 Environment Information ($(date))"
        echo "Environment: $APP_ENVIRONMENT"
        echo "Version: $VERSION_version"
        echo "Log Level: $LOG_LEVEL"
        echo "Database: $DATABASE_URL"
        echo "Cache TTL: $CACHE_TTL seconds"
        echo
        echo "🚩 Feature Flags:"
        env | grep "FEATURE_" | sort
        echo
        echo "⚙️  System Configuration:"
        echo "API Rate Limit: $API_RATE_LIMIT"
        echo "External API Timeout: $EXTERNAL_API_TIMEOUT seconds"
        echo "Debug Mode: $DEBUG_MODE"
        echo "Metrics Enabled: $METRICS_ENABLED"
        echo "=================================="
      }
      
      # Show initial state
      show_environment_info
      
      # Monitor for changes every 60 seconds
      while true; do
        sleep 60
        show_environment_info
      done
    envFrom:
    - configMapRef:
        name: app-config
    - configMapRef:
        name: feature-flags
        prefix: FEATURE_
    - configMapRef:
        name: version-info
        prefix: VERSION_
EOF

# Step 5: Create testing script
cat > scripts/test-deployment.sh << 'EOF'
#!/bin/bash
set -e

ENVIRONMENT=${1:-development}
NAMESPACE=${2:-default}

# Color output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_success() { echo -e "${GREEN}[PASS]${NC} $1"; }
log_error() { echo -e "${RED}[FAIL]${NC} $1"; }

log_info "🧪 Testing deployment in $ENVIRONMENT environment"

# Test 1: Check ConfigMaps exist
log_info "Test 1: Verifying ConfigMaps exist..."
for cm in app-config feature-flags version-info; do
    if kubectl get configmap "$cm" -n "$NAMESPACE" &> /dev/null; then
        log_success "ConfigMap $cm exists"
    else
        log_error "ConfigMap $cm not found"
        exit 1
    fi
done

# Test 2: Check application is running
log_info "Test 2: Verifying application pods are running..."
if kubectl get deployment webapp -n "$NAMESPACE" &> /dev/null; then
    READY=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}')
    DESIRED=$(kubectl get deployment webapp -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
    if [ "$READY" = "$DESIRED" ]; then
        log_success "Application deployment ready ($READY/$DESIRED pods)"
    else
        log_error "Application deployment not ready ($READY/$DESIRED pods)"
    fi
else
    log_error "Application deployment not found"
fi

# Test 3: Test application response
log_info "Test 3: Testing application response..."
kubectl port-forward service/webapp-service -n "$NAMESPACE" 8080:80 > /dev/null 2>&1 &
PORT_FORWARD_PID=$!
sleep 3

if curl -s http://localhost:8080 | grep -q "$ENVIRONMENT"; then
    log_success "Application responds with correct environment"
else
    log_error "Application response incorrect or unreachable"
fi

kill $PORT_FORWARD_PID 2>/dev/null

# Test 4: Check environment inspector
log_info "Test 4: Checking environment inspector..."
if kubectl get pod environment-inspector -n "$NAMESPACE" &> /dev/null; then
    log_success "Environment inspector pod exists"
    log_info "Recent inspector logs:"
    kubectl logs environment-inspector -n "$NAMESPACE" --tail=10
else
    log_error "Environment inspector pod not found"
fi

log_success "🎉 All tests completed for $ENVIRONMENT environment"
EOF

chmod +x scripts/test-deployment.sh

# Step 6: Demo the pipeline
log_info() { echo -e "\033[0;34m[INFO]\033[0m $1"; }

log_info "🚀 Starting Environment Pipeline Demo"

# Deploy to development
log_info "Deploying to development environment..."
./scripts/deploy-environment.sh development webapp-dev v1.0.0

# Apply the application
kubectl apply -f applications/webapp-deployment.yaml -n webapp-dev

# Wait for deployment
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-dev

# Test the deployment
./scripts/test-deployment.sh development webapp-dev

log_info "✅ Development deployment complete!"

# Now let's demonstrate staging deployment
log_info "Deploying to staging environment..."
./scripts/deploy-environment.sh staging webapp-staging v1.0.0

# Apply application to staging
kubectl apply -f applications/webapp-deployment.yaml -n webapp-staging

log_info "Waiting for staging deployment..."
kubectl wait --for=condition=available --timeout=300s deployment/webapp -n webapp-staging

# Test staging
./scripts/test-deployment.sh staging webapp-staging

log_info "✅ Staging deployment complete!"

# Cleanup function
cd .. # Go back to parent directory
```

**What you learned from this project:**
- How to organize environment-specific configurations
- Automated deployment pipelines with ConfigMaps
- Testing and validation strategies
- Real-world deployment patterns

## 🎓 Advanced Level: Production-Ready Patterns

### Advanced Demo 1: ConfigMap Validation and Rollback Strategy

Let's implement a production-ready configuration management system with validation and rollback capabilities:

```bash
# Create advanced configuration management system
mkdir -p advanced-configmaps/{configs,validators,scripts}
cd advanced-configmaps

# Step 1: Create configuration validator
cat > validators/config-validator.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-validator-script
data:
  validate-config.sh: |
    #!/bin/bash
    set -e
    
    CONFIG_FILE=${1:-/config/app.env}
    ERRORS=0
    
    echo "🔍 Validating configuration file: $CONFIG_FILE"
    
    # Check if file exists
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "❌ Configuration file not found: $CONFIG_FILE"
        exit 1
    fi
    
    # Validation rules
    validate_url() {
        local var_name=$1
        local url=$2
        if [[ ! "$url" =~ ^[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9.-]+:[0-9]+.*$ ]]; then
            echo "❌ Invalid URL format for $var_name: $url"
            ((ERRORS++))
        else
            echo "✅ Valid URL for $var_name"
        fi
    }
    
    validate_number() {
        local var_name=$1
        local number=$2
        local min=${3:-0}
        local max=${4:-999999}
        if ! [[ "$number" =~ ^[0-9]+$ ]] || [ "$number" -lt "$min" ] || [ "$number" -gt "$max" ]; then
            echo "❌ Invalid number for $var_name: $number (must be $min-$max)"
            ((ERRORS++))
        else
            echo "✅ Valid number for $var_name"
        fi
    }
    
    validate_boolean() {
        local var_name=$1
        local bool=$2
        if [[ ! "$bool" =~ ^(true|false)$ ]]; then
            echo "❌ Invalid boolean for $var_name: $bool (must be true/false)"
            ((ERRORS++))
        else
            echo "✅ Valid boolean for $var_name"
        fi
    }
    
    validate_log_level() {
        local level=$1
        if [[ ! "$level" =~ ^(debug|info|warn|error)$ ]]; then
            echo "❌ Invalid log level: $level (must be debug|info|warn|error)"
            ((ERRORS++))
        else
            echo "✅ Valid log level"
        fi
    }
    
    # Parse and validate configuration
    while IFS='=' read -r key value; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ ]] && continue
        [[ -z "$key" ]] && continue
        
        case "$key" in
            "DATABASE_URL"|"REDIS_URL")
                validate_url "$key" "$value"
                ;;
            "DATABASE_PORT"|"REDIS_PORT"|"APP_PORT")
                validate_number "$key" "$value" 1 65535
                ;;
            "API_RATE_LIMIT")
                validate_number "$key" "$value" 1 100000
                ;;
            "CACHE_TTL"|"EXTERNAL_API_TIMEOUT")
                validate_number "$key" "$value" 1 86400
                ;;
            "DEBUG_MODE"|"METRICS_ENABLED"|"PROFILING_ENABLED")
                validate_boolean "$key" "$value"
                ;;
            "LOG_LEVEL")
                validate_log_level "$value"
                ;;
        esac
    done < "$CONFIG_FILE"
    
    echo "📊 Validation complete. Errors found: $ERRORS"
    
    if [ $ERRORS -eq 0 ]; then
        echo "🎉 Configuration validation passed!"
        exit 0
    else
        echo "💥 Configuration validation failed!"
        exit 1
    fi
---
apiVersion: batch/v1
kind: Job
metadata:
  name: config-validator
spec:
  template:
    spec:
      containers:
      - name: validator
        image: busybox
        command: ["sh", "/scripts/validate-config.sh"]
        volumeMounts:
        - name: config-to-validate
          mountPath: /config
        - name: validator-script
          mountPath: /scripts
      volumes:
      - name: config-to-validate
        configMap:
          name: app-config-candidate  # The config we want to validate
      - name: validator-script
        configMap:
          name: config-validator-script
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3
EOF

# Step 2: Create configuration rollback system
cat > scripts/config-rollback-manager.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_NAME=${2:-app-config}
ACTION=${3:-backup}

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

case "$ACTION" in
    "backup")
        log_info "🔄 Creating backup of ConfigMap: $CONFIG_NAME"
        
        # Get current timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_NAME="${CONFIG_NAME}-backup-${TIMESTAMP}"
        
        # Check if ConfigMap exists
        if ! kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" &> /dev/null; then
            log_error "ConfigMap $CONFIG_NAME not found in namespace $NAMESPACE"
            exit 1
        fi
        
        # Create backup
        kubectl get configmap "$CONFIG_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $CONFIG_NAME/name: $BACKUP_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        kubectl apply -f -
        
        log_success "✅ Backup created: $BACKUP_NAME"
        
        # Label the backup for easy identification
        kubectl label configmap "$BACKUP_NAME" -n "$NAMESPACE" \
            backup-of="$CONFIG_NAME" \
            backup-timestamp="$TIMESTAMP"
        
        echo "Backup ConfigMap: $BACKUP_NAME"
        ;;
        
    "list-backups")
        log_info "📋 Listing backups for ConfigMap: $CONFIG_NAME"
        kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o custom-columns="NAME:.metadata.name,TIMESTAMP:.metadata.labels.backup-timestamp,AGE:.metadata.creationTimestamp"
        ;;
        
    "rollback")
        BACKUP_NAME=$4
        if [ -z "$BACKUP_NAME" ]; then
            log_error "Backup name required for rollback"
            log_info "Usage: $0 <namespace> <config-name> rollback <backup-name>"
            log_info "Use 'list-backups' to see available backups"
            exit 1
        fi
        
        log_warning "⚠️  Rolling back $CONFIG_NAME to backup: $BACKUP_NAME"
        read -p "Are you sure? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Rollback cancelled"
            exit 0
        fi
        
        # Create backup of current state before rollback
        ./config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup
        
        # Perform rollback
        kubectl get configmap "$BACKUP_NAME" -n "$NAMESPACE" -o yaml | \
        sed "s/name: $BACKUP_NAME/name: $CONFIG_NAME/" | \
        sed '/resourceVersion:/d' | \
        sed '/uid:/d' | \
        sed '/backup-of:/d' | \
        sed '/backup-timestamp:/d' | \
        kubectl apply -f -
        
        log_success "✅ Rollback completed: $CONFIG_NAME restored from $BACKUP_NAME"
        ;;
        
    "cleanup")
        KEEP_COUNT=${4:-5}
        log_info "🧹 Cleaning up old backups, keeping $KEEP_COUNT most recent"
        
        # Get backup ConfigMaps sorted by timestamp (oldest first)
        BACKUPS=$(kubectl get configmaps -n "$NAMESPACE" \
            -l backup-of="$CONFIG_NAME" \
            --sort-by='{.metadata.labels.backup-timestamp}' \
            -o jsonpath='{.items[*].metadata.name}')
        
        BACKUP_ARRAY=($BACKUPS)
        TOTAL_BACKUPS=${#BACKUP_ARRAY[@]}
        
        if [ $TOTAL_BACKUPS -le $KEEP_COUNT ]; then
            log_info "Found $TOTAL_BACKUPS backups, keeping all (within limit of $KEEP_COUNT)"
            exit 0
        fi
        
        DELETE_COUNT=$((TOTAL_BACKUPS - KEEP_COUNT))
        log_warning "Found $TOTAL_BACKUPS backups, will delete $DELETE_COUNT oldest"
        
        for ((i=0; i<DELETE_COUNT; i++)); do
            BACKUP_TO_DELETE=${BACKUP_ARRAY[$i]}
            log_info "Deleting backup: $BACKUP_TO_DELETE"
            kubectl delete configmap "$BACKUP_TO_DELETE" -n "$NAMESPACE"
        done
        
        log_success "✅ Cleanup completed, kept $KEEP_COUNT most recent backups"
        ;;
        
    *)
        log_error "Unknown action: $ACTION"
        log_info "Available actions: backup, list-backups, rollback, cleanup"
        exit 1
        ;;
esac
EOF

chmod +x scripts/config-rollback-manager.sh

# Step 3: Create safe configuration update system
cat > scripts/safe-config-update.sh << 'EOF'
#!/bin/bash
set -e

NAMESPACE=${1:-default}
CONFIG_FILE=${2}
CONFIG_NAME=${3:-app-config}

if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <namespace> <config-file> [config-name]"
    echo "Example: $0 default new-config.env app-config"
    exit 1
fi

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[SAFE-UPDATE]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

log_info "🛡️  Starting safe configuration update process"
log_info "Target: $CONFIG_NAME in namespace $NAMESPACE"
log_info "Source: $CONFIG_FILE"

# Step 1: Validate the configuration file exists
if [ ! -f "$CONFIG_FILE" ]; then
    log_error "Configuration file not found: $CONFIG_FILE"
    exit 1
fi

# Step 2: Create backup of current configuration
log_info "📦 Creating backup of current configuration..."
./scripts/config-rollback-manager.sh "$NAMESPACE" "$CONFIG_NAME" backup

# Step 3: Create candidate ConfigMap for validation
log_info "🧪 Creating candidate configuration for validation..."
kubectl create configmap app-config-candidate \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 4: Run validation
log_info "✅ Running configuration validation..."
kubectl apply -f validators/config-validator.yaml -n "$NAMESPACE"

# Wait for validation job to complete
log_info "Waiting for validation to complete..."
kubectl wait --for=condition=complete --timeout=300s job/config-validator -n "$NAMESPACE"

# Check validation result
if kubectl get job config-validator -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' | grep -q "Complete"; then
    log_success "🎉 Configuration validation passed!"
else
    log_error "💥 Configuration validation failed!"
    kubectl logs job/config-validator -n "$NAMESPACE"
    
    # Cleanup candidate ConfigMap
    kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
    kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found
    exit 1
fi

# Step 5: Apply the validated configuration
log_info "🚀 Applying validated configuration..."
kubectl create configmap "$CONFIG_NAME" \
    --from-env-file="$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Step 6: Verify applications are still healthy
log_info "🏥 Checking application health after configuration update..."
sleep 10

# Check if we have deployments using this ConfigMap
DEPLOYMENTS=$(kubectl get deployments -n "$NAMESPACE" -o json | \
    jq -r '.items[] | select(.spec.template.spec.containers[].envFrom[]?.configMapRef.name == "'$CONFIG_NAME'") | .metadata.name')

if [ -n "$DEPLOYMENTS" ]; then
    for deployment in $DEPLOYMENTS; do
        log_info "Checking deployment: $deployment"
        if kubectl get deployment "$deployment" -n "$NAMESPACE" -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_success "✅ Deployment $deployment is healthy"
        else
            log_warning "⚠️  Deployment $deployment may have issues"
        fi
    done
else
    log_info "No deployments found using this ConfigMap"
fi

# Step 7: Cleanup
log_info "🧹 Cleaning up validation resources..."
kubectl delete configmap app-config-candidate -n "$NAMESPACE" --ignore-not-found
kubectl delete job config-validator -n "$NAMESPACE" --ignore-not-found

log_success "🎉 Safe configuration update completed successfully!"
log_info "💡 To rollback if needed, use:"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME list-backups"
echo "   ./scripts/config-rollback-manager.sh $NAMESPACE $CONFIG_NAME rollback <backup-name>"
EOF

chmod +x scripts/safe-config-update.sh

# Step 4: Demo the advanced system
log_info "🎭 Demonstrating Advanced Configuration Management"

# Create a namespace for our demo
kubectl create namespace config-advanced --dry-run=client -o yaml | kubectl apply -f -

# Create initial configuration
cat > configs/initial-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=info
DATABASE_URL=postgres://prod-db:5432/myapp
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=5000
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=false
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

# Apply initial configuration
kubectl create configmap app-config \
    --from-env-file=configs/initial-config.env \
    --namespace=config-advanced

# Create an application that uses this config
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-demo-app
  namespace: config-advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: config-demo
  template:
    metadata:
      labels:
        app: config-demo
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - -text=Config Demo App - Environment: $(APP_ENVIRONMENT), Log Level: $(LOG_LEVEL)
        ports:
        - containerPort: 5678
        envFrom:
        - configMapRef:
            name: app-config
        readinessProbe:
          httpGet:
            path: /
            port: 5678
          initialDelaySeconds: 5
          periodSeconds: 10
EOF

# Wait for deployment
kubectl wait --for=condition=available --timeout=300s deployment/config-demo-app -n config-advanced

# Test the system
log_info "🧪 Testing the advanced configuration management system"

# 1. Create a backup
log_info "Step 1: Creating backup..."
./scripts/config-rollback-manager.sh config-advanced app-config backup

# 2. List backups
log_info "Step 2: Listing backups..."
./scripts/config-rollback-manager.sh config-advanced app-config list-backups

# 3. Create an invalid configuration to test validation
cat > configs/invalid-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=invalid_level
DATABASE_URL=not-a-valid-url
REDIS_URL=redis://prod-redis:6379/0
API_RATE_LIMIT=not_a_number
CACHE_TTL=3600
EXTERNAL_API_TIMEOUT=30
DEBUG_MODE=maybe
METRICS_ENABLED=true
PROFILING_ENABLED=false
EOF

log_info "Step 3: Testing with invalid configuration (should fail)..."
if ./scripts/safe-config-update.sh config-advanced configs/invalid-config.env app-config; then
    log_error "❌ Validation should have failed!"
else
    log_success "✅ Validation correctly failed for invalid configuration"
fi

# 4. Create a valid updated configuration
cat > configs/updated-config.env << 'EOF'
APP_ENVIRONMENT=production
LOG_LEVEL=debug
DATABASE_URL=postgres://new-prod-db:5432/myapp
REDIS_URL=redis://new-prod-redis:6379/0
API_RATE_LIMIT=10000
CACHE_TTL=7200
EXTERNAL_API_TIMEOUT=60
DEBUG_MODE=true
METRICS_ENABLED=true
PROFILING_ENABLED=true
EOF

log_info "Step 4: Applying valid updated configuration..."
./scripts/safe-config-update.sh config-advanced configs/updated-config.env app-config

# 5. Verify the update
log_info "Step 5: Verifying configuration update..."
kubectl get configmap app-config -n config-advanced -o yaml | grep -A 20 "data:"

# 6. Test rollback
log_info "Step 6: Testing rollback functionality..."
BACKUP_NAME=$(./scripts/config-rollback-manager.sh config-advanced app-config list-backups | tail -1 | awk '{print $1}')
if [ -n "$BACKUP_NAME" ] && [ "$BACKUP_NAME" != "NAME" ]; then
    echo "y" | ./scripts/config-rollback-manager.sh config-advanced app-config rollback "$BACKUP_NAME"
    log_success "✅ Rollback test completed"
fi

log_success "🎉 Advanced Configuration Management Demo Complete!"

cd .. # Go back to parent directory
```

### Advanced Demo 2: Multi-Environment GitOps Configuration Pipeline

```bash
# Create GitOps-style configuration management
mkdir -p gitops-configs/{environments,applications,kustomization}
cd gitops-configs

# Base configuration (shared across all environments)
mkdir -p environments/base
cat > environments/base/configmap.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: "MyWebApp"
  APP_PORT: "8080"
  METRICS_ENABLED: "true"
  HEALTH_CHECK_ENABLED: "true"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_METRICS_DASHBOARD: "true"
  FEATURE_HEALTH_CHECKS: "true"
EOF

cat > environments/base/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- configmap.yaml

commonLabels:
  managed-by: kustomize
  component: configuration
EOF

# Development overlay
mkdir -p environments/overlays/development
cat > environments/overlays/development/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "development"
  LOG_LEVEL: "debug"
  DATABASE_URL: "postgres://dev-db:5432/myapp_dev"
  REDIS_URL: "redis://dev-redis:6379/0"
  API_RATE_LIMIT: "100"
  CACHE_TTL: "60"
  DEBUG_MODE: "true"
  EXTERNAL_API_TIMEOUT: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "true"
  FEATURE_EXPERIMENTAL_API: "true"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/development/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-dev

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: development
  
images:
- name: webapp
  newTag: latest
EOF

# Staging overlay
mkdir -p environments/overlays/staging
cat > environments/overlays/staging/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "staging"
  LOG_LEVEL: "info"
  DATABASE_URL: "postgres://staging-db:5432/myapp_staging"
  REDIS_URL: "redis://staging-redis:6379/0"
  API_RATE_LIMIT: "1000"
  CACHE_TTL: "300"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "15"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "true"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "true"
EOF

cat > environments/overlays/staging/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-staging

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: staging
  
images:
- name: webapp
  newTag: v1.0.0-rc
EOF

# Production overlay
mkdir -p environments/overlays/production
cat > environments/overlays/production/configmap-patch.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENVIRONMENT: "production"
  LOG_LEVEL: "warn"
  DATABASE_URL: "postgres://prod-db:5432/myapp_production"
  REDIS_URL: "redis://prod-redis:6379/0"
  API_RATE_LIMIT: "5000"
  CACHE_TTL: "3600"
  DEBUG_MODE: "false"
  EXTERNAL_API_TIMEOUT: "30"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  FEATURE_NEW_UI: "false"
  FEATURE_BETA_FEATURES: "false"
  FEATURE_EXPERIMENTAL_API: "false"
  FEATURE_A_B_TESTING: "false"
EOF

cat > environments/overlays/production/kustomization.yaml << 'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: webapp-prod

resources:
- ../../base

patchesStrategicMerge:
- configmap-patch.yaml

commonLabels:
  environment: production
  
images:
- name: webapp
  newTag: v1.0.0
EOF

# Application deployment template
cat > applications/webapp-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      
  